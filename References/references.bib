@article{Hayat2021,
   abstract = {<p> Sky surveys are the largest data generators in astronomy, making automated tools for extracting meaningful scientific information an absolute necessity. We show that, without the need for labels, self-supervised learning recovers representations of sky survey images that are semantically useful for a variety of scientific tasks. These representations can be directly used as features, or fine-tuned, to outperform supervised methods trained only on labeled data. We apply a contrastive learning framework on multiband galaxy photometry from the Sloan Digital Sky Survey (SDSS), to learn image representations. We then use them for galaxy morphology classification and fine-tune them for photometric redshift estimation, using labels from the Galaxy Zoo 2 data set and SDSS spectroscopy. In both downstream tasks, using the same learned representations, we outperform the supervised state-of-the-art results, and we show that our approach can achieve the accuracy of supervised models while using 2–4 times fewer labels for training. The codes, trained models, and data can be found at <ext-link ext-link-type="uri" href="https://portal.nersc.gov/project/dasrepo/self-supervised-learning-sdss" type="simple">https://portal.nersc.gov/project/dasrepo/self-supervised-learning-sdss</ext-link> . </p>},
   author = {Md Abul Hayat and George Stein and Peter Harrington and Zarija Lukić and Mustafa Mustafa},
   doi = {10.3847/2041-8213/abf2c7},
   issn = {2041-8205},
   issue = {2},
   journal = {The Astrophysical Journal Letters},
   month = {4},
   pages = {L33},
   title = {Self-supervised Representation Learning for Astronomical Images},
   volume = {911},
   url = {https://iopscience.iop.org/article/10.3847/2041-8213/abf2c7},
   year = {2021},
}
@article{,
   abstract = {Contrastive Learning has recently received interest due to its success in
self-supervised representation learning in the computer vision domain. However,
the origins of Contrastive Learning date as far back as the 1990s and its
development has spanned across many fields and domains including Metric
Learning and natural language processing. In this paper we provide a
comprehensive literature review and we propose a general Contrastive
Representation Learning framework that simplifies and unifies many different
contrastive learning methods. We also provide a taxonomy for each of the
components of contrastive learning in order to summarise it and distinguish it
from other forms of machine learning. We then discuss the inductive biases
which are present in any contrastive learning system and we analyse our
framework under different views from various sub-fields of Machine Learning.
Examples of how contrastive learning has been applied in computer vision,
natural language processing, audio processing, and others, as well as in
Reinforcement Learning are also presented. Finally, we discuss the challenges
and some of the most promising future research directions ahead.},
   author = {Phuc H. Le-Khac and Graham Healy and Alan F. Smeaton},
   doi = {10.1109/access.2020.3031549},
   month = {10},
   title = {Contrastive Representation Learning: A Framework and Review},
   url = {https://arxiv.org/abs/2010.05113},
   year = {2020},
}
@article{Zhu2016,
   abstract = {Image segmentation refers to the process to divide an image into meaningful non-overlapping regions according to human perception, which has become a classic topic since the early ages of computer vision. A lot of research has been conducted and has resulted in many applications. While many segmentation algorithms exist, there are only a few sparse and outdated summarizations available. Thus, in this paper, we aim to provide a comprehensive review of the recent progress in the field. Covering 190 publications, we give an overview of broad segmentation topics including not only the classic unsupervised methods, but also the recent weakly-/semi-supervised methods and the fully-supervised methods. In addition, we review the existing influential datasets and evaluation metrics. We also suggest some design choices and research directions for future research in image segmentation.},
   author = {Hongyuan Zhu and Fanman Meng and Jianfei Cai and Shijian Lu},
   doi = {10.1016/J.JVCIR.2015.10.012},
   issn = {1047-3203},
   journal = {Journal of Visual Communication and Image Representation},
   keywords = {Image cosegmentation,Image segmentation,Interactive image segmentation,Object proposal,Semantic image parsing,Superpixel,Unsupervised image segmentation,Weakly-supervised image segmentation},
   month = {1},
   pages = {12-27},
   publisher = {Academic Press},
   title = {Beyond pixels: A comprehensive survey from bottom-up to semantic image segmentation and cosegmentation},
   volume = {34},
   year = {2016},
}
@article{Kirillov2019,
   abstract = {We present a new method for efficient high-quality image segmentation of
objects and scenes. By analogizing classical computer graphics methods for
efficient rendering with over- and undersampling challenges faced in pixel
labeling tasks, we develop a unique perspective of image segmentation as a
rendering problem. From this vantage, we present the PointRend (Point-based
Rendering) neural network module: a module that performs point-based
segmentation predictions at adaptively selected locations based on an iterative
subdivision algorithm. PointRend can be flexibly applied to both instance and
semantic segmentation tasks by building on top of existing state-of-the-art
models. While many concrete implementations of the general idea are possible,
we show that a simple design already achieves excellent results. Qualitatively,
PointRend outputs crisp object boundaries in regions that are over-smoothed by
previous methods. Quantitatively, PointRend yields significant gains on COCO
and Cityscapes, for both instance and semantic segmentation. PointRend's
efficiency enables output resolutions that are otherwise impractical in terms
of memory or computation compared to existing approaches. Code has been made
available at
https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend.},
   author = {Alexander Kirillov and Yuxin Wu and Kaiming He and Ross Girshick},
   doi = {10.48550/arxiv.1912.08193},
   month = {12},
   title = {PointRend: Image Segmentation as Rendering},
   url = {https://arxiv.org/abs/1912.08193},
   year = {2019},
}
@article{Chen2018,
   abstract = {Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0\% and 82.1\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at \url\{https://github.com/tensorflow/models/tree/master/research/deeplab\}.},
   author = {Liang-Chieh Chen and Yukun Zhu and George Papandreou and Florian Schroff and Hartwig Adam},
   doi = {10.48550/arxiv.1802.02611},
   month = {2},
   title = {Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation},
   url = {http://arxiv.org/abs/1802.02611},
   year = {2018},
}
@article{Zhou2018,
   abstract = {In this paper, we present UNet++, a new, more powerful architecture for
medical image segmentation. Our architecture is essentially a deeply-supervised
encoder-decoder network where the encoder and decoder sub-networks are
connected through a series of nested, dense skip pathways. The re-designed skip
pathways aim at reducing the semantic gap between the feature maps of the
encoder and decoder sub-networks. We argue that the optimizer would deal with
an easier learning task when the feature maps from the decoder and encoder
networks are semantically similar. We have evaluated UNet++ in comparison with
U-Net and wide U-Net architectures across multiple medical image segmentation
tasks: nodule segmentation in the low-dose CT scans of chest, nuclei
segmentation in the microscopy images, liver segmentation in abdominal CT
scans, and polyp segmentation in colonoscopy videos. Our experiments
demonstrate that UNet++ with deep supervision achieves an average IoU gain of
3.9 and 3.4 points over U-Net and wide U-Net, respectively.},
   author = {Zongwei Zhou and Md Mahfuzur Rahman Siddiquee and Nima Tajbakhsh and Jianming Liang},
   doi = {10.48550/arxiv.1807.10165},
   month = {7},
   title = {UNet++: A Nested U-Net Architecture for Medical Image Segmentation},
   url = {https://arxiv.org/abs/1807.10165},
   year = {2018},
}
@report{,
   abstract = {In this article, we describe an automatic differentiation module of PyTorch-a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd [4], and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.},
   author = {Adam Paszke and Sam Gross and Soumith Chintala and Gregory Chanan and Edward Yang and Zachary Devito Facebook and A I Research and Zeming Lin and Alban Desmaison and Luca Antiga and Orobix Srl and Adam Lerer},
   title = {Automatic differentiation in PyTorch},
}
@article{Tao2018,
   abstract = {Automated spectral classification is an active research area in astronomy at the age of data explosion. While new generation of sky survey telescopes (e.g. LAMOST and SDSS) produce huge amount of spectra, automated spectral classification is highly required to replace the current model fitting approach with human intervention. Galaxies, and especially active galactic nucleus (AGNs), are important targets of sky survey programs. Efficient and automated methods for galaxy spectra classification is the basis of systematic study on physical properties and evolution of galaxies. To address the problem, in this paper we carry out an experiment on Alibaba Cloud AI plaform (PAI) to explore automated galaxy spectral classification using machine learning approach. Supervised machine learning algorithms (Logistic Regression, Random Forest and Linear SVM) were performed on a dataset consist of ~ 10000 galaxy spectra of SDSS DR14, and the classification results of which are compared and discussed. These galaxy spectra each has a subclass tag (i.e. AGNs, Starburst, Starforming, and etc.) that we use as training labels.},
   author = {Yihan Tao and Yanxia Zhang and Chenzhou Cui and Ge Zhang},
   doi = {10.48550/arxiv.1801.04839},
   month = {1},
   title = {Automated Spectral Classification of Galaxies using Machine Learning Approach on Alibaba Cloud AI platform (PAI)},
   url = {http://arxiv.org/abs/1801.04839},
   year = {2018},
}
@article{Abazajian2009,
   author = {Kevork N. Abazajian and Jennifer K. Adelman-McCarthy and Marcel A. Agüeros and Sahar S. Allam and Carlos Allende Prieto and Deokkeun An and Kurt S. J. Anderson and Scott F. Anderson and James Annis and Neta A. Bahcall and C. A. L. Bailer-Jones and J. C. Barentine and Bruce A. Bassett and Andrew C. Becker and Timothy C. Beers and Eric F. Bell and Vasily Belokurov and Andreas A. Berlind and Eileen F. Berman and Mariangela Bernardi and Steven J. Bickerton and Dmitry Bizyaev and John P. Blakeslee and Michael R. Blanton and John J. Bochanski and William N. Boroski and Howard J. Brewington and Jarle Brinchmann and J. Brinkmann and Robert J. Brunner and Tamás Budavári and Larry N. Carey and Samuel Carliles and Michael A. Carr and Francisco J. Castander and David Cinabro and A. J. Connolly and István Csabai and Carlos E. Cunha and Paul C. Czarapata and James R. A. Davenport and Ernst de Haas and Ben Dilday and Mamoru Doi and Daniel J. Eisenstein and Michael L. Evans and N. W. Evans and Xiaohui Fan and Scott D. Friedman and Joshua A. Frieman and Masataka Fukugita and Boris T. Gänsicke and Evalyn Gates and Bruce Gillespie and G. Gilmore and Belinda Gonzalez and Carlos F. Gonzalez and Eva K. Grebel and James E. Gunn and Zsuzsanna Györy and Patrick B. Hall and Paul Harding and Frederick H. Harris and Michael Harvanek and Suzanne L. Hawley and Jeffrey J. E. Hayes and Timothy M. Heckman and John S. Hendry and Gregory S. Hennessy and Robert B. Hindsley and J. Hoblitt and Craig J. Hogan and David W. Hogg and Jon A. Holtzman and Joseph B. Hyde and Shin-ichi Ichikawa and Takashi Ichikawa and Myungshin Im and Željko Ivezić and Sebastian Jester and Linhua Jiang and Jennifer A. Johnson and Anders M. Jorgensen and Mario Jurić and Stephen M. Kent and R. Kessler and S. J. Kleinman and G. R. Knapp and Kohki Konishi and Richard G. Kron and Jurek Krzesinski and Nikolay Kuropatkin and Hubert Lampeitl and Svetlana Lebedeva and Myung Gyoon Lee and Young Sun Lee and R. French Leger and Sébastien Lépine and Nolan Li and Marcos Lima and Huan Lin and Daniel C. Long and Craig P. Loomis and Jon Loveday and Robert H. Lupton and Eugene Magnier and Olena Malanushenko and Viktor Malanushenko and Rachel Mandelbaum and Bruce Margon and John P. Marriner and David Martínez-Delgado and Takahiko Matsubara and Peregrine M. McGehee and Timothy A. McKay and Avery Meiksin and Heather L. Morrison and Fergal Mullally and Jeffrey A. Munn and Tara Murphy and Thomas Nash and Ada Nebot and Eric H. Neilsen and Heidi Jo Newberg and Peter R. Newman and Robert C. Nichol and Tom Nicinski and Maria Nieto-Santisteban and Atsuko Nitta and Sadanori Okamura and Daniel J. Oravetz and Jeremiah P. Ostriker and Russell Owen and Nikhil Padmanabhan and Kaike Pan and Changbom Park and George Pauls and John Peoples and Will J. Percival and Jeffrey R. Pier and Adrian C. Pope and Dimitri Pourbaix and Paul A. Price and Norbert Purger and Thomas Quinn and M. Jordan Raddick and Paola Re Fiorentin and Gordon T. Richards and Michael W. Richmond and Adam G. Riess and Hans-Walter Rix and Constance M. Rockosi and Masao Sako and David J. Schlegel and Donald P. Schneider and Ralf-Dieter Scholz and Matthias R. Schreiber and Axel D. Schwope and Uroš Seljak and Branimir Sesar and Erin Sheldon and Kazu Shimasaku and Valena C. Sibley and A. E. Simmons and Thirupathi Sivarani and J. Allyn Smith and Martin C. Smith and Vernesa Smolčić and Stephanie A. Snedden and Albert Stebbins and Matthias Steinmetz and Chris Stoughton and Michael A. Strauss and Mark SubbaRao and Yasushi Suto and Alexander S. Szalay and István Szapudi and Paula Szkody and Masayuki Tanaka and Max Tegmark and Luis F. A. Teodoro and Aniruddha R. Thakar and Christy A. Tremonti and Douglas L. Tucker and Alan Uomoto and Daniel E. Vanden Berk and Jan Vandenberg and S. Vidrih and Michael S. Vogeley and Wolfgang Voges and Nicole P. Vogt and Yogesh Wadadekar and Shannon Watters and David H. Weinberg and Andrew A. West and Simon D. M. White and Brian C. Wilhite and Alainna C. Wonders and Brian Yanny and D. R. Yocum and Donald G. York and Idit Zehavi and Stefano Zibetti and Daniel B. Zucker},
   doi = {10.1088/0067-0049/182/2/543},
   issn = {0067-0049},
   issue = {2},
   journal = {The Astrophysical Journal Supplement Series},
   month = {6},
   pages = {543-558},
   title = {THE SEVENTH DATA RELEASE OF THE SLOAN DIGITAL SKY SURVEY},
   volume = {182},
   year = {2009},
}
@article{Dewdney2009,
   abstract = {The Square Kilometre Array (SKA) will be an ultrasensitive radio telescope, built to further the understanding of the most important phenomena in the Universe, including some pertaining to the birth and eventual death of the Universe itself. Over the next few years, the SKA will make the transition from an early formative to a well-defined design. This paper outlines how the scientific challenges are translated into technical challenges, how the application of recent technology offers the potential of affordably meeting these challenges, and how the choices of technology will ultimately be made. The SKA will be an array of coherently connected antennas spread over an area about 3000 km in extent, with an aggregate antenna collecting area of up to 106 m $^\{2\}$ at centimeter and meter wavelengths. A key scientific requirement is the ability to carry out sensitive observations of the sky over large areas (surveys). The survey speed of the SKA will be enabled by the application of the most up-to-date signal-processing technology available. The SKA science impact will be widely felt in astroparticle physics and cosmology, fundamental physics, galactic and extragalactic astronomy, solar system science, and astrobiology. © 2006 IEEE.},
   author = {P.E. Dewdney and P.J. Hall and R.T. Schilizzi and T.J.L.W. Lazio},
   doi = {10.1109/JPROC.2009.2021005},
   issue = {8},
   journal = {Proceedings of the IEEE},
   pages = {1482-1496},
   title = {The square kilometre array},
   volume = {97},
   year = {2009},
}
@article{Yan2022,
   author = {Ruiqing Yan and Rong Ma and Wei Liu and Zongyao Yin and Zhengang Zhao and Siying Chen and Sheng Chang and Hui Zhu and Dan Hu and Xianchuan Yu},
   doi = {10.1016/J.DSP.2022.103663},
   issn = {1051-2004},
   journal = {Digital Signal Processing},
   month = {9},
   pages = {103663},
   publisher = {Academic Press},
   title = {Weak celestial source fringes detection based on channel attention shrinkage networks and cluster-based anchor boxes generation algorithm},
   volume = {129},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S1051200422002809},
   year = {2022},
}
@book_section{Vittaut2002,
   author = {Jean-Noël Vittaut and Massih-Reza Amini and Patrick Gallinari},
   doi = {10.1007/3-540-36755-1_39},
   pages = {468-479},
   title = {Learning Classification with Both Labeled and Unlabeled Data},
   year = {2002},
}
@article{Agrawala1970,
   author = {A. Agrawala},
   doi = {10.1109/TIT.1970.1054472},
   issn = {0018-9448},
   issue = {4},
   journal = {IEEE Transactions on Information Theory},
   month = {7},
   pages = {373-379},
   title = {Learning with a probabilistic teacher},
   volume = {16},
   year = {1970},
}
@article{Wang2020,
   abstract = {In this work, we aim at building a simple, direct, and fast instance segmentation framework with strong performance. We follow the principle of the SOLO method of Wang et al. "SOLO: segmenting objects by locations". Importantly, we take one step further by dynamically learning the mask head of the object segmenter such that the mask head is conditioned on the location. Specifically, the mask branch is decoupled into a mask kernel branch and mask feature branch, which are responsible for learning the convolution kernel and the convolved features respectively. Moreover, we propose Matrix NMS (non maximum suppression) to significantly reduce the inference time overhead due to NMS of masks. Our Matrix NMS performs NMS with parallel matrix operations in one shot, and yields better results. We demonstrate a simple direct instance segmentation system, outperforming a few state-of-the-art methods in both speed and accuracy. A light-weight version of SOLOv2 executes at 31.3 FPS and yields 37.1% AP. Moreover, our state-of-the-art results in object detection (from our mask byproduct) and panoptic segmentation show the potential to serve as a new strong baseline for many instance-level recognition tasks besides instance segmentation. Code is available at: https://git.io/AdelaiDet},
   author = {Xinlong Wang and Rufeng Zhang and Tao Kong and Lei Li and Chunhua Shen},
   doi = {10.48550/arxiv.2003.10152},
   month = {3},
   title = {SOLOv2: Dynamic and Fast Instance Segmentation},
   url = {http://arxiv.org/abs/2003.10152},
   year = {2020},
}
@article{Chen2019,
   abstract = {Sliding-window object detectors that generate bounding-box object predictions over a dense, regular grid have advanced rapidly and proven popular. In contrast, modern instance segmentation approaches are dominated by methods that first detect object bounding boxes, and then crop and segment these regions, as popularized by Mask R-CNN. In this work, we investigate the paradigm of dense sliding-window instance segmentation, which is surprisingly under-explored. Our core observation is that this task is fundamentally different than other dense prediction tasks such as semantic segmentation or bounding-box object detection, as the output at every spatial location is itself a geometric structure with its own spatial dimensions. To formalize this, we treat dense instance segmentation as a prediction task over 4D tensors and present a general framework called TensorMask that explicitly captures this geometry and enables novel operators on 4D tensors. We demonstrate that the tensor view leads to large gains over baselines that ignore this structure, and leads to results comparable to Mask R-CNN. These promising results suggest that TensorMask can serve as a foundation for novel advances in dense mask prediction and a more complete understanding of the task. Code will be made available.},
   author = {Xinlei Chen and Ross Girshick and Kaiming He and Piotr Dollár},
   doi = {10.48550/arxiv.1903.12174},
   month = {3},
   title = {TensorMask: A Foundation for Dense Object Segmentation},
   url = {http://arxiv.org/abs/1903.12174},
   year = {2019},
}
@article{Zhang2021,
   abstract = {Semantic, instance, and panoptic segmentations have been addressed using
different and specialized frameworks despite their underlying connections. This
paper presents a unified, simple, and effective framework for these essentially
similar tasks. The framework, named K-Net, segments both instances and semantic
categories consistently by a group of learnable kernels, where each kernel is
responsible for generating a mask for either a potential instance or a stuff
class. To remedy the difficulties of distinguishing various instances, we
propose a kernel update strategy that enables each kernel dynamic and
conditional on its meaningful group in the input image. K-Net can be trained in
an end-to-end manner with bipartite matching, and its training and inference
are naturally NMS-free and box-free. Without bells and whistles, K-Net
surpasses all previous published state-of-the-art single-model results of
panoptic segmentation on MS COCO test-dev split and semantic segmentation on
ADE20K val split with 55.2% PQ and 54.3% mIoU, respectively. Its instance
segmentation performance is also on par with Cascade Mask R-CNN on MS COCO with
60%-90% faster inference speeds. Code and models will be released at
https://github.com/ZwwWayne/K-Net/.},
   author = {Wenwei Zhang and Jiangmiao Pang and Kai Chen and Chen Change Loy},
   doi = {10.48550/arxiv.2106.14855},
   month = {6},
   title = {K-Net: Towards Unified Image Segmentation},
   url = {https://arxiv.org/abs/2106.14855},
   year = {2021},
}
@article{Tanoglidis2022,
   abstract = {Wide-field astronomical surveys are often affected by the presence of undesirable reflections (often known as “ghosting artifacts” or “ghosts”) and scattered-light artifacts. The identification and mitigation of these artifacts is important for rigorous astronomical analyses of faint and low-surface-brightness systems. However, the identification of ghosts and scattered-light artifacts is challenging due to (a) the complex morphology of these features and (b) the large data volume of current and near-future surveys. In this work, we use images from the Dark Energy Survey (DES) to train, validate, and test a deep neural network (Mask R-CNN) to detect and localize ghosts and scattered-light artifacts. We find that the ability of the Mask R-CNN model to identify affected regions is superior to that of conventional algorithms and traditional convolutional neural networks methods. We propose that a multi-step pipeline combining Mask R-CNN segmentation with a classical CNN classifier provides a powerful technique for the automated detection of ghosting and scattered-light artifacts in current and near-future surveys.},
   author = {D. Tanoglidis and A. Ćiprijanović and A. Drlica-Wagner and B. Nord and M. H.L.S. Wang and A. Jacob Amsellem and K. Downey and S. Jenkins and D. Kafkes and Z. Zhang},
   doi = {10.1016/J.ASCOM.2022.100580},
   issn = {2213-1337},
   journal = {Astronomy and Computing},
   keywords = {Deep learning,Image artifacts,Object detection},
   month = {4},
   pages = {100580},
   publisher = {Elsevier},
   title = {DeepGhostBusters: Using Mask R-CNN to detect and mask ghosting and scattered-light artifacts from optical survey images},
   volume = {39},
   year = {2022},
}
@article{,
   author = {H Domínguez Sánchez and M Huertas-Company and M Bernardi and S Kaviraj and J L Fischer and T M C Abbott and F B Abdalla and J Annis and S Avila and D Brooks and E Buckley-Geer and A Carnero Rosell and M Carrasco Kind and J Carretero and C E Cunha and C B D’Andrea and L N da Costa and C Davis and J De Vicente and P Doel and A E Evrard and P Fosalba and J Frieman and J García-Bellido and E Gaztanaga and D W Gerdes and D Gruen and R A Gruendl and J Gschwend and G Gutierrez and W G Hartley and D L Hollowood and K Honscheid and B Hoyle and D J James and K Kuehn and N Kuropatkin and O Lahav and M A G Maia and M March and P Melchior and F Menanteau and R Miquel and B Nord and A A Plazas and E Sanchez and V Scarpine and R Schindler and M Schubnell and M Smith and R C Smith and M Soares-Santos and F Sobreira and E Suchyta and M E C Swanson and G Tarle and D Thomas and A R Walker and J Zuntz},
   doi = {10.1093/mnras/sty3497},
   issn = {0035-8711},
   issue = {1},
   journal = {Monthly Notices of the Royal Astronomical Society},
   month = {3},
   pages = {93-100},
   title = {Transfer learning for galaxy morphology from one survey to another},
   volume = {484},
   year = {2019},
}
@article{Marianer2021,
   abstract = {By now, tens of gravitational-wave (GW) events have been detected by the LIGO and Virgo detectors. These GWs have all been emitted by compact binary coalescence, for which we have excellent predictive models. However, there might be other sources for which we do not have reliable models. Some are expected to exist but to be very rare (e.g. supernovae), while others may be totally unanticipated. So far, no unmodelled sources have been discovered, but the lack of models makes the search for such sources much more difficult and less sensitive. We present here a search for unmodelled GW signals using semisupervised machine learning. We apply deep learning and outlier detection algorithms to labelled spectrograms of GW strain data, and then search for spectrograms with anomalous patterns in public LIGO data. We searched $\{\sim\}13\{\{\ \rm per\ cent\}\}$ of the coincident data from the first two observing runs. No candidates of GW signals were detected in the data analyzed. We evaluate the sensitivity of the search using simulated signals, we show that this search can detect spectrograms containing unusual or unexpected GW patterns, and we report the waveforms and amplitudes for which a $50\{\{\ \rm per\ cent\}\}$ detection rate is achieved.},
   author = {Tom Marianer and Dovi Poznanski and J Xavier Prochaska},
   doi = {10.1093/mnras/staa3550},
   issn = {0035-8711},
   issue = {4},
   journal = {Monthly Notices of the Royal Astronomical Society},
   month = {2},
   pages = {5408-5419},
   title = {A semisupervised machine learning search for never-seen gravitational-wave sources},
   volume = {500},
   url = {https://doi.org/10.1093/mnras/staa3550},
   year = {2021},
}
@article{Spindler2021,
   abstract = {<p>We present AstroVaDEr (Astronomical Variational Deep Embedder), a variational autoencoder designed to perform unsupervised clustering and synthetic image generation using astronomical imaging catalogues. The model is a convolutional neural network that learns to embed images into a low-dimensional latent space, and simultaneously optimizes a Gaussian Mixture Model (GMM) on the embedded vectors to cluster the training data. By utilizing variational inference, we are able to use the learned GMM as a statistical prior on the latent space to facilitate random sampling and generation of synthetic images. We demonstrate AstroVaDEr’s capabilities by training it on grey-scaled gri images from the Sloan Digital Sky Survey, using a sample of galaxies that are classified by Galaxy Zoo 2. An unsupervised clustering model is found that separates galaxies based on learned morphological features such as axial ratio, surface brightness profile, orientation, and the presence of companions. We use the learned mixture model to generate synthetic images of galaxies based on the morphological profiles of the Gaussian components. AstroVaDEr succeeds in producing a morphological classification scheme from unlabelled data, but unexpectedly places high importance on the presence of companion objects – demonstrating the importance of human interpretation. The network is scalable and flexible, allowing for larger data sets to be classified, or different kinds of imaging data. We also demonstrate the generative properties of the model, which allow for realistic synthetic images of galaxies to be sampled from the learned classification scheme. These can be used to create synthetic image catalogues or to perform image processing tasks such as deblending.</p>},
   author = {Ashley Spindler and James E Geach and Michael J Smith},
   doi = {10.1093/mnras/staa3670},
   issn = {0035-8711},
   issue = {1},
   journal = {Monthly Notices of the Royal Astronomical Society},
   month = {2},
   pages = {985-1007},
   title = {AstroVaDEr: astronomical variational deep embedder for unsupervised morphological classification of galaxies and synthetic image generation},
   volume = {502},
   year = {2021},
}
@article{Shamshad2022,
   abstract = {Following unprecedented success on the natural language tasks, Transformers
have been successfully applied to several computer vision problems, achieving
state-of-the-art results and prompting researchers to reconsider the supremacy
of convolutional neural networks (CNNs) as \{de facto\} operators. Capitalizing
on these advances in computer vision, the medical imaging field has also
witnessed growing interest for Transformers that can capture global context
compared to CNNs with local receptive fields. Inspired from this transition, in
this survey, we attempt to provide a comprehensive review of the applications
of Transformers in medical imaging covering various aspects, ranging from
recently proposed architectural designs to unsolved issues. Specifically, we
survey the use of Transformers in medical image segmentation, detection,
classification, reconstruction, synthesis, registration, clinical report
generation, and other tasks. In particular, for each of these applications, we
develop taxonomy, identify application-specific challenges as well as provide
insights to solve them, and highlight recent trends. Further, we provide a
critical discussion of the field's current state as a whole, including the
identification of key challenges, open problems, and outlining promising future
directions. We hope this survey will ignite further interest in the community
and provide researchers with an up-to-date reference regarding applications of
Transformer models in medical imaging. Finally, to cope with the rapid
development in this field, we intend to regularly update the relevant latest
papers and their open-source implementations at
\url\{https://github.com/fahadshamshad/awesome-transformers-in-medical-imaging\}.},
   author = {Fahad Shamshad and Salman Khan and Syed Waqas Zamir and Muhammad Haris Khan and Munawar Hayat and Fahad Shahbaz Khan and Huazhu Fu},
   doi = {10.48550/arxiv.2201.09873},
   month = {1},
   title = {Transformers in Medical Imaging: A Survey},
   url = {https://arxiv.org/abs/2201.09873},
   year = {2022},
}
@article{Lin2021,
   abstract = {Quantifying the morphology of galaxies has been an important task in
astrophysics to understand the formation and evolution of galaxies. In recent
years, the data size has been dramatically increasing due to several on-going
and upcoming surveys. Labeling and identifying interesting objects for further
investigations has been explored by citizen science through the Galaxy Zoo
Project and by machine learning in particular with the convolutional neural
networks (CNNs). In this work, we explore the usage of Vision Transformer (ViT)
for galaxy morphology classification for the first time. We show that ViT could
reach competitive results compared with CNNs, and is specifically good at
classifying smaller-sized and fainter galaxies. With this promising preliminary
result, we believe the ViT network architecture can be an important tool for
galaxy morphological classification for the next generation surveys. Our open
source, is publicly available at \url\{https://github.com/sliao-mi-luku/Galaxy-Zoo-Classification\}},
   author = {Joshua Yao-Yu Lin and Song-Mao Liao and Hung-Jin Huang and Wei-Ting Kuo and Olivia Hsuan-Min Ou},
   doi = {10.48550/arxiv.2110.01024},
   month = {10},
   title = {Galaxy Morphological Classification with Efficient Vision Transformer},
   url = {https://arxiv.org/abs/2110.01024},
   year = {2021},
}
@inproceedings{Vaswani2017,
   author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N Gomez and Ł ukasz Kaiser and Illia Polosukhin},
   editor = {I Guyon and U Von Luxburg and S Bengio and H Wallach and R Fergus and S Vishwanathan and R Garnett},
   journal = {Advances in Neural Information Processing Systems},
   publisher = {Curran Associates, Inc.},
   title = {Attention is All you Need},
   volume = {30},
   url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
   year = {2017},
}
@article{Bertin1996,
   author = {E. Bertin and S. Arnouts},
   doi = {10.1051/aas:1996164},
   issn = {0365-0138},
   issue = {2},
   journal = {Astronomy and Astrophysics Supplement Series},
   month = {6},
   pages = {393-404},
   title = {SExtractor: Software for source extraction},
   volume = {117},
   url = {http://aas.aanda.org/10.1051/aas:1996164},
   year = {1996},
}
@article{Teichmann2018,
   abstract = {For the challenging semantic image segmentation task the most efficient
models have traditionally combined the structured modelling capabilities of
Conditional Random Fields (CRFs) with the feature extraction power of CNNs. In
more recent works however, CRF post-processing has fallen out of favour. We
argue that this is mainly due to the slow training and inference speeds of
CRFs, as well as the difficulty of learning the internal CRF parameters. To
overcome both issues we propose to add the assumption of conditional
independence to the framework of fully-connected CRFs. This allows us to
reformulate the inference in terms of convolutions, which can be implemented
highly efficiently on GPUs. Doing so speeds up inference and training by a
factor of more then 100. All parameters of the convolutional CRFs can easily be
optimized using backpropagation. To facilitating further CRF research we make
our implementation publicly available. Please visit:
https://github.com/MarvinTeichmann/ConvCRF},
   author = {Marvin T. T. Teichmann and Roberto Cipolla},
   doi = {10.48550/arxiv.1805.04777},
   month = {5},
   title = {Convolutional CRFs for Semantic Segmentation},
   url = {https://arxiv.org/abs/1805.04777},
   year = {2018},
}
@article{Jun2020,
   author = {Ma Jun},
   journal = {arXiv preprint arXiv:2005.13449},
   title = {Segmentation Loss Odyssey},
   year = {2020},
}
@book_section{Saha2019,
   author = {Sudipan Saha and Swathikiran Sudhakaran and Biplab Banerjee and Sumedh Pendurkar},
   doi = {10.1007/978-3-030-30645-8_46},
   pages = {499-510},
   title = {Semantic Guided Deep Unsupervised Image Segmentation},
   year = {2019},
}
@inproceedings{Kanezaki2018,
   author = {Asako Kanezaki},
   doi = {10.1109/ICASSP.2018.8462533},
   isbn = {978-1-5386-4658-8},
   journal = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
   month = {4},
   pages = {1543-1547},
   publisher = {IEEE},
   title = {Unsupervised Image Segmentation by Backpropagation},
   year = {2018},
}
@article{Lee2019,
   abstract = {We propose a simple yet efficient anchor-free instance segmentation, called
CenterMask, that adds a novel spatial attention-guided mask (SAG-Mask) branch
to anchor-free one stage object detector (FCOS) in the same vein with Mask
R-CNN. Plugged into the FCOS object detector, the SAG-Mask branch predicts a
segmentation mask on each box with the spatial attention map that helps to
focus on informative pixels and suppress noise. We also present an improved
backbone networks, VoVNetV2, with two effective strategies: (1) residual
connection for alleviating the optimization problem of larger VoVNet
\cite\{lee2019energy\} and (2) effective Squeeze-Excitation (eSE) dealing with
the channel information loss problem of original SE. With SAG-Mask and
VoVNetV2, we deign CenterMask and CenterMask-Lite that are targeted to large
and small models, respectively. Using the same ResNet-101-FPN backbone,
CenterMask achieves 38.3%, surpassing all previous state-of-the-art methods
while at a much faster speed. CenterMask-Lite also outperforms the
state-of-the-art by large margins at over 35fps on Titan Xp. We hope that
CenterMask and VoVNetV2 can serve as a solid baseline of real-time instance
segmentation and backbone network for various vision tasks, respectively. The
Code is available at https://github.com/youngwanLEE/CenterMask.},
   author = {Youngwan Lee and Jongyoul Park},
   doi = {10.48550/arxiv.1911.06667},
   month = {11},
   title = {CenterMask : Real-Time Anchor-Free Instance Segmentation},
   url = {https://arxiv.org/abs/1911.06667},
   year = {2019},
}
@article{Zhao2016,
   abstract = {Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes.},
   author = {Hengshuang Zhao and Jianping Shi and Xiaojuan Qi and Xiaogang Wang and Jiaya Jia},
   doi = {10.48550/arxiv.1612.01105},
   month = {12},
   title = {Pyramid Scene Parsing Network},
   url = {http://arxiv.org/abs/1612.01105},
   year = {2016},
}
@article{Dieleman2015,
   author = {Sander Dieleman and Kyle W. Willett and Joni Dambre},
   doi = {10.1093/mnras/stv632},
   issn = {1365-2966},
   issue = {2},
   journal = {Monthly Notices of the Royal Astronomical Society},
   month = {6},
   pages = {1441-1459},
   title = {Rotation-invariant convolutional neural networks for galaxy morphology prediction},
   volume = {450},
   year = {2015},
}
@article{Ma2020,
   abstract = {Loss functions are one of the crucial ingredients in deep learning-based
medical image segmentation methods. Many loss functions have been proposed in
existing literature, but are studied separately or only investigated with few
other losses. In this paper, we present a systematic taxonomy to sort existing
loss functions into four meaningful categories. This helps to reveal links and
fundamental similarities between them. Moreover, we explore the relationship
between the traditional region-based and the more recent boundary-based loss
functions. The PyTorch implementations of these loss functions are publicly
available at \url\{https://github.com/JunMa11/SegLoss\}.},
   author = {Jun Ma},
   doi = {10.48550/arxiv.2005.13449},
   month = {5},
   title = {Segmentation Loss Odyssey},
   url = {https://arxiv.org/abs/2005.13449},
   year = {2020},
}
@article{,
   abstract = {Being able to learn dense semantic representations of images without
supervision is an important problem in computer vision. However, despite its
significance, this problem remains rather unexplored, with a few exceptions
that considered unsupervised semantic segmentation on small-scale datasets with
a narrow visual domain. In this paper, we make a first attempt to tackle the
problem on datasets that have been traditionally utilized for the supervised
case. To achieve this, we introduce a two-step framework that adopts a
predetermined mid-level prior in a contrastive optimization objective to learn
pixel embeddings. This marks a large deviation from existing works that relied
on proxy tasks or end-to-end clustering. Additionally, we argue about the
importance of having a prior that contains information about objects, or their
parts, and discuss several possibilities to obtain such a prior in an
unsupervised manner. Experimental evaluation shows that our method comes with key advantages over
existing works. First, the learned pixel embeddings can be directly clustered
in semantic groups using K-Means on PASCAL. Under the fully unsupervised
setting, there is no precedent in solving the semantic segmentation task on
such a challenging benchmark. Second, our representations can improve over
strong baselines when transferred to new datasets, e.g. COCO and DAVIS. The
code is available.},
   author = {Wouter Van Gansbeke and Simon Vandenhende and Stamatios Georgoulis and Luc Van Gool},
   doi = {10.48550/arxiv.2102.06191},
   month = {2},
   title = {Unsupervised Semantic Segmentation by Contrasting Object Mask Proposals},
   url = {https://arxiv.org/abs/2102.06191},
   year = {2021},
}
@article{Wang2021,
   abstract = {Current semantic segmentation methods focus only on mining "local" context,
i.e., dependencies between pixels within individual images, by
context-aggregation modules (e.g., dilated convolution, neural attention) or
structure-aware optimization criteria (e.g., IoU-like loss). However, they
ignore "global" context of the training data, i.e., rich semantic relations
between pixels across different images. Inspired by the recent advance in
unsupervised contrastive representation learning, we propose a pixel-wise
contrastive framework for semantic segmentation in the fully supervised
setting. The core idea is to enforce pixel embeddings belonging to a same
semantic class to be more similar than embeddings from different classes. It
raises a pixel-wise metric learning paradigm for semantic segmentation, by
explicitly exploring the structures of labeled pixels, which were rarely
explored before. Our method can be effortlessly incorporated into existing
segmentation frameworks without extra overhead during testing. We
experimentally show that, with famous segmentation models (i.e., DeepLabV3,
HRNet, OCR) and backbones (i.e., ResNet, HR-Net), our method brings consistent
performance improvements across diverse datasets (i.e., Cityscapes,
PASCAL-Context, COCO-Stuff, CamVid). We expect this work will encourage our
community to rethink the current de facto training paradigm in fully supervised
semantic segmentation.},
   author = {Wenguan Wang and Tianfei Zhou and Fisher Yu and Jifeng Dai and Ender Konukoglu and Luc Van Gool},
   doi = {10.48550/arxiv.2101.11939},
   month = {1},
   title = {Exploring Cross-Image Pixel Contrast for Semantic Segmentation},
   url = {https://arxiv.org/abs/2101.11939},
   year = {2021},
}
@article{Ye2019,
   abstract = {This paper studies the unsupervised embedding learning problem, which requires an effective similarity measurement between samples in low-dimensional embedding space. Motivated by the positive concentrated and negative separated properties observed from category-wise supervised learning, we propose to utilize the instance-wise supervision to approximate these properties, which aims at learning data augmentation invariant and instance spread-out features. To achieve this goal, we propose a novel instance based softmax embedding method, which directly optimizes the `real' instance features on top of the softmax function. It achieves significantly faster learning speed and higher accuracy than all existing methods. The proposed method performs well for both seen and unseen testing categories with cosine similarity. It also achieves competitive performance even without pre-trained network over samples from fine-grained categories.},
   author = {Mang Ye and Xu Zhang and Pong C. Yuen and Shih-Fu Chang},
   doi = {10.48550/arxiv.1904.03436},
   month = {4},
   title = {Unsupervised Embedding Learning via Invariant and Spreading Instance Feature},
   url = {http://arxiv.org/abs/1904.03436},
   year = {2019},
}
@article{Chen2020,
   abstract = {Siamese networks have become a common structure in various recent models for unsupervised visual representation learning. These models maximize the similarity between two augmentations of one image, subject to certain conditions for avoiding collapsing solutions. In this paper, we report surprising empirical results that simple Siamese networks can learn meaningful representations even using none of the following: (i) negative sample pairs, (ii) large batches, (iii) momentum encoders. Our experiments show that collapsing solutions do exist for the loss and structure, but a stop-gradient operation plays an essential role in preventing collapsing. We provide a hypothesis on the implication of stop-gradient, and further show proof-of-concept experiments verifying it. Our "SimSiam" method achieves competitive results on ImageNet and downstream tasks. We hope this simple baseline will motivate people to rethink the roles of Siamese architectures for unsupervised representation learning. Code will be made available.},
   author = {Xinlei Chen and Kaiming He},
   doi = {10.48550/arxiv.2011.10566},
   month = {11},
   title = {Exploring Simple Siamese Representation Learning},
   url = {http://arxiv.org/abs/2011.10566},
   year = {2020},
}
@book{Szeliski2022,
   author = {Richard Szeliski},
   city = {Cham},
   doi = {10.1007/978-3-030-34372-9},
   isbn = {978-3-030-34371-2},
   publisher = {Springer International Publishing},
   title = {Computer Vision},
   year = {2022},
}
@book{Dougherty2009,
   author = {Geoff Dougherty},
   doi = {10.1017/CBO9780511609657},
   isbn = {9780521860857},
   month = {4},
   publisher = {Cambridge University Press},
   title = {Digital Image Processing for Medical Applications},
   year = {2009},
}
@article{,
   author = {Tao An},
   doi = {10.1007/s11433-018-9360-x},
   title = {Science opportunities and challenges associated with SKA big data},
   url = {https://doi.org/10.1007/s11433-018-9360-x},
}
@article{York2000,
   author = {Donald G. York and J. Adelman and John E. Anderson, Jr. and Scott F. Anderson and James Annis and Neta A. Bahcall and J. A. Bakken and Robert Barkhouser and Steven Bastian and Eileen Berman and William N. Boroski and Steve Bracker and Charlie Briegel and John W. Briggs and J. Brinkmann and Robert Brunner and Scott Burles and Larry Carey and Michael A. Carr and Francisco J. Castander and Bing Chen and Patrick L. Colestock and A. J. Connolly and J. H. Crocker and István Csabai and Paul C. Czarapata and John Eric Davis and Mamoru Doi and Tom Dombeck and Daniel Eisenstein and Nancy Ellman and Brian R. Elms and Michael L. Evans and Xiaohui Fan and Glenn R. Federwitz and Larry Fiscelli and Scott Friedman and Joshua A. Frieman and Masataka Fukugita and Bruce Gillespie and James E. Gunn and Vijay K. Gurbani and Ernst de Haas and Merle Haldeman and Frederick H. Harris and J. Hayes and Timothy M. Heckman and G. S. Hennessy and Robert B. Hindsley and Scott Holm and Donald J. Holmgren and Chi-hao Huang and Charles Hull and Don Husby and Shin-Ichi Ichikawa and Takashi Ichikawa and Željko Ivezić and Stephen Kent and Rita S. J. Kim and E. Kinney and Mark Klaene and A. N. Kleinman and S. Kleinman and G. R. Knapp and John Korienek and Richard G. Kron and Peter Z. Kunszt and D. Q. Lamb and B. Lee and R. French Leger and Siriluk Limmongkol and Carl Lindenmeyer and Daniel C. Long and Craig Loomis and Jon Loveday and Rich Lucinio and Robert H. Lupton and Bryan MacKinnon and Edward J. Mannery and P. M. Mantsch and Bruce Margon and Peregrine McGehee and Timothy A. McKay and Avery Meiksin and Aronne Merelli and David G. Monet and Jeffrey A. Munn and Vijay K. Narayanan and Thomas Nash and Eric Neilsen and Rich Neswold and Heidi Jo Newberg and R. C. Nichol and Tom Nicinski and Mario Nonino and Norio Okada and Sadanori Okamura and Jeremiah P. Ostriker and Russell Owen and A. George Pauls and John Peoples and R. L. Peterson and Donald Petravick and Jeffrey R. Pier and Adrian Pope and Ruth Pordes and Angela Prosapio and Ron Rechenmacher and Thomas R. Quinn and Gordon T. Richards and Michael W. Richmond and Claudio H. Rivetta and Constance M. Rockosi and Kurt Ruthmansdorfer and Dale Sandford and David J. Schlegel and Donald P. Schneider and Maki Sekiguchi and Gary Sergey and Kazuhiro Shimasaku and Walter A. Siegmund and Stephen Smee and J. Allyn Smith and S. Snedden and R. Stone and Chris Stoughton and Michael A. Strauss and Christopher Stubbs and Mark SubbaRao and Alexander S. Szalay and Istvan Szapudi and Gyula P. Szokoly and Anirudda R. Thakar and Christy Tremonti and Douglas L. Tucker and Alan Uomoto and Dan Vanden Berk and Michael S. Vogeley and Patrick Waddell and Shu-i Wang and Masaru Watanabe and David H. Weinberg and Brian Yanny and Naoki Yasuda},
   doi = {10.1086/301513},
   issn = {00046256},
   issue = {3},
   journal = {The Astronomical Journal},
   month = {9},
   pages = {1579-1587},
   title = {The Sloan Digital Sky Survey: Technical Summary},
   volume = {120},
   url = {https://iopscience.iop.org/article/10.1086/301513},
   year = {2000},
}
@article{Albareti2017,
   author = {Franco D. Albareti and Carlos Allende Prieto and Andres Almeida and Friedrich Anders and Scott Anderson and Brett H. Andrews and Alfonso Aragón-Salamanca and Maria Argudo-Fernández and Eric Armengaud and Eric Aubourg and Vladimir Avila-Reese and Carles Badenes and Stephen Bailey and Beatriz Barbuy and Kat Barger and Jorge Barrera-Ballesteros and Curtis Bartosz and Sarbani Basu and Dominic Bates and Giuseppina Battaglia and Falk Baumgarten and Julien Baur and Julian Bautista and Timothy C. Beers and Francesco Belfiore and Matthew Bershady and Sara Bertran de Lis and Jonathan C. Bird and Dmitry Bizyaev and Guillermo A. Blanc and Michael Blanton and Michael Blomqvist and Adam S. Bolton and J. Borissova and Jo Bovy and William Nielsen Brandt and Jonathan Brinkmann and Joel R. Brownstein and Kevin Bundy and Etienne Burtin and Nicolás G. Busca and Hugo Orlando Camacho Chavez and M. Cano Díaz and Michele Cappellari and Ricardo Carrera and Yanping Chen and Brian Cherinka and Edmond Cheung and Cristina Chiappini and Drew Chojnowski and Chia-Hsun Chuang and Haeun Chung and Rafael Fernando Cirolini and Nicolas Clerc and Roger E. Cohen and Julia M. Comerford and Johan Comparat and Janaina Correa do Nascimento and Marie-Claude Cousinou and Kevin Covey and Jeffrey D. Crane and Rupert Croft and Katia Cunha and Jeremy Darling and James W. Davidson and Kyle Dawson and Luiz Da Costa and Gabriele Da Silva Ilha and Alice Deconto Machado and Timothée Delubac and Nathan De Lee and Axel De la Macorra and Sylvain De la Torre and Aleksandar M. Diamond-Stanic and John Donor and Juan Jose Downes and Niv Drory and Cheng Du and Hélion Du Mas des Bourboux and Tom Dwelly and Garrett Ebelke and Arthur Eigenbrot and Daniel J. Eisenstein and Yvonne P. Elsworth and Eric Emsellem and Michael Eracleous and Stephanie Escoffier and Michael L. Evans and Jesús Falcón-Barroso and Xiaohui Fan and Ginevra Favole and Emma Fernandez-Alvar and J. G. Fernandez-Trincado and Diane Feuillet and Scott W. Fleming and Andreu Font-Ribera and Gordon Freischlad and Peter Frinchaboy and Hai Fu and Yang Gao and Rafael A. Garcia and R. Garcia-Dias and D. A. Garcia-Hernández and Ana E. Garcia Pérez and Patrick Gaulme and Junqiang Ge and Douglas Geisler and Bruce Gillespie and Hector Gil Marin and Léo Girardi and Daniel Goddard and Yilen Gomez Maqueo Chew and Violeta Gonzalez-Perez and Kathleen Grabowski and Paul Green and Catherine J. Grier and Thomas Grier and Hong Guo and Julien Guy and Alex Hagen and Matt Hall and Paul Harding and R. E. Harley and Sten Hasselquist and Suzanne Hawley and Christian R. Hayes and Fred Hearty and Saskia Hekker and Hector Hernandez Toledo and Shirley Ho and David W. Hogg and Kelly Holley-Bockelmann and Jon A. Holtzman and Parker H. Holzer and Jian Hu and Daniel Huber and Timothy Alan Hutchinson and Ho Seong Hwang and Héctor J. Ibarra-Medel and Inese I. Ivans and KeShawn Ivory and Kurt Jaehnig and Trey W. Jensen and Jennifer A. Johnson and Amy Jones and Eric Jullo and T. Kallinger and Karen Kinemuchi and David Kirkby and Mark Klaene and Jean-Paul Kneib and Juna A. Kollmeier and Ivan Lacerna and Richard R. Lane and Dustin Lang and Pierre Laurent and David R. Law and Alexie Leauthaud and Jean-Marc Le Goff and Chen Li and Cheng Li and Niu Li and Ran Li and Fu-Heng Liang and Yu Liang and Marcos Lima and Lihwai Lin and Lin Lin and Yen-Ting Lin and Chao Liu and Dan Long and Sara Lucatello and Nicholas MacDonald and Chelsea L. MacLeod and J. Ted Mackereth and Suvrath Mahadevan and Marcio Antonio Geimba Maia and Roberto Maiolino and Steven R. Majewski and Olena Malanushenko and Viktor Malanushenko and Nícolas Dullius Mallmann and Arturo Manchado and Claudia Maraston and Rui Marques-Chaves and Inma Martinez Valpuesta and Karen L. Masters and Savita Mathur and Ian D. McGreer and Andrea Merloni and Michael R. Merrifield and Szabolcs Meszáros and Andres Meza and Andrea Miglio and Ivan Minchev and Karan Molaverdikhani and Antonio D. Montero-Dorta and Benoit Mosser and Demitri Muna and Adam Myers and Preethi Nair and Kirpal Nandra and Melissa Ness and Jeffrey A. Newman and Robert C. Nichol and David L. Nidever and Christian Nitschelm and Julia O’Connell and Audrey Oravetz and Daniel J. Oravetz and Zachary Pace and Nelson Padilla and Nathalie Palanque-Delabrouille and Kaike Pan and John Parejko and Isabelle Paris and Changbom Park and John A. Peacock and Sebastien Peirani and Marcos Pellejero-Ibanez and Samantha Penny and Will J. Percival and Jeffrey W. Percival and Ismael Perez-Fournon and Patrick Petitjean and Matthew Pieri and Marc H. Pinsonneault and Alice Pisani and Francisco Prada and Abhishek Prakash and Natalie Price-Jones and M. Jordan Raddick and Mubdi Rahman and Anand Raichoor and Sandro Barboza Rembold and A. M. Reyna and James Rich and Hannah Richstein and Jethro Ridl and Rogemar A. Riffel and Rogério Riffel and Hans-Walter Rix and Annie C. Robin and Constance M. Rockosi and Sergio Rodríguez-Torres and Thaíse S. Rodrigues and Natalie Roe and A. Roman Lopes and Carlos Román-Zúñiga and Ashley J. Ross and Graziano Rossi and John Ruan and Rossana Ruggeri and Jessie C. Runnoe and Salvador Salazar-Albornoz and Mara Salvato and Sebastian F. Sanchez and Ariel G. Sanchez and José R. Sanchez-Gallego and Basílio Xavier Santiago and Ricardo Schiavon and Jaderson S. Schimoia and Eddie Schlafly and David J. Schlegel and Donald P. Schneider and Ralph Schönrich and Mathias Schultheis and Axel Schwope and Hee-Jong Seo and Aldo Serenelli and Branimir Sesar and Zhengyi Shao and Matthew Shetrone and Michael Shull and Victor Silva Aguirre and M. F. Skrutskie and Anže Slosar and Michael Smith and Verne V. Smith and Jennifer Sobeck and Garrett Somers and Diogo Souto and David V. Stark and Keivan G. Stassun and Matthias Steinmetz and Dennis Stello and Thaisa Storchi Bergmann and Michael A. Strauss and Alina Streblyanska and Guy S. Stringfellow and Genaro Suarez and Jing Sun and Manuchehr Taghizadeh-Popp and Baitian Tang and Charling Tao and Jamie Tayar and Mita Tembe and Daniel Thomas and Jeremy Tinker and Rita Tojeiro and Christy Tremonti and Nicholas Troup and Jonathan R. Trump and Eduardo Unda-Sanzana and O. Valenzuela and Remco Van den Bosch and Mariana Vargas-Magaña and Jose Alberto Vazquez and Sandro Villanova and M. Vivek and Nicole Vogt and David Wake and Rene Walterbos and Yuting Wang and Enci Wang and Benjamin Alan Weaver and Anne-Marie Weijmans and David H. Weinberg and Kyle B. Westfall and David G. Whelan and Eric Wilcots and Vivienne Wild and Rob A. Williams and John Wilson and W. M. Wood-Vasey and Dominika Wylezalek and Ting Xiao and Renbin Yan and Meng Yang and Jason E. Ybarra and Christophe Yeche and Fang-Ting Yuan and Nadia Zakamska and Olga Zamora and Gail Zasowski and Kai Zhang and Cheng Zhao and Gong-Bo Zhao and Zheng Zheng and Zheng Zheng and Zhi-Min Zhou and Guangtun Zhu and Joel C. Zinn and Hu Zou},
   doi = {10.3847/1538-4365/aa8992},
   issn = {1538-4365},
   issue = {2},
   journal = {The Astrophysical Journal Supplement Series},
   month = {12},
   pages = {25},
   title = {The 13th Data Release of the Sloan Digital Sky Survey: First Spectroscopic Data from the SDSS-IV Survey Mapping Nearby Galaxies at Apache Point Observatory},
   volume = {233},
   url = {https://iopscience.iop.org/article/10.3847/1538-4365/aa8992},
   year = {2017},
}
@article{,
   abstract = {Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we formulate the semantic segmentation problem and define the terminology of this field as well as interesting background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and goals. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. We also devote a part of the paper to review common loss functions and error metrics for this problem. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.},
   author = {Alberto Garcia-Garcia and Sergio Orts-Escolano and Sergiu Oprea and Victor Villena-Martinez and Pablo Martinez-Gonzalez and Jose Garcia-Rodriguez},
   doi = {10.1016/J.ASOC.2018.05.018},
   issn = {1568-4946},
   journal = {Applied Soft Computing},
   keywords = {Deep learning,Scene labeling,Semantic segmentation},
   month = {9},
   pages = {41-65},
   publisher = {Elsevier},
   title = {A survey on deep learning techniques for image and video semantic segmentation},
   volume = {70},
   year = {2018},
}
@web_page{,
   author = {J.M. Ibáñez-Mengual},
   month = {9},
   title = {Repositorio Trabajo Fin de Máster DATCOM (UGR)},
   url = {https://github.com/ppmim/TFM},
   year = {2022},
}
@article{Walmsley2022,
   abstract = {New astronomical tasks are often related to earlier tasks for which labels
have already been collected. We adapt the contrastive framework BYOL to
leverage those labels as a pretraining task while also enforcing augmentation
invariance. For large-scale pretraining, we introduce GZ-Evo v0.1, a set of
96.5M volunteer responses for 552k galaxy images plus a further 1.34M
comparable unlabelled galaxies. Most of the 206 GZ-Evo answers are unknown for
any given galaxy, and so our pretraining task uses a Dirichlet loss that
naturally handles unknown answers. GZ-Evo pretraining, with or without hybrid
learning, improves on direct training even with plentiful downstream labels
(+4% accuracy with 44k labels). Our hybrid pretraining/contrastive method
further improves downstream accuracy vs. pretraining or contrastive learning,
especially in the low-label transfer regime (+6% accuracy with 750 labels).},
   author = {Mike Walmsley and Inigo Val Slijepcevic and Micah Bowles and Anna M. M. Scaife},
   doi = {10.48550/arxiv.2206.11927},
   month = {6},
   title = {Towards Galaxy Foundation Models with Hybrid Contrastive Learning},
   url = {https://arxiv.org/abs/2206.11927},
   year = {2022},
}
@article{Jadon2020,
   abstract = {Image Segmentation has been an active field of research as it has a wide
range of applications, ranging from automated disease detection to self-driving
cars. In the past five years, various papers came up with different objective
loss functions used in different cases such as biased data, sparse
segmentation, etc. In this paper, we have summarized some of the well-known
loss functions widely used for Image Segmentation and listed out the cases
where their usage can help in fast and better convergence of a model.
Furthermore, we have also introduced a new log-cosh dice loss function and
compared its performance on the NBFS skull-segmentation open-source data-set
with widely used loss functions. We also showcased that certain loss functions
perform well across all data-sets and can be taken as a good baseline choice in
unknown data distribution scenarios. Our code is available at Github:
https://github.com/shruti-jadon/Semantic-Segmentation-Loss-Functions.},
   author = {Shruti Jadon},
   doi = {10.1109/cibcb48159.2020.9277638},
   month = {6},
   title = {A survey of loss functions for semantic segmentation},
   url = {https://arxiv.org/abs/2006.14822},
   year = {2020},
}
@article{Weng2017,
   author = {Lilian Weng},
   journal = {lilianweng.github.io},
   title = {Object Detection for Dummies Part 3: R-CNN Family},
   url = {https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/},
   year = {2017},
}
@article{Goodfellow2014,
   abstract = {We propose a new framework for estimating generative models via an
adversarial process, in which we simultaneously train two models: a generative
model G that captures the data distribution, and a discriminative model D that
estimates the probability that a sample came from the training data rather than
G. The training procedure for G is to maximize the probability of D making a
mistake. This framework corresponds to a minimax two-player game. In the space
of arbitrary functions G and D, a unique solution exists, with G recovering the
training data distribution and D equal to 1/2 everywhere. In the case where G
and D are defined by multilayer perceptrons, the entire system can be trained
with backpropagation. There is no need for any Markov chains or unrolled
approximate inference networks during either training or generation of samples.
Experiments demonstrate the potential of the framework through qualitative and
quantitative evaluation of the generated samples.},
   author = {Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
   doi = {10.48550/arxiv.1406.2661},
   month = {6},
   title = {Generative Adversarial Networks},
   url = {https://arxiv.org/abs/1406.2661},
   year = {2014},
}
@article{Min2018,
   abstract = {\begin\{abstract\} Learning-based methods suffer from a deficiency of clean
annotations, especially in biomedical segmentation. Although many
semi-supervised methods have been proposed to provide extra training data,
automatically generated labels are usually too noisy to retrain models
effectively. In this paper, we propose a Two-Stream Mutual Attention Network
(TSMAN) that weakens the influence of back-propagated gradients caused by
incorrect labels, thereby rendering the network robust to unclean data. The
proposed TSMAN consists of two sub-networks that are connected by three types
of attention models in different layers. The target of each attention model is
to indicate potentially incorrect gradients in a certain layer for both
sub-networks by analyzing their inferred features using the same input. In
order to achieve this purpose, the attention models are designed based on the
propagation analysis of noisy gradients at different layers. This allows the
attention models to effectively discover incorrect labels and weaken their
influence during the parameter updating process. By exchanging multi-level
features within the two-stream architecture, the effects of noisy labels in
each sub-network are reduced by decreasing the updating gradients. Furthermore,
a hierarchical distillation is developed to provide more reliable pseudo labels
for unlabelded data, which further boosts the performance of our retrained
TSMAN. The experiments using both the HVSMR 2016 and BRATS 2015 benchmarks
demonstrate that our semi-supervised learning framework surpasses the
state-of-the-art fully-supervised results.},
   author = {Shaobo Min and Xuejin Chen and Zheng-Jun Zha and Feng Wu and Yongdong Zhang},
   doi = {10.48550/arxiv.1807.11719},
   month = {7},
   title = {A Two-Stream Mutual Attention Network for Semi-supervised Biomedical Segmentation with Noisy Labels},
   url = {https://arxiv.org/abs/1807.11719},
   year = {2018},
}
@inproceedings{Souly2017,
   abstract = {Semantic segmentation has been a long standing challenging task in computer vision. It aims at assigning a label to each image pixel and needs a significant number of pixel-level annotated data, which is often unavailable. To address this lack of annotations, in this paper, we leverage, on one hand, a massive amount of available unlabeled or weakly labeled data, and on the other hand, non-real images created through Generative Adversarial Networks. In particular, we propose a semi-supervised framework - based on Generative Adversarial Networks (GANs) - which consists of a generator network to provide extra training examples to a multi-class classifier, acting as discriminator in the GAN framework, that assigns sample a label y from the K possible classes or marks it as a fake sample (extra class). The underlying idea is that adding large fake visual data forces real samples to be close in the feature space, which, in turn, improves multiclass pixel classification. To ensure a higher quality of generated images by GANs with consequently improved pixel classification, we extend the above framework by adding weakly annotated data, i.e., we provide class level information to the generator. We test our approaches on several challenging benchmarking visual datasets, i.e. PASCAL, SiftFLow, Stanford and CamVid, achieving competitive performance compared to state-of-the-art semantic segmentation methods.},
   author = {N. Souly and C. Spampinato and M. Shah},
   doi = {10.1109/ICCV.2017.606},
   isbn = {9781538610329},
   journal = {Proceedings of the IEEE International Conference on Computer Vision},
   pages = {5689-5697},
   title = {Semi Supervised Semantic Segmentation Using Generative Adversarial Network},
   volume = {2017-Octob},
   year = {2017},
}
@inproceedings{Radosavovic2018,
   abstract = {We investigate omni-supervised learning, a special regime of semi-supervised learning in which the learner exploits all available labeled data plus internet-scale sources of unlabeled data. Omni-supervised learning is lower-bounded by performance on existing labeled datasets, offering the potential to surpass state-of-the-art fully supervised methods. To exploit the omni-supervised setting, we propose data distillation, a method that ensembles predictions from multiple transformations of unlabeled data, using a single model, to automatically generate new training annotations. We argue that visual recognition models have recently become accurate enough that it is now possible to apply classic ideas about self-training to challenging real-world data. Our experimental results show that in the cases of human keypoint detection and general object detection, state-of-the-art models trained with data distillation surpass the performance of using labeled data from the COCO dataset alone.},
   author = {I. Radosavovic and P. Dollar and R. Girshick and G. Gkioxari and K. He},
   doi = {10.1109/CVPR.2018.00433},
   isbn = {9781538664209},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   pages = {4119-4128},
   title = {Data Distillation: Towards Omni-Supervised Learning},
   year = {2018},
}
@book_section{Baur2017,
   author = {Christoph Baur and Shadi Albarqouni and Nassir Navab},
   doi = {10.1007/978-3-319-66179-7_36},
   pages = {311-319},
   title = {Semi-supervised Deep Learning for Fully Convolutional Networks},
   year = {2017},
}
@inproceedings{Lin2016,
   abstract = {Large-scale data is of crucial importance for learning semantic segmentation models, but annotating per-pixel masks is a tedious and inefficient procedure. We note that for the topic of interactive image segmentation, scribbles are very widely used in academic research and commercial software, and are recognized as one of the most userfriendly ways of interacting. In this paper, we propose to use scribbles to annotate images, and develop an algorithm to train convolutional networks for semantic segmentation supervised by scribbles. Our algorithm is based on a graphical model that jointly propagates information from scribbles to unmarked pixels and learns network parameters. We present competitive object semantic segmentation results on the PASCAL VOC dataset by using scribbles as annotations. Scribbles are also favored for annotating stuff (e.g., water, sky, grass) that has no well-defined shape, and our method shows excellent results on the PASCALCONTEXT dataset thanks to extra inexpensive scribble annotations. Our scribble annotations on PASCAL VOC are available at http://research.microsoft.com/en-us/um/people/jifdai/downloads/scribble-sup.},
   author = {D. Lin and J. Dai and J. Jia and K. He and J. Sun},
   doi = {10.1109/CVPR.2016.344},
   isbn = {9781467388504},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   pages = {3159-3167},
   title = {ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation},
   volume = {2016-Decem},
   year = {2016},
}
@article{Rajchl2017,
   abstract = {In this paper, we propose DeepCut, a method to obtain pixelwise object segmentations given an image dataset labelled weak annotations, in our case bounding boxes. It extends the approach of the well-known GrabCut [1] method to include machine learning by training a neural network classifier from bounding box annotations. We formulate the problem as an energy minimisation problem over a densely-connected conditional random field and iteratively update the training targets to obtain pixelwise object segmentations. Additionally, we propose variants of the DeepCut method and compare those to a naïve approach to CNN training under weak supervision. We test its applicability to solve brain and lung segmentation problems on a challenging fetal magnetic resonance dataset and obtain encouraging results in terms of accuracy.},
   author = {M. Rajchl and M.C.H. Lee and O. Oktay and K. Kamnitsas and J. Passerat-Palmbach and W. Bai and M. Damodaram and M.A. Rutherford and J.V. Hajnal and B. Kainz and B. Kainz and D. Rueckert},
   doi = {10.1109/TMI.2016.2621185},
   issue = {2},
   journal = {IEEE Transactions on Medical Imaging},
   pages = {674-683},
   title = {DeepCut: Object Segmentation from Bounding Box Annotations Using Convolutional Neural Networks},
   volume = {36},
   year = {2017},
}
@article{Kervadec2019,
   abstract = {Weakly-supervised learning based on, e.g., partially labelled images or image-tags, is currently attracting significant attention in CNN segmentation as it can mitigate the need for full and laborious pixel/voxel annotations. Enforcing high-order (global) inequality constraints on the network output (for instance, to constrain the size of the target region) can leverage unlabeled data, guiding the training process with domain-specific knowledge. Inequality constraints are very flexible because they do not assume exact prior knowledge. However, constrained Lagrangian dual optimization has been largely avoided in deep networks, mainly for computational tractability reasons. To the best of our knowledge, the method of Pathak et al. (2015a) is the only prior work that addresses deep CNNs with linear constraints in weakly supervised segmentation. It uses the constraints to synthesize fully-labeled training masks (proposals) from weak labels, mimicking full supervision and facilitating dual optimization. We propose to introduce a differentiable penalty, which enforces inequality constraints directly in the loss function, avoiding expensive Lagrangian dual iterates and proposal generation. From constrained-optimization perspective, our simple penalty-based approach is not optimal as there is no guarantee that the constraints are satisfied. However, surprisingly, it yields substantially better results than the Lagrangian-based constrained CNNs in Pathak et al. (2015a), while reducing the computational demand for training. By annotating only a small fraction of the pixels, the proposed approach can reach a level of segmentation performance that is comparable to full supervision on three separate tasks. While our experiments focused on basic linear constraints such as the target-region size and image tags, our framework can be easily extended to other non-linear constraints, e.g., invariant shape moments (Klodt and Cremers, 2011) and other region statistics (Lim et al., 2014). Therefore, it has the potential to close the gap between weakly and fully supervised learning in semantic medical image segmentation. Our code is publicly available.},
   author = {H. Kervadec and J. Dolz and M. Tang and E. Granger and Y. Boykov and I. Ben Ayed},
   doi = {10.1016/j.media.2019.02.009},
   journal = {Medical Image Analysis},
   pages = {88-99},
   title = {Constrained-CNN losses for weakly supervised segmentation},
   volume = {54},
   year = {2019},
}
@inproceedings{Papandreou2015,
   abstract = {Deep convolutional neural networks (DCNNs) trained on a large number of images with strong pixel-level annotations have recently significantly pushed the state-of-art in semantic image segmentation. We study the more challenging problem of learning DCNNs for semantic image segmentation from either (1) weakly annotated training data such as bounding boxes or image-level labels or (2) a combination of few strongly labeled and many weakly labeled images, sourced from one or multiple datasets. We develop Expectation-Maximization (EM) methods for semantic image segmentation model training under these weakly supervised and semi-supervised settings. Extensive experimental evaluation shows that the proposed techniques can learn models delivering competitive results on the challenging PASCAL VOC 2012 image segmentation benchmark, while requiring significantly less annotation effort. We share source code implementing the proposed system at https://bitbucket.org/deeplab/deeplab-public.},
   author = {G. Papandreou and L.-C. Chen and K.P. Murphy and A.L. Yuille},
   doi = {10.1109/ICCV.2015.203},
   isbn = {9781467383912},
   journal = {Proceedings of the IEEE International Conference on Computer Vision},
   pages = {1742-1750},
   title = {Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation},
   volume = {2015 Inter},
   year = {2015},
}
@article{Pan2021,
   abstract = {This work addresses weakly-supervised image semantic segmentation based on image-level class labels. One common approach to this task is to propagate the activation scores of Class Activation Maps (CAMs) using a random-walk mechanism in order to arrive at complete pseudo labels for training a semantic segmentation network in a fully-supervised manner. However, the feed-forward nature of the random walk imposes no regularization on the quality of the resulting complete pseudo labels. To overcome this issue, we propose a Graph Convolutional Network (GCN)-based feature propagation framework. We formulate the generation of complete pseudo labels as a semi-supervised learning task and learn a 2-layer GCN separately for every training image by back-propagating a Laplacian and an entropy regularization loss. Experimental results on the PASCAL VOC 2012 dataset confirm the superiority of our scheme to several state-of-the-art baselines. Our code is available at https://github.com/Xavier-Pan/WSGCN.},
   author = {Shun-Yi Pan and Cheng-You Lu and Shih-Po Lee and Wen-Hsiao Peng},
   doi = {10.48550/arxiv.2103.16762},
   month = {3},
   title = {Weakly-Supervised Image Semantic Segmentation Using Graph Convolutional Networks},
   url = {http://arxiv.org/abs/2103.16762},
   year = {2021},
}
@inproceedings{Dai2015,
   abstract = {Recent leading approaches to semantic segmentation rely on deep convolutional networks trained with human-annotated, pixel-level segmentation masks. Such pixel-accurate supervision demands expensive labeling effort and limits the performance of deep networks that usually benefit from more training data. In this paper, we propose a method that achieves competitive accuracy but only requires easily obtained bounding box annotations. The basic idea is to iterate between automatically generating region proposals and training convolutional networks. These two steps gradually recover segmentation masks for improving the networks, and vise versa. Our method, called "BoxSup", produces competitive results (e.g., 62.0% mAP for validation) supervised by boxes only, on par with strong baselines (e.g., 63.8% mAP) fully supervised by masks under the same setting. By leveraging a large amount of bounding boxes, BoxSup further yields state-of-the-art results on PASCAL VOC 2012 and PASCAL-CONTEXT [26].},
   author = {J. Dai and K. He and J. Sun},
   doi = {10.1109/ICCV.2015.191},
   isbn = {9781467383912},
   journal = {Proceedings of the IEEE International Conference on Computer Vision},
   pages = {1635-1643},
   title = {BoxSup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation},
   volume = {2015 Inter},
   year = {2015},
}
@inproceedings{Vezhnevets2010,
   abstract = {We address the task of learning a semantic segmentation from weakly supervised data. Our aim is to devise a system that predicts an object label for each pixel by making use of only image level labels during training - the information whether a certain object is present or not in the image. Such coarse tagging of images is faster and easier to obtain as opposed to the tedious task of pixelwise labeling required in state of the art systems. We cast this task naturally as a multiple instance learning (MIL) problem. We use Semantic Texton Forest (STF) as the basic framework and extend it for the MIL setting. We make use of multitask learning (MTL) to regularize our solution. Here, an external task of geometric context estimation is used to improve on the task of semantic segmentation. We report experimental results on the MSRC21 and the very challenging VOC2007 datasets. On MSRC21 dataset we are able, by using 276 weakly labeled images, to achieve the performance of a supervised STF trained on pixelwise labeled training set of 56 images, which is a significant reduction in supervision needed. ©2010 IEEE.},
   author = {A. Vezhnevets and J.M. Buhmann},
   doi = {10.1109/CVPR.2010.5540060},
   isbn = {9781424469840},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   pages = {3249-3256},
   title = {Towards weakly supervised semantic segmentation by means of multiple instance and multitask learning},
   year = {2010},
}
@article{Zhang2019,
   abstract = {Cosmic ray (CR) identification and replacement are critical components of imaging and spectroscopic reduction pipelines involving solid-state detectors. We present deepCR, a deep learning based framework for CR identification and subsequent image inpainting based on the predicted CR mask. To demonstrate the effectiveness of this framework, we train and evaluate models on Hubble Space Telescope ACS/WFC images of sparse extragalactic fields, globular clusters, and resolved galaxies. We demonstrate that at a false positive rate of 0.5%, deepCR achieves close to 100% detection rates in both extragalactic and globular cluster fields, and 91% in resolved galaxy fields, which is a significant improvement over the current state-of-the-art method LACosmic. Compared to a multicore CPU implementation of LACosmic, deepCR CR mask predictions run up to 6.5 times faster on CPU and 90 times faster on a single GPU. For image inpainting, the mean squared errors of deepCR predictions are 20 times lower in globular cluster fields, 5 times lower in resolved galaxy fields, and 2.5 times lower in extragalactic fields, compared to the best performing non-neural technique tested. We present our framework and the trained models as an open-source Python project, with a simple-to-use API. To facilitate reproducibility of the results we also provide a benchmarking codebase.},
   author = {Keming Zhang and Joshua S. Bloom},
   doi = {10.3847/1538-4357/ab3fa6},
   month = {7},
   title = {deepCR: Cosmic Ray Rejection with Deep Learning},
   url = {http://arxiv.org/abs/1907.09500 http://dx.doi.org/10.3847/1538-4357/ab3fa6},
   year = {2019},
}
@article{Reiman2018,
   abstract = {Near-future large galaxy surveys will encounter blended galaxy images at a fraction of up to 50% in the densest regions of the universe. Current deblending techniques may segment the foreground galaxy while leaving missing pixel intensities in the background galaxy flux. The problem is compounded by the diffuse nature of galaxies in their outer regions, making segmentation significantly more difficult than in traditional object segmentation applications. We propose a novel branched generative adversarial network (GAN) to deblend overlapping galaxies, where the two branches produce images of the two deblended galaxies. We show that generative models are a powerful engine for deblending given their innate ability to infill missing pixel values occluded by the superposition. We maintain high peak signal-to-noise ratio and structural similarity scores with respect to ground truth images upon deblending. Our model also predicts near-instantaneously, making it a natural choice for the immense quantities of data soon to be created by large surveys such as LSST, Euclid and WFIRST.},
   author = {David M. Reiman and Brett E. Göhre},
   doi = {10.1093/mnras/stz575},
   month = {10},
   title = {Deblending galaxy superpositions with branched generative adversarial networks},
   url = {http://arxiv.org/abs/1810.10098 http://dx.doi.org/10.1093/mnras/stz575},
   year = {2018},
}
@book_section{Najman2003,
   author = {Laurent Najman and Michel Couprie},
   doi = {10.1007/978-3-540-39966-7_5},
   pages = {62-71},
   title = {Watershed Algorithms and Contrast Preservation},
   year = {2003},
}
@book_section{Ronneberger2015,
   author = {Olaf Ronneberger and Philipp Fischer and Thomas Brox},
   doi = {10.1007/978-3-319-24574-4_28},
   pages = {234-241},
   title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
   year = {2015},
}
@article{Lin2014,
   abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
   author = {Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Dollár},
   doi = {10.48550/arxiv.1405.0312},
   month = {5},
   title = {Microsoft COCO: Common Objects in Context},
   url = {http://arxiv.org/abs/1405.0312},
   year = {2014},
}
@article{Ren2015,
   abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
   author = {Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun},
   doi = {10.48550/arxiv.1506.01497},
   month = {6},
   title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
   url = {http://arxiv.org/abs/1506.01497},
   year = {2015},
}
@report{,
   abstract = {State-of-the-art approaches for semantic image segmen-tation are built on Convolutional Neural Networks (CNNs). The typical segmentation architecture is composed of (a) a downsampling path responsible for extracting coarse semantic features, followed by (b) an upsampling path trained to recover the input image resolution at the output of the model and, optionally, (c) a post-processing module (e.g. Conditional Random Fields) to refine the model predictions. Recently, a new CNN architecture, Densely Connected Convolutional Networks (DenseNets), has shown excellent results on image classification tasks. The idea of DenseNets is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train. In this paper, we extend DenseNets to deal with the problem of semantic segmentation. We achieve state-of-the-art results on urban scene benchmark datasets such as CamVid and Gatech, without any further post-processing module nor pretraining. Moreover, due to smart construction of the model, our approach has much less parameters than currently published best entries for these datasets. Code to reproduce the experiments is publicly available here : https://github.com/SimJeg/FC-DenseNet},
   author = {Simon Jégou and Michal Drozdzal and David Vazquez and Adriana Romero and Yoshua Bengio},
   title = {The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation},
   url = {https://github.com/SimJeg/FC-DenseNet},
}
@article{,
   abstract = {This paper presents the application of artificial intelligence techniques to optical spectroscopy, a specific field of Astrophysics. We propose the analysis, design and implementation of an intelligent system for the analysis and classification of the low-resolution optical spectra of supergiant, giant and dwarf stars, with luminosity levels I, III and V, respectively. The developed system automatically and objectively collects the most important spectral features, and determines the temperature and luminosity of the stars according to the current standard system. The system development combines signal processing, expert systems and fuzzy logic techniques, and integrates them through the use of a relational database, which allows us to structure the collected astronomical data and to contrast the results of the different classification methods. As an additional research, we have designed and implemented several models of artificial neural networks, including them as an alternative method for the classification of spectra. © 2004 Elsevier Ltd. All rights reserved.},
   author = {A. Rodríguez and B. Arcay and C. Dafonte and M. Manteiga and I. Carricajo},
   doi = {10.1016/j.eswa.2004.01.007},
   issue = {2},
   journal = {Expert Systems with Applications},
   pages = {237-244},
   title = {Automated knowledge-based analysis and classification of stellar spectra using fuzzy reasoning},
   volume = {27},
   year = {2004},
}
@article{Capaccioli1988,
   abstract = {This study reports on the results of the application of an adaptive filtering technique to the two‐dimensional photometric mapping of galaxies. For this test we used the elliptical galaxy NGC 3379, which is a luminosity standard. Our plate material (B‐band), obtained with the Tautenburg Schmidt telescope, was digitized with the Babelsberg microdensitometer and with the ESO PDS. The 2‐D scans were reduced according to the Babelsberg Image Processing Systems, whose key feature is an adaptive filter called ‘H‐Transform’. By comparing our results (E‐W light profile and isophotal parameters) to the most recent studies of NGC 3379 (DE VAUCOULEURS and CAPACCIOLI 1979, and NIETO and VIDAL 1984), we found no systematic errors in the photometric calibration and in the reduction procedure, with typical mean residuals from the other studies not larger than ±0.05 B‐mag up to a threshold of 1% of the night sky level on each plate. We also re‐confirmed the geometrical properties of the galaxy. The new filtering technique proved quite efficient in reducing the noise, with negligible effects on the photometric and geometric characteristics of the star‐like sources. Copyright © 1988 WILEY‐VCH Verlag GmbH & Co. KGaA},
   author = {M. Capaccioli and E.V. Held and H. Lorenz and G.M. Richter and R. Ziener},
   doi = {10.1002/asna.2113090202},
   issue = {2},
   journal = {Astronomische Nachrichten},
   pages = {69-80},
   title = {Application of an adaptive filtering technique to surface photometry of galaxies. I. The method tested on NGC 3379},
   volume = {309},
   year = {1988},
}
@article{Torres2010,
   abstract = {This article presents and discusses a critical compilation of accurate, fundamental determinations of stellar masses and radii. We have identified 95 detached binary systems containing 190 stars (94 eclipsing systems, and α Centauri) that satisfy our criterion that the mass and radius of both stars be known within errors of ±3% accuracy or better. All of them are non-interacting systems, and so the stars should have evolved as if they were single. This sample more than doubles that of the earlier similar review by Andersen (Astron Astrophys Rev 3:91-126, 1991), extends the mass range at both ends and, for the first time, includes an extragalactic binary. In every case, we have examined the original data and recomputed the stellar parameters with a consistent set of assumptions and physical constants. To these we add interstellar reddening, effective temperature, metal abundance, rotational velocity and apsidal motion determinations when available, and we compute a number of other physical parameters, notably luminosity and distance. These accurate physical parameters reveal the effects of stellar evolution with unprecedented clarity, and we discuss the use of the data in observational tests of stellar evolution models in some detail. Earlier findings of significant structural differences between moderately fast-rotating, mildly active stars and single stars, ascribed to the presence of strong magnetic and spot activity, are confirmed beyond doubt. We also show how the best data can be used to test prescriptions for the subtle interplay between convection, diffusion, and other non-classical effects in stellar models. The amount and quality of the data also allow us to analyse the tidal evolution of the systems in considerable depth, testing prescriptions of rotational synchronisation and orbital circularisation in greater detail than possible before. We show that the formulae for pseudo-synchronisation of stars in eccentric orbits predict the observed rotations quite well, except for very young and/or widely separated stars. Deviations do occur, however, especially for stars with convective envelopes. The superior data set finally demonstrates that apsidal motion rates as predicted from General Relativity plus tidal theory are in good agreement with the best observational data. No reliable binary data exist, which challenge General Relativity to any significant extent. The new data also enable us to derive empirical calibrations of M and R for single (post-) main-sequence stars above 0.6 M⊙. Simple, polynomial functions of T eff, log g and [Fe/H] yield M and R within errors of 6 and 3%, respectively. Excellent agreement is found with independent determinations for host stars of transiting extrasolar planets, and good agreement with determinations of M and R from stellar models as constrained by trigonometric parallaxes and spectroscopic values of T eff and [Fe/H]. Finally, we list a set of 23 interferometric binaries with masses known to be better than 3%, but without fundamental radius determinations (except α Aur). We discuss the prospects for improving these and other stellar parameters in the near future. © 2009 Springer-Verlag.},
   author = {G. Torres and J. Andersen and A. Giménez},
   doi = {10.1007/s00159-009-0025-1},
   issue = {1-2},
   journal = {Astronomy and Astrophysics Review},
   pages = {67-126},
   title = {Accurate masses and radii of normal stars: Modern results and applications},
   volume = {18},
   year = {2010},
}
@article{Lintott2008,
   abstract = {In order to understand the formation and subsequent evolution of galaxies one must first distinguish between the two main morphological classes of massive systems: spirals and early-type systems. This paper introduces a project, Galaxy Zoo, which provides visual morphological classifications for nearly one million galaxies, extracted from the Sloan Digital Sky Survey (SDSS). This achievement was made possible by inviting the general public to visually inspect and classify these galaxies via the internet. The project has obtained more than 4 × 107 individual classifications made by ∼10 5 participants. We discuss the motivation and strategy for this project, and detail how the classifications were performed and processed. We find that Galaxy Zoo results are consistent with those for subsets of SDSS galaxies classified by professional astronomers, thus demonstrating that our data provide a robust morphological catalogue. Obtaining morphologies by direct visual inspection avoids introducing biases associated with proxies for morphology such as colour, concentration or structural parameters. In addition, this catalogue can be used to directly compare SDSS morphologies with older data sets. The colour-magnitude diagrams for each morphological class are shown, and we illustrate how these distributions differ from those inferred using colour alone as a proxy for morphology. © 2008 RAS.},
   author = {C.J. Lintott and K. Schawinski and A. Slosar and K. Land and S. Bamford and D. Thomas and M.J. Raddick and R.C. Nichol and A. Szalay and D. Andreescu and P. Murray and J. Vandenberg},
   doi = {10.1111/j.1365-2966.2008.13689.x},
   issue = {3},
   journal = {Monthly Notices of the Royal Astronomical Society},
   pages = {1179-1189},
   title = {Galaxy Zoo: Morphologies derived from visual inspection of galaxies from the Sloan Digital Sky Survey},
   volume = {389},
   year = {2008},
}
@article{Gulati1994,
   abstract = {Here we present a pattern classification technique based on an Artificial Neural Network (ANN) in a multi-level tree configuration to classify ultraviolet stellar spectra from the IUE Low-Dispersion Spectra Reference Atlas. Preliminary results of this technique show that 94% of the spectra have been classified correctly with an accuracy of one sub-class. A conventional χ2 minimization scheme has also been applied to the data to compare the classification obtained from these schemes with that of the IUE catalog classification. © 1995.},
   author = {R.K. Gulati and R. Gupta and P. Gothoskar and S. Khobragade},
   doi = {10.1016/0083-6656(94)90040-X},
   issue = {PART 3},
   journal = {Vistas in Astronomy},
   pages = {293-298},
   title = {Ultraviolet stellar spectral classification using a multilevel tree neural network},
   volume = {38},
   year = {1994},
}
@article{Ntampaka2015,
   abstract = {We present a modern machine learning (ML) approach for cluster dynamical mass measurements that is a factorof- two improvement over using a conventional scaling relation. Different methods are tested against a mock cluster catalog constructed using halos with mass - ≥1014 M⊙h-1 from Multidarks publicly available N-body MDPL halo catalog. In the conventional method, we use a standard M(σv) power-law scaling relation to infer cluster mass, M, from line of sight (LOS) galaxy velocity dispersion, σv. The resulting fractional mass error distribution is broad, with width Dδε≈ 0.87 (68% scatter), and has extended high-error tails. The standard scaling relation can be simply enhanced by including higher-order moments of the LOS velocity distribution. Applying the kurtosis as a correction term to log (sv ) reduces the width of the error distribution to D Dδε≈ 0.74 (16% improvement). ML can be used to take full advantage of all the information in the velocity distribution. We employ the Support Distribution Machines (SDMs) algorithm that learns from distributions of data to predict single values. SDMs trained and tested on the distribution of LOS velocities yield D Dδε≈ 0.46 (47% improvement). Furthermore, the problematic tails of the mass error distribution are effectively eliminated. Decreasing cluster mass errors will improve measurements of the growth of structure and lead to tighter constraints on cosmological parameters.},
   author = {M. Ntampaka and H. Trac and D.J. Sutherland and N. Battaglia and B. Póczos and J. Schneider},
   doi = {10.1088/0004-637X/803/2/50},
   issue = {2},
   journal = {Astrophysical Journal},
   title = {A machine learning approach for dynamical mass measurements of galaxy clusters},
   volume = {803},
   year = {2015},
}
@article{Nigam2000,
   abstract = {This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available. We introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However these assumptions are often violated in practice, and poor performance can result. We present two extensions to the algorithm that improve classification accuracy under these conditions: (1) a weighting factor to modulate the contribution of the unlabeled data, and (2) the use of multiple mixture components per class. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30%.},
   author = {K. Nigam and A.K. Mccallum and S. Thrun and T. Mitchell},
   doi = {10.1023/a:1007692713085},
   issue = {2},
   journal = {Machine Learning},
   pages = {103-134},
   title = {Text classification from labeled and unlabeled documents using EM},
   volume = {39},
   year = {2000},
}
@article{Malyuto2001,
   abstract = {We have simulated a set of "observed" spectra by combining synthetic 20 Å resolution spectra (Kurucz models) with random variables drawn from the standard Gaussian distribution. The simulated spectra have been classified using Cayrel's perturbation method for deriving the stellar parameters: effective temperature, gravity and metallicity. Then we have decreased spectral resolutions artificially step by step. The full range of resolutions (from 20 Å to lower resolution spectrophotometry, narrow-band, intermediate-band, and finally to broad-band photometry) has been covered, and the classification accuracies were estimated at every step. Useful features in the run of classification accuracies with spectral resolution have been noted and discussed. Selected photometric systems have been analyzed with the use of the same perturbation method, also. The results can help in designing optimized observing strategies in future spectral and photometric classification projects. © Elsevier Science B.V.},
   author = {V. Malyuto and R. Lazauskaite and T. Shvelidze},
   doi = {10.1016/S1384-1076(01)00066-5},
   issue = {6},
   journal = {New Astronomy},
   pages = {381-392},
   title = {Simulated quantitative stellar classification at different spectral resolutions},
   volume = {6},
   year = {2001},
}
@article{Hewish1968,
   abstract = {Unusual signals from pulsating radio sources have been recorded at the Mullard Radio Astronomy Observatory. The radiation seems to come from local objects within the galaxy, and may be associated with oscillations of white dwarf or neutron stars. © 1968 Nature Publishing Group.},
   author = {A. Hewish and S.J. Bell and J.D.H. Pilkington and P.F. Scott and R.A. Collins},
   doi = {10.1038/217709a0},
   issue = {5130},
   journal = {Nature},
   pages = {709-713},
   title = {Observation of a rapidly pulsating radio source},
   volume = {217},
   year = {1968},
}
@article{Eatough2010,
   abstract = {Radio pulsar surveys are producing many more pulsar candidates than can be inspected by human experts in a practical length of time. Here we present a technique to automatically identify credible pulsar candidates from pulsar surveys using an artificial neural network. The technique has been applied to candidates from a recent re-analysis of the Parkes multi-beam pulsar survey resulting in the discovery of a previously unidentified pulsar. © 2010 The Authors. Journal compilation. © 2010 RAS.},
   author = {R.P. Eatough and N. Molkenthin and M. Kramer and A. Noutsos and M.J. Keith and B.W. Stappers and A.G. Lyne},
   doi = {10.1111/j.1365-2966.2010.17082.x},
   issue = {4},
   journal = {Monthly Notices of the Royal Astronomical Society},
   pages = {2443-2450},
   title = {Selection of radio pulsar candidates using artificial neural networks},
   volume = {407},
   year = {2010},
}
@article{Guzella2009,
   abstract = {In this paper, we present a comprehensive review of recent developments in the application of machine learning algorithms to Spam filtering, focusing on both textual- and image-based approaches. Instead of considering Spam filtering as a standard classification problem, we highlight the importance of considering specific characteristics of the problem, especially concept drift, in designing new filters. Two particularly important aspects not widely recognized in the literature are discussed: the difficulties in updating a classifier based on the bag-of-words representation and a major difference between two early naive Bayes models. Overall, we conclude that while important advancements have been made in the last years, several aspects remain to be explored, especially under more realistic evaluation settings. © 2009 Elsevier Ltd. All rights reserved.},
   author = {T.S. Guzella and W.M. Caminhas},
   doi = {10.1016/j.eswa.2009.02.037},
   issue = {7},
   journal = {Expert Systems with Applications},
   pages = {10206-10222},
   title = {A review of machine learning approaches to Spam filtering},
   volume = {36},
   year = {2009},
}
@article{Salakhutdinov2009,
   abstract = {We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer "pre-training" phase that allows variational inference to be initialized with a single bottomup pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks. © 2009 by the authors.},
   author = {R. Salakhutdinov and G. Hinton},
   journal = {Journal of Machine Learning Research},
   pages = {448-455},
   title = {Deep Boltzmann machines},
   volume = {5},
   year = {2009},
}
@article{Richter1991,
   abstract = {An adaptive filter for processing of astronomical images is developed and described. The filter recognizes the local signal resolution (which usually varies strongly across the image) and adapts its own impulse response to this resolution. Copyright © 1991 WILEY‐VCH Verlag GmbH & Co. KGaA},
   author = {G.M. Richter and P. Böhm and H. Lorenz and A. Priebe and M. Capaccioli},
   doi = {10.1002/asna.2113120602},
   issue = {6},
   journal = {Astronomische Nachrichten},
   pages = {345-349},
   title = {Adaptive filtering in astronomical image processing},
   volume = {312},
   year = {1991},
}
@article{Brett2004,
   abstract = {We apply the technique of self-organizing maps to the automated classification of singly periodic astronomical light curves. We find that our maps readily distinguish between light curve types in both synthetic and real data sets, and that the resulting maps do not depend sensitively on the chosen learning parameters. Automated data analysis techniques are likely to be become increasingly important as the size of astronomical data sets continues to increase, particularly with the advent of ultra-wide-field survey telescopes such as WASP, RAPTOR and ASAS.},
   author = {D.R. Brett and R.G. West and P.J. Wheatley},
   doi = {10.1111/j.1365-2966.2004.08093.x},
   issue = {2},
   journal = {Monthly Notices of the Royal Astronomical Society},
   pages = {369-376},
   title = {The automated classification of astronomical light curves using Kohonen self-organizing maps},
   volume = {353},
   year = {2004},
}
@article{Morello2014,
   abstract = {We describe SPINN (Straightforward Pulsar Identification using Neural Networks), a highperformance machine learning solution developed to process increasingly large data outputs from pulsar surveys. SPINN has been cross-validated on candidates from the southern High Time Resolution Universe (HTRU) survey and shown to identify every known pulsar found in the survey data while maintaining a false positive rate of 0.64 per cent. Furthermore, it ranks 99 per cent of pulsars among the top 0.11 per cent of candidates, and 95 per cent among the top 0.01 per cent. In conjunction with the PEASOUP pipeline, it has already discovered four new pulsars in a re-processing of the intermediate Galactic latitude area of HTRU, three of which have spin periods shorter than 5 ms. SPINN's ability to reduce the amount of candidates to visually inspect by up to four orders of magnitude makes it a very promising tool for future large-scale pulsar surveys. In an effort to provide a common testing ground for pulsar candidate selection tools and stimulate interest in their development, we also make publicly available the set of candidates on which SPINN was cross-validated. © 2014 The Authors. Published by Oxford University Press on behalf of the Royal Astronomical Society.},
   author = {V. Morello and E.D. Barr and M. Bailes and C.M. Flynn and E.F. Keane and W. van Straten},
   doi = {10.1093/mnras/stu1188},
   issue = {2},
   journal = {Monthly Notices of the Royal Astronomical Society},
   pages = {1651-1662},
   title = {SPINN: A straightforward machine learning solution to the pulsar candidate selection problem},
   volume = {443},
   year = {2014},
}
@article{Hinton2006,
   abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind. © 2006 Massachusetts Institute of Technology.},
   author = {G.E. Hinton and S. Osindero and Y.-W. Teh},
   doi = {10.1162/neco.2006.18.7.1527},
   issue = {7},
   journal = {Neural Computation},
   pages = {1527-1554},
   title = {A fast learning algorithm for deep belief nets},
   volume = {18},
   year = {2006},
}
@article{Zhang2015,
   abstract = {The fields of Astrostatistics and Astroinformatics are vital for dealing with the big data issues now faced by astronomy. Like other disciplines in the big data era, astronomy has many V characteristics. In this paper, we list the different data mining algorithms used in astronomy, along with data mining software and tools related to astronomical applications. We present SDSS, a project often referred to by other astronomical projects, as the most successful sky survey in the history of astronomy and describe the factors influencing its success. We also discuss the success of Astrostatistics and Astroinformatics organizations and the conferences and summer schools on these issues that are held annually. All the above indicates that astronomers and scientists from other areas are ready to face the challenges and opportunities provided by massive data volume.},
   author = {Y. Zhang and Y. Zhao},
   doi = {10.5334/dsj-2015-011},
   journal = {Data Science Journal},
   title = {Astronomy in the big data era},
   volume = {14},
   year = {2015},
}
@article{Udalski2008,
   abstract = {We describe methods applied to the final photometric reductions and calibrations to the standard system of the images collected during the third phase of the Optical Gravitational Lensing Experiment survey - OGLE-III. Astrometric reduction methods are also presented. The OGLE-III data constitute a unique data set covering the Magellanic Clouds, Galactic bulge and Galactic disk fields monitored regularly every clear night since 2001 and being significant extension and continuation of the earlier OGLE observations. With the earlier OGLE-II and OGLE-I photometry some of the observed fields have now 16-year long photometric coverage.},
   author = {A. Udalski and M.K. Szymański and I. Soszyński and R. Poleski},
   issue = {2},
   journal = {Acta Astronomica},
   pages = {69-87},
   title = {The optical gravitational lensing experiment. Final reductions of the OGLE-III data},
   volume = {58},
   year = {2008},
}
@article{Bundy2015,
   abstract = {We present an overview of a new integral field spectroscopic survey called MaNGA (Mapping Nearby Galaxies at Apache Point Observatory), one of three core programs in the fourth-generation Sloan Digital Sky Survey (SDSS-IV) that began on 2014 July 1. MaNGA will investigate the internal kinematic structure and composition of gas and stars in an unprecedented sample of 10,000 nearby galaxies. We summarize essential characteristics of the instrument and survey design in the context of MaNGA's key science goals and present prototype observations to demonstrate MaNGA's scientific potential. MaNGA employs dithered observations with 17 fiber-bundle integral field units that vary in diameter from 12″ (19 fibers) to 32″ (127 fibers). Two dual-channel spectrographs provide simultaneous wavelength coverage over 3600-10300 A˚ at R ∼ 2000.With a typical integration time of 3 hr, MaNGA reaches a target r-band signal-to-noise ratio of 4-8 (A˚-1 per 2″ fiber) at 23 AB mag arcsec-2, which is typical for the outskirts of MaNGA galaxies. Targets are selected with M∗ ≳ 109 M⊙ using SDSS-I redshifts and i-band luminosity to achieve uniform radial coverage in terms of the effective radius, an approximately flat distribution in stellar mass, and a sample spanning a wide range of environments. Analysis of our prototype observations demonstrates MaNGA's ability to probe gas ionization, shed light on recent star formation and quenching, enable dynamical modeling, decompose constituent components, and map the composition of stellar populations.MaNGA's spatially resolved spectra will enable an unprecedented study of the astrophysics of nearby galaxies in the coming 6 yr.},
   author = {K. Bundy and M.A. Bershady and D.R. Law and R. Yan and N. Drory and N. MacDonald and D.A. Wake and B. Cherinka and J.R. Sánchez-Gallego and A.-M. Weijmans and T. Xiao and K. Zhang},
   doi = {10.1088/0004-637X/798/1/7},
   issue = {1},
   journal = {Astrophysical Journal},
   title = {Overview of the SDSS-IV MaNGA survey: Mapping Nearby Galaxies at Apache Point Observatory},
   volume = {798},
   year = {2015},
}
@article{Arel2010,
   abstract = {Mimicking the efficiency and robustness by which the human brain represents information has been a core challenge in artificial intelligence research for decades. Humans are exposed to myriad of sensory data received every second of the day and are somehow able to capture critical aspects of this data in a way that allows for its future use in a concise manner. Over 50 years ago, Richard Bellman, who introduced dynamic programming theory and pioneered the field of optimal control, asserted that high dimensionality of data is a fundamental hurdle in many science and engineering applications. The main difficulty that arises, particularly in the context of pattern classification applications, is that the learning complexity grows exponentially with linear increase in the dimensionality of the data. He coined this phenomenon the curse of dimensionality [1]. The mainstream approach of overcoming the curse has been to pre-process the data in a manner that would reduce its dimensionality to that which can be effectively processed, for example by a classification engine. This dimensionality reduction scheme is often referred to as feature extraction. As a result, it can be argued that the intelligence behind many pattern recognition systems has shifted to the human-engineered feature extraction process, which at times can be challenging and highly application-dependent [2]. Moreover, if incomplete or erroneous features are extracted, the classification process is inherently limited in performance. © 2006 IEEE.},
   author = {I. Arel and D. Rose and T. Karnowski},
   doi = {10.1109/MCI.2010.938364},
   issue = {4},
   journal = {IEEE Computational Intelligence Magazine},
   pages = {13-18},
   title = {Deep machine learning-A new frontier in artificial intelligence research},
   volume = {5},
   year = {2010},
}
@book{Edwards2014,
   author = {K.J. Edwards and M.M. Gaber},
   doi = {10.1007/978-3-319-06599-1_1},
   journal = {Studies in Big Data},
   pages = {1-104},
   title = {Astronomy and Big Data: A Data Clustering Approach to Identifying Uncertain Galaxy Morphology},
   volume = {6},
   year = {2014},
}
@article{Bengio2013,
   abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning. © 1979-2012 IEEE.},
   author = {Y. Bengio and A. Courville and P. Vincent},
   doi = {10.1109/TPAMI.2013.50},
   issue = {8},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   pages = {1798-1828},
   title = {Representation learning: A review and new perspectives},
   volume = {35},
   year = {2013},
}
@article{Devine2016,
   abstract = {Searching for extraterrestrial, transient signals in astronomical data sets is an active area of current research. However, machine learning techniques are lacking in the literature concerning single-pulse detection. This paper presents a new, two-stage approach for identifying and classifying dispersed pulse groups (DPGs) in single-pulse search output. The first stage identified DPGs and extracted features to characterize them using a new peak identification algorithm which tracks sloping tendencies around local maxima in plots of signal-to-noise ratio versus dispersion measure. The second stage used supervised machine learning to classify DPGs. We created four benchmark data sets: one unbalanced and three balanced versions using three different imbalance treatments.We empirically evaluated 48 classifiers by training and testing binary and multiclass versions of six machine learning algorithms on each of the four benchmark versions. While each classifier had advantages and disadvantages, all classifiers with imbalance treatments had higher recall values than those with unbalanced data, regardless of the machine learning algorithm used. Based on the benchmarking results, we selected a subset of classifiers to classify the full, unlabelled data set of over 1.5 million DPGs identified in 42 405 observations made by the Green Bank Telescope. Overall, the classifiers using a multiclass ensemble tree learner in combination with two oversampling imbalance treatments were the most efficient; they identified additional known pulsars not in the benchmark data set and provided six potential discoveries, with significantly less false positives than the other classifiers.},
   author = {T.R. Devine and K. Goseva-Popstojanova and M. McLaughlin},
   doi = {10.1093/mnras/stw655},
   issue = {2},
   journal = {Monthly Notices of the Royal Astronomical Society},
   pages = {1519-1532},
   title = {Detection of dispersed radio pulses: A machine learning approach to candidate identification and classification},
   volume = {459},
   year = {2016},
}
@article{Chen2014,
   abstract = {Deep learning is currently an extremely active research area in machine learning and pattern recognition society. It has gained huge successes in a broad area of applications such as speech recognition, computer vision, and natural language processing. With the sheer size of data available today, big data brings big opportunities and transformative potential for various sectors; on the other hand, it also presents unprecedented challenges to harnessing data and information. As the data keeps getting bigger, deep learning is coming to play a key role in providing big data predictive analytics solutions. In this paper, we provide a brief overview of deep learning, and highlight current research efforts and the challenges to big data, as well as the future trends.},
   author = {X.-W. Chen and X. Lin},
   doi = {10.1109/ACCESS.2014.2325029},
   journal = {IEEE Access},
   pages = {514-525},
   title = {Big data deep learning: Challenges and perspectives},
   volume = {2},
   year = {2014},
}
@article{Du2017,
   abstract = {Stellar spectral classification is one of the most fundamental tasks in survey astronomy. Many automated classification methods have been applied to spectral data. However, their main limitation is that the model parameters must be tuned repeatedly to deal with different data sets. In this paper, we utilize the Bayesian support vector machines (BSVM) to classify the spectral subclass data. Based on Gibbs sampling, BSVM can infer all model parameters adaptively according to different data sets, which allows us to circumvent the time-consuming cross validation for penalty parameter. We explored different normalization methods for stellar spectral data, and the best one has been suggested in this study. Finally, experimental results on several stellar spectral subclass classification problems show that the BSVM model not only possesses good adaptability but also provides better prediction performance than traditional methods.},
   author = {C. Du and A. Luo and H. Yang},
   doi = {10.1016/j.newast.2016.08.015},
   journal = {New Astronomy},
   pages = {51-58},
   title = {Adaptive stellar spectral subclass classification based on Bayesian SVMs},
   volume = {51},
   year = {2017},
}
@article{,
   abstract = {We show that multiple machine learning algorithms can match human performance in classifying transient imaging data from the Sloan Digital Sky Survey (SDSS) supernova survey into real objects and artefacts. This is a first step in any transient science pipeline and is currently still done by humans, but future surveys such as the Large Synoptic Survey Telescope (LSST) will necessitate fully machine-enabled solutions. Using features trained from eigenimage analysis (principal component analysis, PCA) of single-epoch g, r and i difference images, we can reach a completeness (recall) of 96 per cent, while only incorrectly classifying at most 18 per cent of artefacts as real objects, corresponding to a precision (purity) of 84 per cent. In general, random forests performed best, followed by the k-nearest neighbour and the SkyNet artificial neural net algorithms, compared to other methods such as naive Bayes and kernel support vector machine. Our results show that PCA-based machine learning can match human success levels and can naturally be extended by including multiple epochs of data, transient colours and host galaxy information which should allow for significant further improvements, especially at low signal-to-noise.},
   author = {L. du Buisson and N. Sivanandam and B.A. Bassett and M. Smith},
   doi = {10.1093/mnras/stv2041},
   issue = {2},
   journal = {Monthly Notices of the Royal Astronomical Society},
   pages = {2026-2038},
   title = {Machine learning classification of SDSS transient survey images},
   volume = {454},
   year = {2015},
}
@article{Stappers2016,
   abstract = {Improving survey specifications are causing an exponential rise in pulsar candidate numbers and data volumes. We study the candidate filters used to mitigate these problems during the past 50 years. We find that some existing methods such as applying constraints on the total number of candidates collected per observation, may have detrimental effects on the success of pulsar searches. Those methods immune to such effects are found to be ill-equipped to deal with the problems associated with increasing data volumes and candidate numbers, motivating the development of new approaches. We therefore present a new method designed for online operation. It selects promising candidates using a purpose-built tree-based machine learning classifier, the Gaussian Hellinger Very Fast Decision Tree, and a new set of features for describing candidates. The features have been chosen so as to (i) maximize the separation between candidates arising from noise and those of probable astrophysical origin, and (ii) be as survey-independent as possible. Using these features our new approach can process millions of candidates in seconds (~1 million every 15 s), with high levels of pulsar recall (90 per cent+). This technique is therefore applicable to the large volumes of data expected to be produced by the Square Kilometre Array. Use of this approach has assisted in the discovery of 20 new pulsars in data obtained during the Low-Frequency Array Tied-Array All-Sky Survey.},
   author = {B.W. Stappers and S. Cooper and J.M. Brooke and J.D. Knowles},
   doi = {10.1093/mnras/stw656},
   issue = {1},
   journal = {Monthly Notices of the Royal Astronomical Society},
   pages = {1104-1123},
   title = {Fifty years of pulsar candidate selection: From simple filters to a new principled real-time classification approach},
   volume = {459},
   year = {2016},
}
@article{Najafabadi2015,
   abstract = {Big Data Analytics and Deep Learning are two high-focus of data science. Big Data has become important as many organizations both public and private have been collecting massive amounts of domain-specific information, which can contain useful information about problems such as national intelligence, cyber security, fraud detection, marketing, and medical informatics. Companies such as Google and Microsoft are analyzing large volumes of data for business analysis and decisions, impacting existing and future technology. Deep Learning algorithms extract high-level, complex abstractions as data representations through a hierarchical learning process. Complex abstractions are learnt at a given level based on relatively simpler abstractions formulated in the preceding level in the hierarchy. A key benefit of Deep Learning is the analysis and learning of massive amounts of unsupervised data, making it a valuable tool for Big Data Analytics where raw data is largely unlabeled and un-categorized. In the present study, we explore how Deep Learning can be utilized for addressing some important problems in Big Data Analytics, including extracting complex patterns from massive volumes of data, semantic indexing, data tagging, fast information retrieval, and simplifying discriminative tasks. We also investigate some aspects of Deep Learning research that need further exploration to incorporate specific challenges introduced by Big Data Analytics, including streaming data, high-dimensional data, scalability of models, and distributed computing. We conclude by presenting insights into relevant future works by posing some questions, including defining data sampling criteria, domain adaptation modeling, defining criteria for obtaining useful data abstractions, improving semantic indexing, semi-supervised learning, and active learning.},
   author = {M.M. Najafabadi and F. Villanustre and T.M. Khoshgoftaar and N. Seliya and R. Wald and E. Muharemagic},
   doi = {10.1186/s40537-014-0007-7},
   issue = {1},
   journal = {Journal of Big Data},
   title = {Deep learning applications and challenges in big data analytics},
   volume = {2},
   year = {2015},
}
@article{Liu2016,
   abstract = {An important problem to be solved of traditional classification methods is they cannot deal with large-scale classification because of very high time complexity. In order to solve above problem, inspired by the thinking of collaborative management, the non-linearly assembling learning machine (NALM) is proposed and used in the large-scale stellar spectral classification. In NALM, the large-scale dataset is firstly divided into several subsets, and then the traditional classifiers such as support vector machine (SVM) runs on the subset, finally, the classification results on each subset are assembled and the overall classification decision is obtained. In comparative experiments, we investigate the performance of NALM in the stellar spectral subclasses classification compared with SVM. We apply SVM and NALM respectively to classify the four subclasses of K-type spectra, three subclasses of F-type spectra and three subclasses of G-type spectra from Sloan Digital Sky Survey (SDSS). The comparative experiment results show that the performance of NALM is much better than SVM in view of the classification accuracy and the computation time.},
   author = {Z. Liu and L. Song and W. Zhao},
   doi = {10.1093/mnras/stv2600},
   issue = {4},
   journal = {Monthly Notices of the Royal Astronomical Society},
   pages = {4289-4294},
   title = {Classification of large-scale stellar spectra based on the non-linearly assembling learning machine},
   volume = {455},
   year = {2016},
}
@article{Jan2019,
   abstract = {Deep learning methods are extensively applied to various fields of science and engineering such as speech recognition, image classifications, and learning methods in language processing. Similarly, traditional data processing techniques have several limitations of processing large amount of data. In addition, Big Data analytics requires new and sophisticated algorithms based on machine and deep learning techniques to process data in real-time with high accuracy and efficiency. However, recently, research incorporated various deep learning techniques with hybrid learning and training mechanisms of processing data with high speed. Most of these techniques are specific to scenarios and based on vector space thus, shows poor performance in generic scenarios and learning features in big data. In addition, one of the reason of such failure is high involvement of humans to design sophisticated and optimized algorithms based on machine and deep learning techniques. In this article, we bring forward an approach of comparing various deep learning techniques for processing huge amount of data with different number of neurons and hidden layers. The comparative study shows that deep learning techniques can be built by introducing a number of methods in combination with supervised and unsupervised training techniques.},
   author = {B. Jan and H. Farman and M. Khan and M. Imran and I.U. Islam and A. Ahmad and S. Ali and G. Jeon},
   doi = {10.1016/j.compeleceng.2017.12.009},
   journal = {Computers and Electrical Engineering},
   pages = {275-287},
   title = {Deep learning in big data Analytics: A comparative study},
   volume = {75},
   year = {2019},
}
@article{Chen2016,
   abstract = {In this paper, the authors make the first attempt to employ the deep learning method for the representation learning of the solar radio spectrums. The original solar radio spectrums are pre-processed, including normalization, enhancement and etc., to generate new images for the next processing. With the expertise of solar radio astronomy for identifying solar radio activity, we build a solar radio activity database, which contains solar radio spectrums as well as their labels indicating the types of solar radio bursts. The employed deep learning network is firstly pre-trained based on the available massive of unlabeled radio solar images. Afterwards, the weights of the network are further fined-tuned based on the labeled data. Experimental results have demonstrated that the employed network can effectively classify the solar radio image into the labeled categories. Moreover, the pre-training process can help improve the classification accuracy.},
   author = {Z. Chen and L. Ma and L. Xu and C. Tan and Y. Yan},
   doi = {10.1007/s11042-015-2528-2},
   issue = {5},
   journal = {Multimedia Tools and Applications},
   pages = {2859-2875},
   title = {Imaging and representation learning of solar radio spectrums for classification},
   volume = {75},
   year = {2016},
}
@article{Wang2016,
   abstract = {Machine learning is an artificial intelligence method of discovering knowledge for making intelligent decisions. Big Data has great impacts on scientific discoveries and value creation. This paper introduces methods in machine learning, main technologies in Big Data, and some applications of machine learning in Big Data. Challenges of machine learning applications in Big Data are discussed. Some new methods and technology progress of machine learning in Big Data are also presented.},
   author = {L. Wang and C.A. Alexander},
   doi = {10.33889/ijmems.2016.1.2-006},
   issue = {2},
   journal = {International Journal of Mathematical, Engineering and Management Sciences},
   pages = {52-61},
   title = {Machine learning in big data},
   volume = {1},
   year = {2016},
}
@article{Nishizuka2017,
   abstract = {We developed a flare prediction model using machine learning, which is optimized to predict the maximum class of flares occurring in the following 24 hr. Machine learning is used to devise algorithms that can learn from and make decisions on a huge amount of data. We used solar observation data during the period 2010-2015, such as vector magnetograms, ultraviolet (UV) emission, and soft X-ray emission taken by the Solar Dynamics Observatory and the Geostationary Operational Environmental Satellite. We detected active regions (ARs) from the full-disk magnetogram, from which ∼60 features were extracted with their time differentials, including magnetic neutral lines, the current helicity, the UV brightening, and the flare history. After standardizing the feature database, we fully shuffled and randomly separated it into two for training and testing. To investigate which algorithm is best for flare prediction, we compared three machine-learning algorithms: the support vector machine, k-nearest neighbors (k-NN), and extremely randomized trees. The prediction score, the true skill statistic, was higher than 0.9 with a fully shuffled data set, which is higher than that for human forecasts. It was found that k-NN has the highest performance among the three algorithms. The ranking of the feature importance showed that previous flare activity is most effective, followed by the length of magnetic neutral lines, the unsigned magnetic flux, the area of UV brightening, and the time differentials of features over 24 hr, all of which are strongly correlated with the flux emergence dynamics in an AR.},
   author = {N. Nishizuka and K. Sugiura and Y. Kubo and M. Den and S. Watari and M. Ishii},
   doi = {10.3847/1538-4357/835/2/156},
   issue = {2},
   journal = {Astrophysical Journal},
   title = {Solar Flare Prediction Model with Three Machine-learning Algorithms using Ultraviolet Brightening and Vector Magnetograms},
   volume = {835},
   year = {2017},
}
@article{Kunder2017,
   abstract = {Data Release 5 (DR5) of the Radial Velocity Experiment (RAVE) is the fifth data release from a magnitude-limited (9 < I < 12) survey of stars randomly selected in the Southern Hemisphere. The RAVE medium-resolution spectra (R ∼ 7500) covering the Ca-triplet region (8410-8795 A) span the complete time frame from the start of RAVE observations in 2003 to their completion in 2013. Radial velocities from 520,781 spectra of 457,588 unique stars are presented, of which 255,922 stellar observations have parallaxes and proper motions from the Tycho-Gaia astrometric solution in Gaia DR1. For our main DR5 catalog, stellar parameters (effective temperature, surface gravity, and overall metallicity) are computed using the RAVE DR4 stellar pipeline, but calibrated using recent K2 Campaign 1 seismic gravities and Gaia benchmark stars, as well as results obtained from high-resolution studies. Also included are temperatures from the Infrared Flux Method, and we provide a catalog of red giant stars in the dereddened color - (J Ks) 0 interval (0.50, 0.85) for which the gravities were calibrated based only on seismology. Further data products for subsamples of the RAVE stars include individual abundances for Mg, Al, Si, Ca, Ti, Fe, and Ni, and distances found using isochrones. Each RAVE spectrum is complemented by an error spectrum, which has been used to determine uncertainties on the parameters. The data can be accessed via the RAVE Web site or the VizieR database.},
   author = {A. Kunder and G. Kordopatis and M. Steinmetz and T. Zwitter and P.J. McMillan and L. Casagrande and H. Enke and J. Wojno and M. Valentini and C. Chiappini and A. Miglio and B. Mosser},
   doi = {10.3847/1538-3881/153/2/75},
   issue = {2},
   journal = {Astronomical Journal},
   title = {THE RADIAL VELOCITY EXPERIMENT (RAVE): FIFTH DATA RELEASE},
   volume = {153},
   year = {2017},
}
@article{Kuminski2018,
   abstract = {Modern astronomy relies on massive databases collected by robotic telescopes and digital sky surveys, acquiring data in a much faster pace than what manual analysis can support. Among other data, these sky surveys collect information about millions and sometimes billions of extra-galactic objects. Since the very large number of objects makes manual observation impractical, automatic methods that can analyze and annotate extra-galactic objects are required to fully utilize the discovery power of these databases. Machine learning methods for annotation of celestial objects can be separated broadly into methods that use the photometric information collected by digital sky surveys, and methods that analyze the image of the object. Here we describe a hybrid method that combines photometry and image data to annotate galaxies by their morphology, and a method that uses that information to identify objects that are visually similar to a query object (query-by-example). The results are compared to using just photometric information from SDSS, and to using just the morphological descriptors extracted directly from the images. The comparison shows that for automatic classification the image data provide marginal addition to the information provided by the photometry data. For query-by-example, however, the analysis of the image data provides more information that improves the automatic detection substantially. The source code and binaries of the method can be downloaded through the Astrophysics Source Code Library.},
   author = {E. Kuminski and L. Shamir},
   doi = {10.1016/j.ascom.2018.10.008},
   journal = {Astronomy and Computing},
   pages = {257-269},
   title = {A hybrid approach to machine learning annotation of large galaxy image databases},
   volume = {25},
   year = {2018},
}
@article{Nolte2019,
   abstract = {We present a machine learning analysis of five labelled galaxy catalogues from the Galaxy And Mass Assembly (GAMA): The SersicCatVIKING and SersicCatUKIDSS catalogues containing morphological features, the GaussFitSimplecatalogue containing spectroscopic features, the MagPhys catalogue including physical parameters for galaxies, and the Lambdar catalogue, which contains photometric measurements. Extending work previously presented at the ESANN 2018 conference – in an analysis based on Generalized Relevance Matrix Learning Vector Quantization and Random Forests – we find that neither the data from the individual catalogues nor a combined dataset based on all 5 catalogues fully supports the visual-inspection-based galaxy classification scheme employed to categorise the galaxies. In particular, only one class, the Little Blue Spheroids, is consistently separable from the other classes. To aid further insight into the nature of the employed visual-based classification scheme with respect to physical and morphological features, we present the galaxy parameters that are discriminative for the achieved class distinctions.},
   author = {A. Nolte and L. Wang and M. Bilicki and B. Holwerda and M. Biehl},
   doi = {10.1016/j.neucom.2018.12.076},
   journal = {Neurocomputing},
   pages = {172-190},
   title = {Galaxy classification: A machine learning analysis of GAMA catalogue data},
   volume = {342},
   year = {2019},
}
@article{Zhou2017,
   abstract = {Machine learning (ML) is continuously unleashing its power in a wide range of applications. It has been pushed to the forefront in recent years partly owing to the advent of big data. ML algorithms have never been better promised while challenged by big data. Big data enables ML algorithms to uncover more fine-grained patterns and make more timely and accurate predictions than ever before; on the other hand, it presents major challenges to ML such as model scalability and distributed computing. In this paper, we introduce a framework of ML on big data (MLBiD) to guide the discussion of its opportunities and challenges. The framework is centered on ML which follows the phases of preprocessing, learning, and evaluation. In addition, the framework is also comprised of four other components, namely big data, user, domain, and system. The phases of ML and the components of MLBiD provide directions for identification of associated opportunities and challenges and open up future work in many unexplored or under explored research areas.},
   author = {L. Zhou and S. Pan and J. Wang and A.V. Vasilakos},
   doi = {10.1016/j.neucom.2017.01.026},
   journal = {Neurocomputing},
   pages = {350-361},
   title = {Machine learning on big data: Opportunities and challenges},
   volume = {237},
   year = {2017},
}
@article{Bethapudi2018,
   abstract = {We evaluate the performance of four different machine learning (ML) algorithms: an Artificial Neural Network Multi-Layer Perceptron (ANN MLP), Adaboost, Gradient Boosting Classifier (GBC), and XGBoost, for the separation of pulsars from radio frequency interference (RFI) and other sources of noise, using a dataset obtained from the post-processing of a pulsar search pipeline. This dataset was previously used for the cross-validation of the SPINN-based machine learning engine, obtained from the reprocessing of the HTRU-S survey data (Morello et al., 2014). We have used the Synthetic Minority Over-sampling Technique (SMOTE) to deal with high-class imbalance in the dataset. We report a variety of quality scores from all four of these algorithms on both the non-SMOTE and SMOTE datasets. For all the above ML methods, we report high accuracy and G-mean for both the non-SMOTE and SMOTE cases. We study the feature importances using Adaboost, GBC, and XGBoost and also from the minimum Redundancy Maximum Relevance approach to report algorithm-agnostic feature ranking. From these methods, we find that the signal to noise of the folded profile to be the best feature. We find that all the ML algorithms report FPRs about an order of magnitude lower than the corresponding FPRs obtained in Morello et al. (2014), for the same recall value.},
   author = {S. Bethapudi and S. Desai},
   doi = {10.1016/j.ascom.2018.02.002},
   journal = {Astronomy and Computing},
   pages = {15-26},
   title = {Separation of pulsar signals from noise using supervised machine learning algorithms},
   volume = {23},
   year = {2018},
}
@article{Mourabit2020,
   abstract = {Maintenance in manufacturing has been developed and researched in the last few decades at a very rapid rate. It’s a major step in process control to build a decision tool that detects defects in equipment or processes as quickly as possible to maintain high process efficiencies. However, the high complexity of machines, and the increase in data available in almost all areas, makes research on improving the accuracy of fault detection via data-mining more and more challenging issue in this field. In our paper we present a new predictive model of semiconductor failures, based on machine learning approach, for predictive maintenance in industry 4.0. The framework of our model includes: Dataset and data acquisition, data preprocessing in three phases (over-sampling, data cleaning, and attribute reduction with principal component analysis (PCA) technique and CfsSubsetEval technique), data modeling, evaluation model and implementation model. We used SECOM dataset to develop four different models based on four algorithms (Naive Bayesian, C4.5 Decision tree, Multilayer perceptron (MLP), Support vector machine), according to the five metrics (True Positive rate, False Positive rate, Precision, F-Mesure and Accuracy). We implemented our new predictive model with 91, 95% of accuracy, as a new efficient predictive model of semiconductor failures.},
   author = {Y.E. Mourabit and Y.E. Habouz and H. Zougagh and Y. Wadiai},
   doi = {10.14569/IJACSA.2020.0111225},
   issue = {12},
   journal = {International Journal of Advanced Computer Science and Applications},
   pages = {199-203},
   title = {Predictive System of Semiconductor Failures based on Machine Learning Approach},
   volume = {11},
   year = {2020},
}
@article{Meingast2017,
   abstract = {Dust extinction is the most robust tracer of the gas distribution in the interstellar medium, but measuring extinction is limited by the systematic uncertainties involved in estimating the intrinsic colors to background stars. In this paper we present a new technique, Pnicer, that estimates intrinsic colors and extinction for individual stars using unsupervised machine learning algorithms. This new method aims to be free from any priors with respect to the column density and intrinsic color distribution. It is applicable to any combination of parameters and works in arbitrary numbers of dimensions. Furthermore, it is not restricted to color space. Extinction toward single sources is determined by fitting Gaussian mixture models along the extinction vector to (extinction-free) control field observations. In this way it becomes possible to describe the extinction for observed sources with probability densities, rather than a single value. Pnicer effectively eliminates known biases found in similar methods and outperforms them in cases of deep observational data where the number of background galaxies is significant, or when a large number of parameters is used to break degeneracies in the intrinsic color distributions. This new method remains computationally competitive, making it possible to correctly de-redden millions of sources within a matter of seconds. With the ever-increasing number of large-scale high-sensitivity imaging surveys, Pnicer offers a fast and reliable way to efficiently calculate extinction for arbitrary parameter combinations without prior information on source characteristics. The Pnicer software package also offers access to the well-established Nicer technique in a simple unified interface and is capable of building extinction maps including the Nicest correction for cloud substructure. Pnicer is offered to the community as an open-source software solution and is entirely written in Python.},
   author = {S. Meingast and M. Lombardi and J. Alves},
   doi = {10.1051/0004-6361/201630032},
   journal = {Astronomy and Astrophysics},
   title = {Estimating extinction using unsupervised machine learning},
   volume = {601},
   year = {2017},
}
@article{Ishida2019,
   author = {E.E.O. Ishida},
   doi = {10.1038/s41550-019-0860-6},
   issue = {8},
   journal = {Nature Astronomy},
   pages = {680-682},
   title = {Machine learning and the future of supernova cosmology},
   volume = {3},
   year = {2019},
}
@inproceedings{Azhari2020,
   abstract = {The pulsar classification represents a major issue in the astrophysical area. The Bagging Algorithm is an ensemble method widely used to improve the performance of classification algorithms, especially in the case of pulsar search. In this way, our paper tries to prove how the Bagging Method can improve the performance of pulsar candidate detection in connection with four basic classifiers: Core Vector Machines (CVM), the K-Nearest-Neighbors (KNN), the Artificial Neural Network (ANN), Cart Decision Tree (CDT). The Error Rate, Area Under the Curve (AUC), Computation Time (CT) are measured to compare the performance of different classifiers. The High Time Resolution Universe (HTRU2) dataset, collected from the UCI Machine Learning Repository, is used in the experimentation phase.},
   author = {M. Azhari and A. Abarda and A. Alaoui and B. Ettaki and J. Zerouaoui},
   doi = {10.1016/j.procs.2020.03.062},
   journal = {Procedia Computer Science},
   pages = {1096-1101},
   title = {Detection of Pulsar Candidates using Bagging Method},
   volume = {170},
   year = {2020},
}
@inproceedings{Khan2020,
   abstract = {During last few years the computer applications have gone dramatic transformation from simple data processing to machine learning, thanks to the availability and accessibility of huge volume of data collected through sensors and internet. The idea of machine learning demonstrates and propagates the facts that computer has the ability to improve itself with the passage of time. The western countries have shown great interest on the topic of machine learning, computer vision, and pattern recognition via organizing conferences, workshops, collective discussion, experimentation, and real life implementation. This study on machine learning and computer vision explores and analytically evaluates the machine learning applications in computer vision and predicts future prospects. The study has found that the machine learning strategies in computer vision are supervised, un-supervised, and semi-supervised. The commonly used algorithms are neural networks, k-means clustering, and support vector machine. The most recent applications of machine learning in computer vision are object detection, object classification, and extraction of relevant information from images, graphic documents, and videos. Additionally, Tensor flow, Faster-RCNN-Inception-V2 model, and Anaconda software development environment used to identify cars and persons in images.},
   author = {A.I. Khan and S. Al-Habsi},
   doi = {10.1016/j.procs.2020.03.355},
   journal = {Procedia Computer Science},
   pages = {1444-1451},
   title = {Machine Learning in Computer Vision},
   volume = {167},
   year = {2020},
}
@book{Dulhare2020,
   author = {U.N. Dulhare and K. Ahmad and K.A.B. Ahmad},
   isbn = {9781119654834},
   journal = {Machine Learning and Big Data: Concepts, Algorithms, Tools and Applications},
   pages = {xix-xx},
   title = {Preface},
   year = {2020},
}
@inproceedings{Zhang2020,
   abstract = {The pulsar is a highly magnetized rotating neutron star that provides the first indirect evidence for the existence of gravitational waves and also provides the possibility to reveal extreme phenomena in neutron star astrophysics. Therefore, the identification of pulsars in the universe is a prerequisite for the study of pulsars and gravitational waves. At present, a large number of pulsar searches have produced millions of pulsar candidates. In the face of these large-scale data, if only relying on manual visual classification by experts in related fields, it will be a huge project. Since the emergence of machine learning, its theory and technology have become increasingly mature, and has been successfully applied to astronomical research fields such as pulsar candidate screening. This paper introduces the related machine learning theory of pulsar candidate recognition firstly, and then reviews the research status of pulsar candidate recognition based on machine learning in recent years. Finally, we discuss and prospect the identification of pulsars in the future.},
   author = {C.J. Zhang and Z.H. Shang and W.M. Chen and L. Xie and X.H. Miao},
   doi = {10.1016/j.procs.2020.02.050},
   journal = {Procedia Computer Science},
   pages = {534-538},
   title = {A review of research on pulsar candidate recognition based on machine learning},
   volume = {166},
   year = {2020},
}
@article{Wu2020,
   abstract = {Rare objects such as white dwarf+main sequence (WDMS) and cataclysmic variables (CVs) are very important for studying the evolution of the galaxy and the universe. The large amount of spectra obtained by the large sky surveys such as the Sloan Digital Sky Survey (SDSS) are rich sources of these rare objects. However, a considerable fraction of these spectra are low-S/N spectra. These low-S/N spectra contain similar useful information as the high-S/N spectra, and making better use of these spectra can significantly improve the chance of finding rare objects. Nevertheless, little research has been done on them. In this study we propose a novel method based on the combination of PCA (Principal Components Analysis) and CFSFDP (Clustering by Fast Search and Find of Density Peak) to search for rare objects from low-S/N spectra. The PCA first extracts principal components from high-S/N spectra to generate general feature spectra and reconstructs low-S/N stellar spectra with these general feature spectra. Then the CFSFDP calculates the Local Density ρ and the Distance δ of the reconstructed spectra, and select the outliers through the decision graph quickly and accurately. We first apply our method to spectra in SDSS stellar classification template library with adding white gaussian noise to search for rare objects (carbon stars, carbon white dwarfs, carbon_lines, white dwarfs and white dwarfs magnetic). Then we apply our method to observed spectra with different low-S/Ns from SDSS and compared with Lick-index+K-means and Support Vector Machines (SVM). The experimental results show that our method has a higher efficiency compared to other methods.},
   author = {M. Wu and J. Pan and Z. Yi and P. Wei},
   doi = {10.1109/ACCESS.2020.2983745},
   journal = {IEEE Access},
   pages = {66475-66488},
   title = {Rare Object Search from Low-S/N Stellar Spectra in SDSS},
   volume = {8},
   year = {2020},
}
@article{Chiu2021,
   abstract = {Accurate measurements of statistical properties, such as the star formation rate and the lifetime of young stellar objects (YSOs) in different stages, are essential for constraining star formation theories. However, it is a difficult task to separate galaxies and YSOs based on spectral energy distributions (SEDs) alone, because they contain both thermal emission from stars and dust around them and no reliable theories can be applied to distinguish them. Here we compare different machine learning algorithms and develop the Spectrum Classifier of Astronomical Objects (SCAO), based on Fully Connected Neural Network (FCN), to classify regular stars, galaxies, and YSOs. Superior to previous classifiers, SCAO is solely trained by high quality data labeled in Molecular Cores to Planet-forming Disks (c2d) catalog without a priori theoretical knowledge, and provides excellent results with high precision (>96%) and recall (>98%) for YSOs when only eight bands are included. We systematically investigate the effects of observation errors and distance effects, and show that high accuracy performance is still maintained even when using fluxes of only three bands (IRAC 3, a=IRAC 4, and MIPS 1) in the long wavelengths regime, because the silicate absorption feature is automatically detected by SCAO. Finally, we applied SCAO to Spitzer Enhanced Imaging Products (SEIP), the most complete catalog of Spitzer observations, and found 129219 YSO candidates. The website from SCAO is available at http://scao.astr.nthu.edu.tw.},
   author = {Y.-L. Chiu and C.-T. Ho and D.-W. Wang and S.-P. Lai},
   doi = {10.1016/j.ascom.2021.100470},
   journal = {Astronomy and Computing},
   title = {Searching for young stellar objects through SEDs by machine learning},
   volume = {36},
   year = {2021},
}
@article{Lin2020,
   abstract = {It is an active topic to investigate the schemes based on machine learning (ML) methods for detecting pulsars as the data volume growing exponentially in modern surveys. To improve the detection performance, input features into an ML model should be investigated specifically. In the existing pulsar detection researches based on ML methods, there are mainly two kinds of feature designs: the empirical features and statistical features. Due to the combinational effects from multiple features, however, there exist some redundancies and even irrelevant components in the available features, which can reduce the accuracy of a pulsar detection model. Therefore, it is essential to select a subset of relevant features from a set of available candidate features and known as feature selection. In this work, two feature selection algorithms -Grid Search (GS) and Recursive Feature Elimination (RFE) - are proposed to improve the detection performance by removing the redundant and irrelevant features. The algorithms were evaluated on the Southern High Time Resolution University survey (HTRU-S) with five pulsar detection models. The experimental results verify the effectiveness and efficiency of our proposed feature selection algorithms. By the GS, a model with only two features reach a recall rate as high as 99 per cent and a false positive rate (FPR) as low as 0.65 per cent; by the RFE, another model with only three features achieves a recall rate of 99 per cent and an FPR of 0.16 per cent in pulsar candidates classification. Furthermore, this work investigated the number of features required as well as the misclassified pulsars by our models.},
   author = {H. Lin and X. Li and Z. Luo},
   doi = {10.1093/mnras/staa218},
   issue = {2},
   journal = {Monthly Notices of the Royal Astronomical Society},
   pages = {1842-1854},
   title = {Pulsars detection by machine learning with very few features},
   volume = {493},
   year = {2020},
}
@article{Fluke2020,
   abstract = {High-quality, useable, and effective software is essential for supporting astronomers in the discovery-focused tasks of data analysis and visualisation. As the volume, and perhaps more crucially, the velocity of astronomical data grow, the role of the astronomer is changing. There is now an increased reliance on automated and autonomous discovery and decision-making workflows rather than visual inspection. We assert the need for an improved understanding of how astronomers (humans) currently make visual discoveries from data. This insight is a critical element for the future design, development and effective use of cyber-human discovery systems, where astronomers work in close collaboration with automated systems to gain understanding from continuous, real-time data streams. We discuss how relevant human performance data could be gathered, specifically targeting the domains of expertise and skill at visual discovery, and the identification and management of cognitive factors. By looking at other disciplines where human performance is assessed and measured, we propose four early-stage applications that would: (1) allow astronomers to evaluate, and potentially improve, their own visual discovery skills; (2) support just-in-time coaching; (3) enable talent identification; and (4) result in user interfaces that automatically respond to skill level and cognitive state. Throughout, we advocate for the importance of user studies and the incorporation of participatory design and co-design practices into the planning, implementation and evaluation of alternative user interfaces and visual discovery environments.},
   author = {C.J. Fluke and S.E. Hegarty and C.O.-M. MacMahon},
   doi = {10.1016/j.ascom.2020.100423},
   journal = {Astronomy and Computing},
   title = {Understanding the human in the design of cyber-human discovery systems for data-driven astronomy},
   volume = {33},
   year = {2020},
}
@article{,
   abstract = {Semi-supervised learning is the branch of machine learning concerned with using labelled as well as unlabelled data to perform certain learning tasks. Conceptually situated between supervised and unsupervised learning, it permits harnessing the large amounts of unlabelled data available in many use cases in combination with typically smaller sets of labelled data. In recent years, research in this area has followed the general trends observed in machine learning, with much attention directed at neural network-based models and generative learning. The literature on the topic has also expanded in volume and scope, now encompassing a broad spectrum of theory, algorithms and applications. However, no recent surveys exist to collect and organize this knowledge, impeding the ability of researchers and engineers alike to utilize it. Filling this void, we present an up-to-date overview of semi-supervised learning methods, covering earlier work as well as more recent advances. We focus primarily on semi-supervised classification, where the large majority of semi-supervised learning research takes place. Our survey aims to provide researchers and practitioners new to the field as well as more advanced readers with a solid understanding of the main approaches and algorithms developed over the past two decades, with an emphasis on the most prominent and currently relevant work. Furthermore, we propose a new taxonomy of semi-supervised classification algorithms, which sheds light on the different conceptual and methodological approaches for incorporating unlabelled data into the training process. Lastly, we show how the fundamental assumptions underlying most semi-supervised learning algorithms are closely connected to each other, and how they relate to the well-known semi-supervised clustering assumption.},
   author = {J.E. van Engelen and H.H. Hoos},
   doi = {10.1007/s10994-019-05855-6},
   issue = {2},
   journal = {Machine Learning},
   pages = {373-440},
   title = {A survey on semi-supervised learning},
   volume = {109},
   year = {2020},
}
@article{Liu2021,
   abstract = {With the increase of stellar spectra, how to automatically classify these spectra have attracted astronomer's attention. Support Vector Machine (SVM), as a typical classifier, has widely used in stellar spectra classification. Due to its limited performance in various classification problems and higher training time, a model with a pair of hyperspheres named Twin Hypersphere Model (THM), proposed by Peng and Xu, is utilized for stellar spectra classification in this paper. In THM, the samples in one hypersphere is far from another according to the Euclidean distance. The comparative experiments with SVM and Twin Support Vector Machine (TWSVM) on the SDSS datasets shows that the THM model gives the best classification accuracy of 0.8836 for type F, 0.9446 for type G, and 0.9509 for type K, which are better than the classification accuracies of 0.8000, 0.8484, 0.8911 obtained by SVM and 0.8413, 0.8699, 0.9109 obtained by TWSVM. It can be concluded that THM perform better than traditional techniques such as SVM and TWSVM on the K-, F-, G- type stellar spectra classification.},
   author = {Z. Liu},
   doi = {10.1016/j.newast.2021.101613},
   journal = {New Astronomy},
   title = {Stellar spectra classification with twin hypersphere model},
   volume = {88},
   year = {2021},
}
@article{,
   abstract = {Recently, with the emergence of Industry 4.0 (I4.0), smart systems, machine learning (ML) within artificial intelligence (AI), predictive maintenance (PdM) approaches have been extensively applied in industries for handling the health status of industrial equipment. Due to digital transformation towards I4.0, information techniques, computerized control, and communication networks, it is possible to collect massive amounts of operational and processes conditions data generated form several pieces of equipment and harvest data for making an automated fault detection and diagnosis with the aim to minimize downtime and increase utilization rate of the components and increase their remaining useful lives. PdM is inevitable for sustainable smart manufacturing in I4.0. Machine learning (ML) techniques have emerged as a promising tool in PdM applications for smart manufacturing in I4.0, thus it has increased attraction of authors during recent years. This paper aims to provide a comprehensive review of the recent advancements of ML techniques widely applied to PdM for smart manufacturing in I4.0 by classifying the research according to the ML algorithms, ML category, machinery, and equipment used, device used in data acquisition, classification of data, size and type, and highlight the key contributions of the researchers, and thus offers guidelines and foundation for further research.},
   author = {Z.M. Çinar and A.A. Nuhu and Q. Zeeshan and O. Korhan and M. Asmael and B. Safaei},
   doi = {10.3390/su12198211},
   issue = {19},
   journal = {Sustainability (Switzerland)},
   title = {Machine learning in predictive maintenance towards sustainable smart manufacturing in industry 4.0},
   volume = {12},
   year = {2020},
}
@article{,
   abstract = {The classification of celestial spectra is one of the important contents of astronomical research. The key is to select and extract the most effective feature for classification from spectra data. In this paper, we propose a new feature extraction method for astronomical spectra based on two-dimensional Fourier spectrum image, and apply the method to the classification study of LAMOST (the Large Sky Area Multi-Object Fiber Spectroscopic Telescope) stellar spectral data. The spectra data are from LAMOST Data Release 5 (DR5). We select 30000 F, G, and K types of spectra data. The short-time Fourier transform (STFT) is used to transform the one-dimensional spectra data into two-dimensional Fourier spectrum images. We classify and test these two-dimensional Fourier spectrum images with a module based on deep convolution network, and the classification accuracy rate is 92.90%. The experimental result shows that the LAMOST stellar spectra data can be transformed into the two-dimensional Fourier spectrum images by the STFT. These spectral images inform new features, and build a new feature space, which is effective for classification. The method is a fully new attempt in spectra classification, which has certainly a pioneering significance for the classification and mining of massive celestial spectra.},
   author = {Z. Jing-min and M.A. Chen-ye and W. Lu and D. Li-ting and X.U. Ting-ting and A. Lin-pin and Z.H.O.U. Wei-hong},
   doi = {10.1016/j.chinastron.2020.08.004},
   issue = {3},
   journal = {Chinese Astronomy and Astrophysics},
   pages = {334-344},
   title = {A New Stellar Spectral Feature Extraction Method Based on Two-dimensional Fourier Spectrum Image and Its Application in the Stellar Spectral Classification Based on Deep Network},
   volume = {44},
   year = {2020},
}
@article{Thomas2020,
   abstract = {Rigorous documentation and inspection methods are used by financial institutions to manage the risk due to information asymmetry and moral hazard. Nevertheless, the intent of the borrower of funds is not obvious. The unique nature of handwriting can to an extent be a reflection of intent. Besides the financial services sector has a highly competitive market structure. Selling of financial products is a challenge; quickly understanding financial preferences through handwriting can be used to apply concentrated efforts on ‘likely’ customers. Handwriting analysis when facilitated through machine learning methods can have wider application to address the challenges of the financial sector. The present study adopts an experimental approach to apply machine learning techniques to handwriting analysis. Identified handwriting correlates helped to map the individual into corresponding personality type. Risk preferences are mapped to the Big Five Personality Traits (Andreas Oehler, 2017). Thus the paper detailed the evaluation of handwriting features for one of the Big Five Personality Traits viz. Extraversion. The results yielded 7 such handwriting features which were evaluated with machine learning techniques on 112 samples collected through personally administered questionnaire. Extraversion is associated with perceived financial behavior as ‘risk seeker’. We found that individuals who scored high on extraversion had no risk no reward philosophy, were likely to over spend and were inclined to invest in risky financial products like mutual funds.},
   author = {S. Thomas and M. Goel and D. Agrawal},
   doi = {10.1016/j.jbef.2020.100315},
   journal = {Journal of Behavioral and Experimental Finance},
   title = {A framework for analyzing financial behavior using machine learning classification of personality through handwriting analysis},
   volume = {26},
   year = {2020},
}
@article{Chang2021,
   abstract = {Astronomical images are often plagued by unwanted artifacts that arise from a
number of sources including imperfect optics, faulty image sensors, cosmic ray
hits, and even airplanes and artificial satellites. Spurious reflections (known
as "ghosts") and the scattering of light off the surfaces of a camera and/or
telescope are particularly difficult to avoid. Detecting ghosts and scattered
light efficiently in large cosmological surveys that will acquire petabytes of
data can be a daunting task. In this paper, we use data from the Dark Energy
Survey to develop, train, and validate a machine learning model to detect
ghosts and scattered light using convolutional neural networks. The model
architecture and training procedure is discussed in detail, and the performance
on the training and validation set is presented. Testing is performed on data
and results are compared with those from a ray-tracing algorithm. As a proof of
principle, we have shown that our method is promising for the Rubin Observatory
and beyond.},
   author = {Chihway Chang and Alex Drlica-Wagner and Stephen M. Kent and Brian Nord and Donah Michelle Wang and Michael H. L. S. Wang},
   doi = {10.1016/j.ascom.2021.100474},
   month = {5},
   title = {A Machine Learning Approach to the Detection of Ghosting and Scattered Light Artifacts in Dark Energy Survey Images},
   url = {https://arxiv.org/abs/2105.10524},
   year = {2021},
}
@report{Baron2019,
   abstract = {Astronomy is experiencing a rapid growth in data size and complexity. This change fosters the development of data-driven science as a useful companion to the common model-driven data analysis paradigm, where astronomers develop automatic tools to mine datasets and extract novel information from them. In recent years, machine learning algorithms have become increasingly popular among astronomers, and are now used for a wide variety of tasks. In light of these developments, and the promise and challenges associated with them, the IAC Winter School 2018 focused on big data in Astronomy, with a particular emphasis on machine learning and deep learning techniques. This document summarizes the topics of supervised and unsupervised learning algorithms presented during the school, and provides practical information on the application of such tools to astronomical datasets. In this document I cover basic topics in supervised machine learning, including selection and preprocessing of the input dataset, evaluation methods, and three popular supervised learning algorithms, Support Vector Machines, Random Forests, and shallow Artificial Neural Networks. My main focus is on unsupervised machine learning algorithms, that are used to perform cluster analysis, dimensionality reduction, visualization, and outlier detection. Unsupervised learning algorithms are of particular importance to scientific research, since they can be used to extract new knowledge from existing datasets, and can facilitate new discoveries.},
   author = {Dalya Baron},
   keywords = {data analysis,methods,statistical},
   title = {MACHINE LEARNING IN ASTRONOMY: A PRACTICAL OVERVIEW},
   url = {http://www.iac.es/winterschool/2018/},
   year = {2019},
}
@article{,
   author = {Jorge Núñez and Jorge Llacer},
   doi = {10.1016/S0893-6080(03)00011-X},
   issn = {08936080},
   issue = {3-4},
   journal = {Neural Networks},
   month = {4},
   pages = {411-417},
   title = {Astronomical image segmentation by self-organizing neural networks and wavelets},
   volume = {16},
   year = {2003},
}
@article{Riggi2019,
   abstract = {A new era in radio astronomy will begin with the upcoming large-scale surveys planned at the Australian Square Kilometre Array Pathfinder (ASKAP). ASKAP started its Early Science programme in October 2017 and several target fields were observed during the array commissioning phase. The SCORPIO field was the first observed in the Galactic Plane in Band 1 (792-1 032 MHz) using 15 commissioned antennas. The achieved sensitivity and large field of view already allow to discover new sources and survey thousands of existing ones with improved precision with respect to previous surveys. Data analysis is currently ongoing to deliver the first source catalogue. Given the increased scale of the data, source extraction and characterisation, even in this Early Science phase, have to be carried out in a mostly automated way. This process presents significant challenges due to the presence of extended objects and diffuse emission close to the Galactic Plane. In this context, we have extended and optimised a novel source finding tool, named CAESAR, to allow extraction of both compact and extended sources from radio maps. A number of developments have been done driven by the analysis of the SCORPIO map and in view of the future ASKAP Galactic Plane survey. The main goals are the improvement of algorithm performances and scalability as well as of software maintainability and usability within the radio community. In this paper, we present the current status of CAESAR and report a first systematic characterisation of its performance for both compact and extended sources using simulated maps. Future prospects are discussed in the light of the obtained results.},
   author = {S Riggi and F Vitello and U Becciani and C Buemi and F Bufano and A Calanducci and F Cavallaro and A Costa and A Ingallinera and P Leto and S Loru and R P Norris and F Schillirò and E Sciacca and C Trigilio and G Umana},
   doi = {10.1017/pasa.2019.29},
   journal = {Publications of the Astronomical Society of Australia},
   keywords = {Galactic,Plane-radio astronomy-source-finding-software},
   pages = {13},
   title = {CAESAR source finder: Recent developments and testing},
   volume = {36},
   url = {https://doi.org/10.1017/pasa.2019.29},
   year = {2019},
}
@article{Tan2021,
   author = {X Tan and K Xu and Y Cao and Y Zhang and L Ma and R W H Lau},
   doi = {10.1109/TIP.2021.3122004},
   journal = {IEEE Transactions on Image Processing},
   note = {cited By 6},
   pages = {9085-9098},
   title = {Night-Time Scene Parsing with a Large Real Dataset},
   volume = {30},
   year = {2021},
}
@inproceedings{Beyer2019,
   author = {L Beyer and X Zhai and A Oliver and A Kolesnikov},
   doi = {10.1109/ICCV.2019.00156},
   journal = {Proceedings of the IEEE International Conference on Computer Vision},
   note = {cited By 209},
   pages = {1476-1485},
   title = {S4L: Self-supervised semi-supervised learning},
   volume = {2019-October},
   year = {2019},
}
@inproceedings{Zhang2018,
   author = {H Zhang and M Cisse and Y N Dauphin and D Lopez-Paz},
   journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
   note = {cited By 837},
   title = {MixUp: Beyond empirical risk minimization},
   year = {2018},
}
@inproceedings{Cubuk2020,
   author = {E D Cubuk and B Zoph and J Shlens and Q V Le},
   doi = {10.1109/CVPRW50498.2020.00359},
   journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
   note = {cited By 278},
   pages = {3008-3017},
   title = {Randaugment: Practical automated data augmentation with a reduced search space},
   volume = {2020-June},
   year = {2020},
}
@inproceedings{Izmailov2018,
   author = {P Izmailov and D Podoprikhin and T Garipov and D Vetrov and A G Wilson},
   journal = {34th Conference on Uncertainty in Artificial Intelligence 2018, UAI 2018},
   note = {cited By 107},
   pages = {876-885},
   title = {Averaging weights leads to wider optima and better generalization},
   volume = {2},
   year = {2018},
}
@inproceedings{Narang2018,
   author = {S Narang and G Diamos and E Elsen and P Micikevicius and J Alben and D Garcia and B Ginsburg and M Houston and O Kuchaiev and G Venkatesh and H Wu},
   journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
   note = {cited By 139},
   title = {Mixed precision training},
   year = {2018},
}
@inproceedings{Hariharan2011,
   author = {B Hariharan and P Arbeláez and L Bourdev and S Maji and J Malik},
   doi = {10.1109/ICCV.2011.6126343},
   journal = {Proceedings of the IEEE International Conference on Computer Vision},
   note = {cited By 749},
   pages = {991-998},
   title = {Semantic contours from inverse detectors},
   year = {2011},
}
@inproceedings{Oliver2018,
   author = {A Oliver and A Odena and C Raffel and E D Cubuk and I J Goodfellow},
   journal = {Advances in Neural Information Processing Systems},
   note = {cited By 274},
   pages = {3235-3246},
   title = {Realistic evaluation of deep semi-supervised learning algorithms},
   volume = {2018-December},
   year = {2018},
}
@article{Everingham2015,
   author = {M Everingham and S M A Eslami and L Van Gool and C K I Williams and J Winn and A Zisserman},
   doi = {10.1007/s11263-014-0733-5},
   issue = {1},
   journal = {International Journal of Computer Vision},
   note = {cited By 2855},
   pages = {98-136},
   title = {The Pascal Visual Object Classes Challenge: A Retrospective},
   volume = {111},
   year = {2015},
}
@article{Krizhevsky2009,
   author = {A Krizhevsky},
   journal = {Learning Multiple Layers of Features from Tiny Images},
   note = {cited By 10326},
   title = {Learning multiple layers of features from tiny images},
   year = {2009},
}
@inproceedings{Zou2019,
   author = {Y Zou and Z Yu and X Liu and B.V.K.V. Kumar and J Wang},
   doi = {10.1109/ICCV.2019.00608},
   journal = {Proceedings of the IEEE International Conference on Computer Vision},
   note = {cited By 195},
   pages = {5981-5990},
   title = {Confidence regularized self-training},
   volume = {2019-October},
   year = {2019},
}
@article{Berthelot2020,
   author = {David Berthelot and Nicholas Carlini and Ekin D Cubuk and Alex Kurakin and Kihyuk Sohn and Han Zhang and Colin Raffel},
   journal = {International Conference on Learning Representations},
   note = {cited By 94},
   title = {Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring},
   year = {2020},
}
@inproceedings{Laine2017,
   author = {S Laine and T Aila},
   journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
   note = {cited By 425},
   title = {Temporal ensembling for semi-supervised learning},
   year = {2017},
}
@inproceedings{Grandvalet2005,
   author = {Y Grandvalet and Y Bengio},
   journal = {Advances in Neural Information Processing Systems},
   note = {cited By 505},
   title = {Semi-supervised learning by entropy minimization},
   year = {2005},
}
@inproceedings{Li2020,
   author = {S Li and B Liu and D Chen and Q Chu and L Yuan and N Yu},
   doi = {10.1109/CVPR42600.2020.01341},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   note = {cited By 5},
   pages = {13397-13406},
   title = {Density-aware graph for deep semi-supervised visual recognition},
   year = {2020},
}
@inproceedings{Han2018,
   author = {B Han and Q Yao and X Yu and G Niu and M Xu and W Hu and I W Tsang and M Sugiyama},
   journal = {Advances in Neural Information Processing Systems},
   note = {cited By 435},
   pages = {8527-8537},
   title = {Co-teaching: Robust training of deep neural networks with extremely noisy labels},
   volume = {2018-December},
   year = {2018},
}
@inproceedings{Malach2017,
   author = {E Malach and S Shalev-Shwartz},
   journal = {Advances in Neural Information Processing Systems},
   note = {cited By 152},
   pages = {961-971},
   title = {Decoupling "when to update" from "how to update"},
   volume = {2017-December},
   year = {2017},
}
@inproceedings{Goldberger2017,
   author = {J Goldberger and E Ben-Reuven},
   journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
   note = {cited By 193},
   title = {Training deep neural-networks using a noise adaptation layer},
   year = {2017},
}
@article{Angluin1988,
   author = {D Angluin and P Laird},
   doi = {10.1023/A:1022873112823},
   issue = {4},
   journal = {Machine Learning},
   note = {cited By 496},
   pages = {343-370},
   title = {Learning From Noisy Examples},
   volume = {2},
   year = {1988},
}
@article{Ke2020,
   author = {Zhanghan Ke and Di Qiu and Kaican Li and Qiong Yan and Rynson W H Lau},
   journal = {ECCV},
   note = {cited By 8},
   title = {Guided collaborative training for pixel-wise semi-supervised learning},
   volume = {2},
   year = {2020},
}
@inproceedings{Yu2019,
   author = {X Yu and B Han and J Yao and G Niu and I W Tsang and M Sugiyama},
   journal = {36th International Conference on Machine Learning, ICML 2019},
   note = {cited By 66},
   pages = {12407-12417},
   title = {How does disagreement help generalization against label corruption?},
   volume = {2019-June},
   year = {2019},
}
@article{Zhou2010,
   author = {Z.-H. Zhou and M Li},
   doi = {10.1007/s10115-009-0209-z},
   issue = {3},
   journal = {Knowledge and Information Systems},
   note = {cited By 292},
   pages = {415-439},
   title = {Semi-supervised learning by disagreement},
   volume = {24},
   year = {2010},
}
@inproceedings{Ke2019,
   author = {Z Ke and D Wang and Q Yan and J Ren and R Lau},
   doi = {10.1109/ICCV.2019.00683},
   journal = {Proceedings of the IEEE International Conference on Computer Vision},
   note = {cited By 42},
   pages = {6727-6735},
   title = {Dual student: Breaking the limits of the teacher in semi-supervised learning},
   volume = {2019-October},
   year = {2019},
}
@article{Peng2020,
   author = {J Peng and G Estrada and M Pedersoli and C Desrosiers},
   doi = {10.1016/j.patcog.2020.107269},
   journal = {Pattern Recognition},
   note = {cited By 45},
   title = {Deep co-training for semi-supervised image segmentation},
   volume = {107},
   year = {2020},
}
@article{Qiao2018,
   author = {S Qiao and W Shen and Z Zhang and B Wang and A Yuille},
   journal = {ECCV},
   note = {cited By 105},
   pages = {135-152},
   title = {Deep co-training for semi-supervised image recognition},
   year = {2018},
}
@inproceedings{Ouali2020,
   author = {Y Ouali and C Hudelot and M Tami},
   doi = {10.1109/CVPR42600.2020.01269},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   note = {cited By 88},
   pages = {12671-12681},
   title = {Semi-supervised semantic segmentation with cross-consistency training},
   year = {2020},
}
@inproceedings{,
   author = {P Cascante-Bonilla and F Tan and Y Qi and V Ordonez},
   journal = {35th AAAI Conference on Artificial Intelligence, AAAI 2021},
   note = {cited By 7},
   pages = {6912-6920},
   title = {Curriculum Labeling: Revisiting Pseudo-Labeling for Semi-Supervised Learning},
   volume = {8A},
   year = {2021},
}
@inproceedings{Bengio2009,
   author = {Y Bengio and J Louradour and R Collobert and J Weston},
   doi = {10.1145/1553374.1553380},
   journal = {ACM International Conference Proceeding Series},
   note = {cited By 781},
   title = {Curriculum learning},
   volume = {382},
   year = {2009},
}
@article{Zou2018,
   author = {Y Zou and Z Yu and B V K Vijaya Kumar and J Wang},
   journal = {Proceedings of the European Conference on Computer Vision (ECCV)},
   note = {cited By 362},
   pages = {289-305},
   title = {Unsupervised domain adaptation for semantic segmentation via class-balanced self-training},
   year = {2018},
}
@article{Mittal2019,
   author = {Sudhanshu Mittal and Maxim Tatarchenko and Thomas Brox},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   note = {cited By 44},
   pages = {1},
   title = {Semi-supervised semantic segmentation with high-and low-level consistency},
   year = {2019},
}
@inproceedings{Berthelot2019,
   author = {D Berthelot and N Carlini and I Goodfellow and A Oliver and N Papernot and C Raffel},
   journal = {Advances in Neural Information Processing Systems},
   note = {cited By 467},
   title = {MixMatch: A holistic approach to semi-supervised learning},
   volume = {32},
   year = {2019},
}
@article{French2019,
   author = {Geoffrey French and Timo Aila and Samuli Laine and Michal Mackiewicz and Graham D Finlayson},
   journal = {Semi-Supervised Semantic Segmentation Needs Strong, High-Dimensional Perturbations},
   note = {cited By 45},
   title = {Semi-supervised semantic segmentation needs strong, high-dimensional perturbations},
   year = {2019},
}
@inproceedings{Tarvainen2017,
   author = {A Tarvainen and H Valpola},
   journal = {Advances in Neural Information Processing Systems},
   note = {cited By 905},
   pages = {1196-1205},
   title = {Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results},
   volume = {2017-December},
   year = {2017},
}
@inproceedings{Hung2019,
   author = {W.-C. Hung and Y.-H. Tsai and Y.-T. Liou and Y.-Y. Lin and M.-H. Yang},
   journal = {British Machine Vision Conference 2018, BMVC 2018},
   note = {cited By 78},
   title = {Adversarial learning for semi-supervised semantic segmentation},
   year = {2019},
}
@article{Lee2013,
   author = {D.-H. Lee},
   journal = {Workshop on Challenges in Representation Learning, ICML},
   note = {cited By 1033},
   title = {Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks},
   volume = {3},
   year = {2013},
}
@inproceedings{Cordts2016,
   author = {M Cordts and M Omran and S Ramos and T Rehfeld and M Enzweiler and R Benenson and U Franke and S Roth and B Schiele},
   doi = {10.1109/CVPR.2016.350},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   note = {cited By 4454},
   pages = {3213-3223},
   title = {The Cityscapes Dataset for Semantic Urban Scene Understanding},
   volume = {2016-December},
   year = {2016},
}
@article{Yarowsky1995,
   author = {D Yarowsky},
   journal = {Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics},
   note = {cited By 1398},
   pages = {189-196},
   title = {Unsupervised word sense disambiguation rivaling supervised methods},
   year = {1995},
}
@inproceedings{Zhao2017,
   author = {H Zhao and J Shi and X Qi and X Wang and J Jia},
   doi = {10.1109/CVPR.2017.660},
   journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
   note = {cited By 4740},
   pages = {6230-6239},
   title = {Pyramid scene parsing network},
   volume = {2017-January},
   year = {2017},
}
@inproceedings{Zagoruyko2016,
   author = {S Zagoruyko and N Komodakis},
   doi = {10.5244/C.30.87},
   journal = {British Machine Vision Conference 2016, BMVC 2016},
   note = {cited By 1165},
   pages = {87.1-87.12},
   title = {Wide Residual Networks},
   volume = {2016-September},
   year = {2016},
}
@article{Wu2019,
   author = {Z Wu and C Shen and A van den Hengel},
   doi = {10.1016/j.patcog.2019.01.006},
   journal = {Pattern Recognition},
   note = {cited By 455},
   pages = {119-133},
   title = {Wider or Deeper: Revisiting the ResNet Model for Visual Recognition},
   volume = {90},
   year = {2019},
}
@report{,
   abstract = {Semi-supervised learning (SSL) provides a powerful framework for leveraging unlabeled data when labels are limited or expensive to obtain. SSL algorithms based on deep neural networks have recently proven successful on standard benchmark tasks. However, we argue that these benchmarks fail to address many issues that SSL algorithms would face in real-world applications. After creating a unified reimplementation of various widely-used SSL techniques, we test them in a suite of experiments designed to address these issues. We find that the performance of simple baselines which do not use unlabeled data is often underreported, SSL methods differ in sensitivity to the amount of labeled and unlabeled data, and performance can degrade substantially when the unlabeled dataset contains out-of-distribution examples. To help guide SSL research towards real-world applicability, we make our unified reimplemention and evaluation platform publicly available. 2},
   author = {Avital Oliver and Augustus Odena and Colin Raffel and Ekin D Cubuk and Ian J Goodfellow Google Brain},
   title = {Realistic Evaluation of Deep Semi-Supervised Learning Algorithms},
   url = {https://github.com/brain-research/realistic-ssl-evaluation},
}
@report{,
   abstract = {In this paper we revisit the idea of pseudo-labeling in the context of semi-supervised learning where a learning algorithm has access to a small set of labeled samples and a large set of unlabeled samples. Pseudo-labeling works by applying pseudo-labels to samples in the unlabeled set by using a model trained on the combination of the labeled samples and any previously pseudo-labeled samples, and iteratively repeating this process in a self-training cycle. Current methods seem to have abandoned this approach in favor of consistency regularization methods that train models under a combination of different styles of self-supervised losses on the unlabeled samples and standard supervised losses on the labeled samples. We empirically demonstrate that pseudo-labeling can in fact be competitive with the state-of-the-art, while being more resilient to out-of-distribution samples in the unlabeled set. We identify two key factors that allow pseudo-labeling to achieve such remarkable results (1) applying curriculum learning principles and (2) avoiding concept drift by restarting model parameters before each self-training cycle. We obtain 94.91% accuracy on CIFAR-10 using only 4, 000 labeled samples, and 68.87% top-1 accuracy on Imagenet-ILSVRC using only 10% of the labeled samples. The code is available at https://github.com/uvavision/Curriculum-Labeling.},
   author = {Paola Cascante-Bonilla and Fuwen Tan and Yanjun Qi and Vicente Ordonez},
   title = {Curriculum Labeling: Revisiting Pseudo-Labeling for Semi-Supervised Learning},
   url = {www.aaai.org},
   year = {2021},
}
@report{,
   abstract = {Semi-supervised learning (SSL) provides an effective means of leveraging unla-beled data to improve a model's performance. This domain has seen fast progress recently, at the cost of requiring more complex methods. In this paper we propose FixMatch, an algorithm that is a significant simplification of existing SSL methods. FixMatch first generates pseudo-labels using the model's predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, we show that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with 40-just 4 labels per class. We carry out an extensive ablation study to tease apart the experimental factors that are most important to FixMatch's success. The code is available at https://github.com/google-research/fixmatch.},
   author = {Kihyuk Sohn and David Berthelot and Chun-Liang Li and Zizhao Zhang and Nicholas Carlini and Ekin D Cubuk and Alex Kurakin and Han Zhang and Colin Raffel},
   title = {FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence},
   url = {https://github.com/google-research/fixmatch.},
}
@report{,
   abstract = {We present Meta Pseudo Labels, a semi-supervised learning method that achieves a new state-of-the-art top-1 accuracy of 90.2% on ImageNet, which is 1.6% better than the existing state-of-the-art [16]. Like Pseudo Labels, Meta Pseudo Labels has a teacher network to generate pseudo labels on unlabeled data to teach a student network. However, unlike Pseudo Labels where the teacher is fixed, the teacher in Meta Pseudo Labels is constantly adapted by the feedback of the student's performance on the labeled dataset. As a result, the teacher generates better pseudo labels to teach the student. 1},
   author = {Hieu Pham and Zihang Dai and Qizhe Xie and Minh-Thang Luong and Quoc V Le},
   title = {Meta Pseudo Labels},
}
@inproceedings{,
   author = {Dong-Hyun Lee and others},
   title = {Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks},
}
@report{,
   abstract = {Data augmentation is a widely used trick when training deep neural networks: in addition to the original data, properly transformed data are also added to the training set. However, to the best of our knowledge, a clear mathematical framework to explain the performance benefits of data augmentation is not available. In this paper, we develop such a theoretical framework. We show data augmentation is equivalent to an averaging operation over the orbits of a certain group that keeps the data distribution approximately invariant. We prove that it leads to variance reduction. We study empirical risk minimization, and the examples of exponential families, linear regression, and certain two-layer neural networks. We also discuss how data augmentation could be used in problems with symmetry where other approaches are prevalent, such as in cryo-electron microscopy (cryo-EM).},
   author = {Shuxiao Chen and Edgar Dobriban and Jane H Lee},
   keywords = {Data Augmentation,Deep Learning,Empirical Risk Minimization,Invari-ance,Variance Reduction},
   title = {A Group-Theoretic Framework for Data Augmentation},
   url = {https://paperswithcode.com/sota},
}
@book{Chapelle2006,
   author = {Olivier Chapelle and Bernhard Schölkopf and Alexander Zien},
   isbn = {0262033585},
   publisher = {The MIT Press},
   title = {Semi-Supervised Learning (Adaptive Computation and Machine Learning)},
   year = {2006},
}
@inproceedings{Khoreva2017,
   abstract = {Semantic labelling and instance segmentation are two tasks that require particularly costly annotations. Starting from weak supervision in the form of bounding box detection annotations, we propose a new approach that does not require modification of the segmentation training procedure. We show that when carefully designing the input labels from given bounding boxes, even a single round of training is enough to improve over previously reported weakly supervised results. Overall, our weak supervision approach reaches ∼95% of the quality of the fully supervised model, both for semantic labelling and instance segmentation.},
   author = {A. Khoreva and R. Benenson and J. Hosang and M. Hein and B. Schiele},
   doi = {10.1109/CVPR.2017.181},
   isbn = {9781538604571},
   journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
   pages = {1665-1674},
   title = {Simple does It: Weakly supervised instance and semantic segmentation},
   volume = {2017-Janua},
   year = {2017},
}
@article{Wang2022,
   abstract = {In this paper, we aim to tackle semi-and-weakly supervised semantic segmentation (SWSSS), where many image-level classification labels and a few pixel-level annotations are available. We believe the most crucial point for solving SWSSS is to produce high-quality pseudo labels, and our method deals with it from two perspectives. Firstly, we introduce a class-aware cross entropy (CCE) loss for network training. Compared to conventional cross entropy loss, CCE loss encourages the model to distinguish concurrent classes only and simplifies the learning target of pseudo label generation. Secondly, we propose a progressive cross training (PCT) method to build cross supervision between two networks with a dynamic evaluation mechanism, which progressively introduces high-quality predictions as additional supervision for network training. Our method significantly improves the quality of generated pseudo labels in the regime with extremely limited annotations. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods significantly. The code is released for public access.},
   author = {Y. Wang and J. Zhang and M. Kan and S. Shan},
   doi = {10.1016/j.patcog.2022.108925},
   journal = {Pattern Recognition},
   title = {Learning pseudo labels for semi-and-weakly supervised semantic segmentation},
   volume = {132},
   year = {2022},
}
@article{Peng2020,
   abstract = {In this paper, we aim to improve the performance of semantic image segmentation in a semi-supervised setting where training is performed with a reduced set of annotated images and additional non-annotated images. We present a method based on an ensemble of deep segmentation models. Models are trained on subsets of the annotated data and use non-annotated images to exchange information with each other, similar to co-training. Diversity across models is enforced with the use of adversarial samples. We demonstrate the potential of our method on three challenging image segmentation problems, and illustrate its ability to share information between simultaneously trained models, while preserving their diversity. Results indicate clear advantages in terms of performance compared to recently proposed semi-supervised methods for segmentation.},
   author = {J. Peng and G. Estrada and M. Pedersoli and C. Desrosiers},
   doi = {10.1016/j.patcog.2020.107269},
   journal = {Pattern Recognition},
   title = {Deep co-training for semi-supervised image segmentation},
   volume = {107},
   year = {2020},
}
@article{Gao2021,
   abstract = {Powered by the ImageNet dataset, unsupervised learning on large-scale data
has made significant advances for classification tasks. There are two major
challenges to allowing such an attractive learning modality for segmentation
tasks: i) a large-scale benchmark for assessing algorithms is missing; ii)
unsupervised category/shape representation learning is difficult. We propose a
new problem of large-scale unsupervised semantic segmentation (LUSS) with a
newly created benchmark dataset to track the research progress. Based on the
ImageNet dataset, we propose the ImageNet-S dataset with 1.2 million training
images and 50k high-quality semantic segmentation annotations for evaluation.
Our benchmark has a high data diversity and a clear task objective. We also
present a simple yet effective method that works surprisingly well for LUSS. In
addition, we benchmark related un/weakly/fully supervised methods accordingly,
identifying the challenges and possible directions of LUSS.},
   author = {Shanghua Gao and Zhong-Yu Li and Ming-Hsuan Yang and Ming-Ming Cheng and Junwei Han and Philip Torr},
   doi = {10.48550/arxiv.2106.03149},
   month = {6},
   title = {Large-scale Unsupervised Semantic Segmentation},
   url = {https://arxiv.org/abs/2106.03149},
   year = {2021},
}
@inproceedings{Abadi2016,
   abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, generalpurpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that Tensor-Flow achieves for several real-world applications.},
   author = {M. Abadi and P. Barham and J. Chen and Z. Chen and A. Davis and J. Dean and M. Devin and S. Ghemawat and G. Irving and M. Isard and Y. Yu and X. Zheng},
   isbn = {9781931971331},
   journal = {Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016},
   pages = {265-283},
   title = {TensorFlow: A system for large-scale machine learning},
   year = {2016},
}
@article{Huang2021,
   abstract = {Peak Response Map (PRM) highlighting the discriminative regions can be extracted from a pre-trained classification network. We can accurately localize instances of each class with the help of these response maps. However, these maps cannot provide reliable information for segmentation even with off-the-shelf object proposals. This is because neither PRM nor the proposals know which regions can be regarded as a complete instance. In this paper, we tackle this problem by proposing an Instance-aware Cue-propagation Network (ICN) with a new proposal-matching strategy. In particular, the ICN aims to filter out background distractions and cover the complete instance, while our proposed proposal-matching strategy adds a re-balancing constraint on the contributions of multi-scale object proposals. Extensive experiments conducted on the PASCAL VOC 2012 dataset show the superior performance of our method over weakly-supervised state-of-the-arts for both semantic and instance segmentation.},
   author = {Xin Huang and Qianshu Zhu and Yongtuo Liu and Shengfeng He},
   doi = {10.1016/j.neucom.2021.02.093},
   issn = {18728286},
   journal = {Neurocomputing},
   keywords = {Instance segmentation,Weakly supervised learning},
   month = {8},
   pages = {1-9},
   publisher = {Elsevier B.V.},
   title = {Weakly supervised segmentation via instance-aware propagation},
   volume = {447},
   year = {2021},
}
@article{Zhu2016,
   abstract = {Image segmentation refers to the process to divide an image into meaningful non-overlapping regions according to human perception, which has become a classic topic since the early ages of computer vision. A lot of research has been conducted and has resulted in many applications. While many segmentation algorithms exist, there are only a few sparse and outdated summarizations available. Thus, in this paper, we aim to provide a comprehensive review of the recent progress in the field. Covering 190 publications, we give an overview of broad segmentation topics including not only the classic unsupervised methods, but also the recent weakly-/semi-supervised methods and the fully-supervised methods. In addition, we review the existing influential datasets and evaluation metrics. We also suggest some design choices and research directions for future research in image segmentation.},
   author = {Hongyuan Zhu and Fanman Meng and Jianfei Cai and Shijian Lu},
   doi = {10.1016/J.JVCIR.2015.10.012},
   issn = {1047-3203},
   journal = {Journal of Visual Communication and Image Representation},
   keywords = {Image cosegmentation,Image segmentation,Interactive image segmentation,Object proposal,Semantic image parsing,Superpixel,Unsupervised image segmentation,Weakly-supervised image segmentation},
   month = {1},
   pages = {12-27},
   publisher = {Academic Press},
   title = {Beyond pixels: A comprehensive survey from bottom-up to semantic image segmentation and cosegmentation},
   volume = {34},
   year = {2016},
}
@report{,
   abstract = {This survey gives an overview over different techniques used for pixel-level semantic segmentation. Metrics and datasets for the evaluation of segmenta-tion algorithms and traditional approaches for segmen-tation such as unsupervised methods, Decision Forests and SVMs are described and pointers to the relevant papers are given. Recently published approaches with convolutional neural networks are mentioned and typical problematic situations for segmentation algorithms are examined. A taxonomy of segmentation algorithms is given.},
   author = {Martin Thoma},
   title = {A Survey of Semantic Segmentation},
}
@article{Yu2018,
   abstract = {Semantic segmentation, also called scene labeling, refers to the process of assigning a semantic label (e.g. car, people, and road) to each pixel of an image. It is an essential data processing step for robots and other unmanned systems to understand the surrounding scene. Despite decades of efforts, semantic segmentation is still a very challenging task due to large variations in natural scenes. In this paper, we provide a systematic review of recent advances in this field. In particular, three categories of methods are reviewed and compared, including those based on hand-engineered features, learned features and weakly supervised learning. In addition, we describe a number of popular datasets aiming for facilitating the development of new segmentation algorithms. In order to demonstrate the advantages and disadvantages of different semantic segmentation models, we conduct a series of comparisons between them. Deep discussions about the comparisons are also provided. Finally, this review is concluded by discussing future directions and challenges in this important field of research.},
   author = {Hongshan Yu and Zhengeng Yang and Lei Tan and Yaonan Wang and Wei Sun and Mingui Sun and Yandong Tang},
   doi = {10.1016/J.NEUCOM.2018.03.037},
   issn = {0925-2312},
   journal = {Neurocomputing},
   keywords = {3D point clouds labeling,Convolutional neural network,Markov random fields,Semantic segmentation,Weakly supervised method},
   month = {8},
   pages = {82-103},
   publisher = {Elsevier},
   title = {Methods and datasets on semantic segmentation: A review},
   volume = {304},
   year = {2018},
}
@article{Sehar2022,
   author = {Uroosa Sehar and Muhammad Luqman Naseem},
   doi = {10.1007/s11042-022-12821-3},
   issn = {1380-7501},
   journal = {Multimedia Tools and Applications},
   month = {4},
   title = {How deep learning is empowering semantic segmentation},
   year = {2022},
}
@article{,
   abstract = {Remote sensing image classification plays a significant role in urban applications, precision agriculture, water resource management. The task of classification in the field of remote sensing is to map raw images to semantic maps. Typically, fully convolutional network (FCN) is one of the most effective deep neural networks for semantic segmentation. However, small objects in remote sensing images can be easily overlooked and misclassified as the majority label, which is often the background of the image. Although many works have attempted to deal with this problem, making a trade-off between background semantics and edge details is still a problem. This is mainly because they are based on a single neural network model. To deal with this problem, a convolutional deep network with regions (R-CNN), which is highly effective for object detection is leveraged as a complementary component in our work. A learning-based and decision-level strategy is applied to fuse both semantic maps from a semantic model and an object detection model. The proposed network is referred to as Mask-R-FCN. Experimental results on real remote sensing images from the Zurich dataset, Gaofen Image Dataset (GID), and DataFountain2017 show that the proposed network can obtain higher accuracy than single deep neural networks and other machine learning algorithms. The proposed network achieved better average accuracies, which are approximately 2% higher than those of any other single deep neural networks on the Zurich, GID, and DataFoundation2017 datasets. INDEX TERMS Deep fusion, deep semantic segmentation, fully convolutional network, object detection, remote sensing.},
   author = {Yunfeng Zhang and Mingmin Chi},
   doi = {10.1109/ACCESS.2020.3012701},
   title = {Mask-R-FCN: A Deep Fusion Network for Semantic Segmentation},
}
@article{Guo2018,
   abstract = {During the long history of computer vision, one of the grand challenges has been semantic segmentation which is the ability to segment an unknown image into different parts and objects (e.g., beach, ocean, sun, dog, swimmer). Furthermore, segmentation is even deeper than object recognition because recognition is not necessary for segmentation. Specifically, humans can perform image segmentation without even knowing what the objects are (for example, in satellite imagery or medical X-ray scans, there may be several objects which are unknown, but they can still be segmented within the image typically for further investigation). Performing segmentation without knowing the exact identity of all objects in the scene is an important part of our visual understanding process which can give us a powerful model to understand the world and also be used to improve or augment existing computer vision techniques. Herein this work, we review the field of semantic segmentation as pertaining to deep convolutional neural networks. We provide comprehensive coverage of the top approaches and summarize the strengths, weaknesses and major challenges.},
   author = {Yanming Guo and Yu Liu and Theodoros Georgiou and Michael S Lew},
   doi = {10.1007/s13735-017-0141-z},
   journal = {International Journal of Multimedia Information Retrieval},
   keywords = {Computer vision ·,Convolutional neural networks ·,Deep learning ·,Image segmentation ·,Machine learning},
   pages = {87-93},
   title = {A review of semantic segmentation using deep neural networks},
   volume = {7},
   url = {https://doi.org/10.1007/s13735-017-0141-z},
   year = {2018},
}
@article{Lateef2019,
   abstract = {Semantic segmentation is a challenging task in computer vision systems. A lot of methods have been developed to tackle this problem ranging from autonomous vehicles, human-computer interaction, to robotics, medical research, agriculture and so on. Many of these methods have been built using the deep learning paradigm that has shown a salient performance. For this reason, we propose to survey these methods by, first categorizing them into ten different classes according to the common concepts underlying their architectures. Second, by providing an overview of the publicly available datasets on which they have been assessed. In addition, we present the common evaluation matrix used to measure their accuracy. Moreover, we focus on some of the methods and look closely at their architectures in order to find out how they have achieved their reported performances. Finally, we conclude by discussing some of the open problems and their possible solutions.},
   author = {Fahad Lateef and Yassine Ruichek},
   doi = {10.1016/J.NEUCOM.2019.02.003},
   issn = {0925-2312},
   journal = {Neurocomputing},
   keywords = {Deep learning,Recurrent neural network,Semantic segmentation,Semi-weakly supervised networks},
   month = {4},
   pages = {321-348},
   publisher = {Elsevier},
   title = {Survey on semantic segmentation using deep learning techniques},
   volume = {338},
   year = {2019},
}
@article{Sarker2021,
   abstract = {Deep learning (DL), a branch of machine learning (ML) and artificial intelligence (AI) is nowadays considered as a core technology of today's Fourth Industrial Revolution (4IR or Industry 4.0). Due to its learning capabilities from data, DL technology originated from artificial neural network (ANN), has become a hot topic in the context of computing, and is widely applied in various application areas like healthcare, visual recognition, text analytics, cybersecurity, and many more. However, building an appropriate DL model is a challenging task, due to the dynamic nature and variations in real-world problems and data. Moreover, the lack of core understanding turns DL methods into black-box machines that hamper development at the standard level. This article presents a structured and comprehensive view on DL techniques including a taxonomy considering various types of real-world tasks like supervised or unsupervised. In our taxonomy, we take into account deep networks for supervised or discriminative learning, unsupervised or generative learning as well as hybrid learning and relevant others. We also summarize real-world application areas where deep learning techniques can be used. Finally, we point out ten potential aspects for future generation DL modeling with research directions. Overall, this article aims to draw a big picture on DL modeling that can be used as a reference guide for both academia and industry professionals.},
   author = {Iqbal H Sarker},
   doi = {10.1007/s42979-021-00815-1},
   isbn = {0123456789},
   keywords = {Artificial intelligence,Artificial neural network,Deep learning,Discriminative learning,Generative learning,Hybrid learning,Intelligent systems},
   pages = {420},
   title = {Deep Learning: A Comprehensive Overview on Techniques, Taxonomy, Applications and Research Directions},
   volume = {2},
   url = {https://doi.org/10.1007/s42979-021-00815-1},
   year = {2021},
}
@article{Khan123,
   abstract = {Deep Convolutional Neural Network (CNN) is a special type of Neural Networks, which has shown exemplary performance on several competitions related to Computer Vision and Image Processing. Some of the exciting application areas of CNN include Image Classification and Segmentation, Object Detection, Video Processing, Natural Language Processing , and Speech Recognition. The powerful learning ability of deep CNN is primarily due to the use of multiple feature extraction stages that can automatically learn representations from the data. The availability of a large amount of data and improvement in the hardware technology has accelerated the research in CNNs, and recently interesting deep CNN archi-tectures have been reported. Several inspiring ideas to bring advancements in CNNs have been explored, such as the use of different activation and loss functions, parameter optimization , regularization, and architectural innovations. However, the significant improvement in the representational capacity of the deep CNN is achieved through architectural innovations. Notably, the ideas of exploiting spatial and channel information, depth and width of architecture, and multi-path information processing have gained substantial attention. Similarly , the idea of using a block of layers as a structural unit is also gaining popularity. This survey thus focuses on the intrinsic taxonomy present in the recently reported deep CNN architectures and, consequently, classifies the recent innovations in CNN architectures into seven different categories. These seven categories are based on spatial exploitation, depth, multi-path, width, feature-map exploitation, channel boosting, and attention. Additionally, the elementary understanding of CNN components, current challenges, and applications of CNN are also provided.},
   author = {Asifullah Khan and Anabia Sohail and Umme Zahoora and · Aqsa and Saeed Qureshi and A Khan},
   doi = {10.1007/s10462-020-09825-6},
   isbn = {0123456789},
   journal = {Artificial Intelligence Review},
   keywords = {Channel boosted CNN,Convolutional neural networks,Deep learning,Representational capacity,Residual learning,Taxonomy},
   pages = {5455-5516},
   title = {A survey of the recent architectures of deep convolutional neural networks},
   volume = {53},
   url = {https://doi.org/10.1007/s10462-020-09825-6},
   year = {123},
}
@report{,
   abstract = {Deep neural networks demonstrated their ability to provide remarkable performances on a wide range of supervised learning tasks (e.g., image classification) when trained on extensive collections of labeled data (e.g., ImageNet). However, creating such large datasets requires a considerable amount of resources, time, and effort. Such resources may not be available in many practical cases, limiting the adoption and the application of many deep learning methods. In a search for more data-efficient deep learning methods to overcome the need for large annotated datasets, there is a rising research interest in semi-supervised learning and its applications to deep neural networks to reduce the amount of labeled data required, by either developing novel methods or adopting existing semi-supervised learning frameworks for a deep learning setting. In this paper, we provide a comprehensive overview of deep semi-supervised learning, starting with an introduction to the field, followed by a summarization of the dominant semi-supervised approaches in deep learning 1 .},
   author = {Yassine Ouali and Céline Hudelot and Myriam Tami},
   keywords = {consistency training,deep learning,entropy mini-mization,generative models,graph neural networks,neural networks,proxy labeling,semi-supervised learning},
   title = {An Overview of Deep Semi-Supervised Learning},
}
@report{,
   abstract = {In many real-world scenarios, labeled data for a specific machine learning task is costly to obtain. Semi-supervised training methods make use of abundantly available unla-beled data and a smaller number of labeled examples. We propose a new framework for semi-supervised training of deep neural networks inspired by learning in humans. "As-sociations" are made from embeddings of labeled samples to those of unlabeled ones and back. The optimization schedule encourages correct association cycles that end up at the same class from which the association was started and penalizes wrong associations ending at a different class. The implementation is easy to use and can be added to any existing end-to-end training setup. We demonstrate the capabilities of learning by association on several data sets and show that it can improve performance on classification tasks tremendously by making use of additionally available unlabeled data. In particular, for cases with few labeled data, our training scheme outperforms the current state of the art on SVHN.},
   author = {Philip Haeusser and Alexander Mordvintsev and Daniel Cremers},
   title = {Learning by Association A versatile semi-supervised training method for neural networks},
   url = {https://git.io/vyzrl},
}
@book_section{Ma2019,
   author = {Zhixian Ma and Jie Zhu and Yongkai Zhu and Haiguang Xu},
   doi = {10.1007/978-981-32-9563-6_20},
   pages = {191-200},
   title = {Classification of Radio Galaxy Images with Semi-supervised Learning},
   year = {2019},
}
@article{Sen2022,
   author = {Snigdha Sen and Sonali Agarwal and Pavan Chakraborty and Krishna Pratap Singh},
   doi = {10.1007/s10686-021-09827-4},
   issn = {0922-6435},
   issue = {1},
   journal = {Experimental Astronomy},
   month = {2},
   pages = {1-43},
   title = {Astronomical big data processing using machine learning: A comprehensive review},
   volume = {53},
   year = {2022},
}
@article{Slijepcevic2022,
   abstract = {<p>In this work, we examine the classification accuracy and robustness of a state-of-the-art semi-supervised learning (SSL) algorithm applied to the morphological classification of radio galaxies. We test if SSL with fewer labels can achieve test accuracies comparable to the supervised state of the art and whether this holds when incorporating previously unseen data. We find that for the radio galaxy classification problem considered, SSL provides additional regularization and outperforms the baseline test accuracy. However, in contrast to model performance metrics reported on computer science benchmarking data sets, we find that improvement is limited to a narrow range of label volumes, with performance falling off rapidly at low label volumes. Additionally, we show that SSL does not improve model calibration, regardless of whether classification is improved. Moreover, we find that when different underlying catalogues drawn from the same radio survey are used to provide the labelled and unlabelled data sets required for SSL, a significant drop in classification performance is observed, highlighting the difficulty of applying SSL techniques under data set shift. We show that a class-imbalanced unlabelled data pool negatively affects performance through prior probability shift, which we suggest may explain this performance drop, and that using the Fréchet distance between labelled and unlabelled data sets as a measure of data set shift can provide a prediction of model performance, but that for typical radio galaxy data sets with labelled sample volumes of $\mathcal \{O\}(10^3)$, the sample variance associated with this technique is high and the technique is in general not sufficiently robust to replace a train–test cycle.</p>},
   author = {Inigo V Slijepcevic and Anna M M Scaife and Mike Walmsley and Micah Bowles and O Ivy Wong and Stanislav S Shabala and Hongming Tang},
   doi = {10.1093/mnras/stac1135},
   issn = {0035-8711},
   issue = {2},
   journal = {Monthly Notices of the Royal Astronomical Society},
   month = {6},
   pages = {2599-2613},
   title = {Radio Galaxy Zoo: using semi-supervised learning to leverage large unlabelled data sets for radio galaxy classification under data set shift},
   volume = {514},
   year = {2022},
}
@report{Slijepcevic2022,
   abstract = {Unknown class distributions in unlabelled astro-physical training data have previously been shown to detrimentally affect model performance due to dataset shift between training and validation sets. For radio galaxy classification, we demonstrate in this work that removing low angular extent sources from the unlabelled data before training produces qualitatively different training dynamics for a contrastive model. By applying the model on an unlabelled data-set with unknown class balance and sub-population distribution to generate a representation space of radio galaxies, we show that with an appropriate cut threshold we can find a representation with FRI/FRII class separation approaching that of a supervised baseline explicitly trained to separate radio galaxies into these two classes. Furthermore we show that an excessively conservative cut threshold blocks any increase in validation accuracy. We then use the learned representation for the downstream task of performing a similarity search on rare hybrid sources, finding that the contrastive model can reliably return semantically similar samples, with the added bonus of finding duplicates which remain after pre-processing.},
   author = {Inigo Val Slijepcevic and Anna M M Scaife and Mike Walmsley and Micah Bowles},
   title = {Learning useful representations for radio astronomy "in the wild" with contrastive learning},
   year = {2022},
}
@article{Triguero2015,
   author = {Isaac Triguero and Salvador García and Francisco Herrera},
   doi = {10.1007/s10115-013-0706-y},
   issn = {0219-1377},
   issue = {2},
   journal = {Knowledge and Information Systems},
   month = {2},
   pages = {245-284},
   title = {Self-labeled techniques for semi-supervised learning: taxonomy, software and empirical study},
   volume = {42},
   year = {2015},
}
@report{,
   abstract = {We investigate the generalization of semi-supervised learning (SSL) to diverse pixel-wise tasks. Although SSL methods have achieved impressive results in image classification, the performances of applying them to pixel-wise tasks are unsatisfactory due to their need for dense outputs. In addition, existing pixel-wise SSL approaches are only suitable for certain tasks as they usually require to use task-specific properties. In this paper, we present a new SSL framework, named Guided Collab-orative Training (GCT), for pixel-wise tasks, with two main technical contributions. First, GCT addresses the issues caused by the dense outputs through a novel flaw detector. Second, the modules in GCT learn from unlabeled data collaboratively through two newly proposed constraints that are independent of task-specific properties. As a result, GCT can be applied to a wide range of pixel-wise tasks without structural adaptation. Our extensive experiments on four challenging vision tasks, including semantic segmentation, real image denoising, portrait image matting, and night image enhancement, show that GCT outper-forms state-of-the-art SSL methods by a large margin. Our code available at: https://github.com/ZHKKKe/PixelSSL (i) .},
   author = {Zhanghan Ke and Di Qiu and Kaican Li and Qiong Yan and Rynson W H Lau},
   keywords = {Learning ·,Pixel-wise,Semi-,Supervised,Tasks,Vision},
   title = {Guided Collaborative Training for Pixel-wise Semi-Supervised Learning},
   url = {https://github.com/ZHKKKe/PixelSSL},
}
@report{,
   abstract = {Generative Adversarial Networks (GANs) are known to be difficult to train, despite considerable research effort. Several regularization techniques for stabilizing training have been proposed, but they introduce non-trivial computational overheads and interact poorly with existing techniques like spectral normalization. In this work, we propose a simple, effective training stabilizer based on the notion of consistency regularization-a popular technique in the semi-supervised learning literature. In particular, we augment data passing into the GAN discriminator and penalize the sensitivity of the discriminator to these augmentations. We conduct a series of experiments to demonstrate that consistency regularization works effectively with spectral normalization and various GAN architectures, loss functions and optimizer settings. Our method achieves the best FID scores for unconditional image generation compared to other regularization methods on CIFAR-10 and CelebA. Moreover, Our consistency regularized GAN (CR-GAN) improves state-of-the-art FID scores for conditional generation from 14.73 to 11.48 on CIFAR-10 and from 8.73 to 6.66 on ImageNet-2012.},
   author = {Han Zhang and Zizhao Zhang and Augustus Odena and Honglak Lee and Google Research},
   title = {CONSISTENCY REGULARIZATION FOR GENERATIVE ADVERSARIAL NETWORKS},
}
@report{Englesson2021,
   abstract = {Consistency regularization is a commonly-used technique for semi-supervised and self-supervised learning. It is an auxiliary objective function that encourages the prediction of the network to be similar in the vicinity of the observed training samples. Hendrycks et al. (2020) have recently shown such regularization naturally brings test-time robustness to corrupted data and helps with calibration. This paper empirically studies the relevance of consistency regularization for training-time robustness to noisy labels. First, we make two interesting and useful observations regarding the consistency of networks trained with the standard cross entropy loss on noisy datasets which are: (i) networks trained on noisy data have lower consistency than those trained on clean data, and (ii) the consistency reduces more significantly around noisy-labelled training data points than correctly-labelled ones. Then, we show that a simple loss function that encourages consistency improves the robustness of the models to label noise on both synthetic (CIFAR-10, CIFAR-100) and real-world (WebVision) noise as well as different noise rates and types and achieves state-of-the-art results.},
   author = {Erik Englesson and Hossein Azizpour},
   title = {Consistency Regularization Can Improve Robustness to Label Noise},
   year = {2021},
}
@report{,
   abstract = {The trend towards increasingly deep neural networks has been driven by a general observation that increasing depth increases the performance of a network. Recently, however, evidence has been amassing that simply increasing depth may not be the best way to increase performance, particularly given other limitations. Investigations into deep residual networks have also suggested that they may not in fact be operating as a single deep network, but rather as an ensemble of many relatively shallow networks. We examine these issues, and in doing so arrive at a new interpretation of the unravelled view of deep residual networks which explains some of the behaviours that have been observed experimentally. As a result, we are able to derive a new, shallower , architecture of residual networks which significantly outperforms much deeper models such as ResNet-200 on the ImageNet classification dataset. We also show that this performance is transferable to other problem domains by developing a semantic segmentation approach which out-performs the state-of-the-art by a remarkable margin on datasets including PASCAL VOC, PASCAL Context, and Cityscapes. The architecture that we propose thus outper-forms its comparators, including very deep ResNets, and yet is more efficient in memory use and sometimes also in training time. The code and models are available at https://github.com/itijyou/ademxapp.},
   author = {Zifeng Wu and Chunhua Shen and Anton Van Den Hengel},
   isbn = {1611.10080v1},
   title = {Wider or Deeper: Revisiting the ResNet Model for Visual Recognition *},
   url = {https://github.com/itijyou/ademxapp.},
}
@article{,
   abstract = {In recent years, the need for semantic segmentation has arisen across several different applications and environments. However, the expense and redundancy of annotation often limits the quantity of labels available for training in any domain, while deployment is easier if a single model works well across domains. In this paper, we pose the novel problem of universal semi-supervised semantic segmenta-tion and propose a solution framework, to meet the dual needs of lower annotation and deployment costs. In contrast to counterpoints such as fine tuning, joint training or unsupervised domain adaptation, universal semi-supervised segmentation ensures that across all domains: (i) a single model is deployed, (ii) unlabeled data is used, (iii) performance is improved, (iv) only a few labels are needed and (v) label spaces may differ. To address this, we minimize supervised as well as within and cross-domain unsupervised losses, introducing a novel feature alignment objective based on pixel-aware entropy regularization for the latter. We demonstrate quantitative advantages over other approaches on several combinations of segmentation datasets across different geographies (Germany, England, India) and environments (outdoors, indoors), as well as qualitative insights on the aligned representations 1 .},
   author = {Tarun Kalluri and Girish Varma and Manmohan Chandraker and C V Jawahar},
   title = {Universal Semi-Supervised Semantic Segmentation},
   url = {https://github.com/tarun005/USSS},
}
@article{Cheng2020,
   abstract = {There are several supervised machine learning methods used for the application of automated morphological classification of galaxies; however, there has not yet been a clear comparison of these different methods using imaging data, or an investigation for maximizing their effectiveness. We carry out a comparison between several common machine learning methods for galaxy classification [Convolutional Neural Network (CNN), K-nearest neighbour, logistic regression, Support Vector Machine, Random Forest, and Neural Networks] by using Dark Energy Survey (DES) data combined with visual classifications from the Galaxy Zoo 1 project (GZ1). Our goal is to determine the optimal machine learning methods when using imaging data for galaxy classification. We show that CNN is the most successful method of these ten methods in our study. Using a sample of ∼2800 galaxies with visual classification from GZ1, we reach an accuracy of ∼0.99 for the morphological classification of ellipticals and spirals. The further investigation of the galaxies that have a different ML and visual classification but with high predicted probabilities in our CNN usually reveals the incorrect classification provided by GZ1. We further find the galaxies having a low probability of being either spirals or ellipticals are visually lenticulars (S0), demonstrating that supervised learning is able to rediscover that this class of galaxy is distinct from both ellipticals and spirals. We confirm that ∼2.5 per cent galaxies are misclassified by GZ1 in our study. After correcting these galaxies' labels, we improve our CNN performance to an average accuracy of over 0.99 (accuracy of 0.994 is our best result).},
   author = {Ting-Yun Cheng and Christopher J Conselice and Alfonso Aragón-Salamanca and Nan Li and Asa F L Bluck and Will G Hartley and James Annis and David Brooks and Peter Doel and Juan García-Bellido and David J James and Kyler Kuehn and Nikolay Kuropatkin and Mathew Smith and Flavia Sobreira and Gregory Tarle},
   doi = {10.1093/mnras/staa501},
   journal = {MNRAS},
   keywords = {data analysis-methods,methods,statistical-galaxies,structure},
   pages = {4209-4228},
   title = {Optimizing automatic morphological classification of galaxies with machine learning and deep learning using Dark Energy Survey imaging},
   volume = {493},
   url = {https://academic.oup.com/mnras/article/493/3/4209/5740728},
   year = {2020},
}
@report{Lupton2004,
   abstract = {We present a new, and we believe arguably correct, algorithm for producing red-green-blue (RGB) composites from three-band astronomical images. Our method ensures that an object with a specified astronomical color (e.g., gr and ri) has a unique color in the RGB image, as opposed to the burnt-out white stars to which we are accustomed. A natural consequence of this is that we can use the same colors to code color-magnitude diagrams, providing a natural "index" to our images. We also introduce the use of an arcsinh stretch that allows us to show faint objects while simultaneously preserving the structure of brighter objects in the field, such as the spiral arms of large galaxies. We believe that in addition to their aesthetic value, our images convey far more information than do the traditional ones, and we provide examples from Sloan Digital Sky Survey (SDSS) imaging, the Hubble Deep Field (HDF), and Chandra to support our claims. 4 The purest and most thoughtful minds are those which love colour the most.-John Ruskin, The Stones of Venice (1852)},
   author = {Robert Lupton and Michael R Blanton and George Fekete and David W Hogg and Wil O'mullane and Alex Szalay and Nicholas Wherry},
   journal = {Publications of the Astronomical Society of the Pacific},
   pages = {133-137},
   title = {Preparing Red-Green-Blue Images from CCD Data},
   volume = {116},
   url = {http://www.astro.princeton.edu/},
   year = {2004},
}
@article{Fukushima1982,
   abstract = {Suggested by the structure of the visual nervous system, a new algorithm is proposed for pattern recognition. This algorithm can be realized with a multilayered network consisting of neuron-like cells. The network, "neocognitron", is self-organized by unsupervised learning, and acquires the ability to recognize stimulus patterns according to the differences in their shapes: Any patterns which we human beings judge to be alike are also judged to be of the same category by the neocognitron. The neocognitron recognizes stimulus patterns correctly without being affected by shifts in position or even by considerable distortions in shape of the stimulus patterns. © 1982.},
   author = {Kunihiko Fukushima and Sei Miyake},
   doi = {10.1016/0031-3203(82)90024-3},
   issn = {00313203},
   issue = {6},
   journal = {Pattern Recognition},
   title = {Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position},
   volume = {15},
   year = {1982},
}
@generic{LeCun1989,
   abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
   author = {Y`ann LeCun and Bernhard Boser and John S Denker and Donnie Henderson and Richard E Howard and Wayne Hubbard and Lawrence D Jackel},
   issue = {4},
   journal = {Neural computation},
   pages = {541-551},
   title = {Backpropagation applied to digit recognition},
   volume = {1},
   url = {https://www.ics.uci.edu/~welling/teaching/273ASpring09/lecun-89e.pdf},
   year = {1989},
}
@article{Rumelhart1986,
   abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure 1. © 1986 Nature Publishing Group.},
   author = {David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
   doi = {10.1038/323533a0},
   issn = {00280836},
   issue = {6088},
   journal = {Nature},
   title = {Learning representations by back-propagating errors},
   volume = {323},
   year = {1986},
}
@article{Srivastava2014,
   abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets. © 2014 Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever and Ruslan Salakhutdinov.},
   author = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
   issn = {15337928},
   journal = {Journal of Machine Learning Research},
   title = {Dropout: A simple way to prevent neural networks from overfitting},
   volume = {15},
   year = {2014},
}
@report{,
   abstract = {Letting a deep network be aware of the quality of its own predictions is an interesting yet important problem. In the task of instance segmentation, the confidence of instance classification is used as mask quality score in most instance segmentation frameworks. However, the mask quality , quantified as the IoU between the instance mask and its ground truth, is usually not well correlated with classification score. In this paper, we study this problem and propose Mask Scoring R-CNN which contains a network block to learn the quality of the predicted instance masks. The proposed network block takes the instance feature and the corresponding predicted mask together to regress the mask IoU. The mask scoring strategy calibrates the misalignment between mask quality and mask score, and improves instance segmentation performance by prioritizing more accurate mask predictions during COCO AP evaluation. By extensive evaluations on the COCO dataset, Mask Scoring R-CNN brings consistent and noticeable gain with different models, and outperforms the state-of-the-art Mask R-CNN. We hope our simple and effective approach will provide a new direction for improving instance segmentation. The source code of our method is available at https:// github.com/zjhuang22/maskscoring_rcnn.},
   author = {Zhaojin Huang and Lichao Huang and Yongchao Gong and Chang Huang and Xinggang Wang},
   title = {Mask Scoring R-CNN},
}
@report{,
   abstract = {Image Processing in Astronomy is a major field of research and involves a lot of techniques pertaining to improve analyzing the properties of the celestial objects or obtaining preliminary inference from the image data. In this paper, we provide a comprehensive case study of advanced image processing techniques applied to Astronomical Galaxy Images for improved analysis, accurate inferences and faster analysis.},
   author = {Diganta Misra and Sparsha Mishra and Bhargav Appasani},
   keywords = {Astronomy,Elliptical Galaxy,Image Processing,Segmentation},
   title = {Advanced Image Processing for Astronomical Images},
}
@report{,
   abstract = {In this paper, we present UNet++, a new, more powerful architecture for medical image segmentation. Our architecture is essentially a deeply-supervised encoder-decoder network where the encoder and de-coder sub-networks are connected through a series of nested, dense skip pathways. The redesigned skip pathways aim at reducing the semantic gap between the feature maps of the encoder and decoder sub-networks. We argue that the optimizer would deal with an easier learning task when the feature maps from the decoder and encoder networks are semantically similar. We have evaluated UNet++ in comparison with U-Net and wide U-Net architectures across multiple medical image segmentation tasks: nodule segmentation in the low-dose CT scans of chest, nuclei segmen-tation in the microscopy images, liver segmentation in abdominal CT scans, and polyp segmentation in colonoscopy videos. Our experiments demonstrate that UNet++ with deep supervision achieves an average IoU gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.},
   author = {Zongwei Zhou and Rahman Siddiquee and Nima Tajbakhsh and Jianming Liang},
   title = {UNet++: A Nested U-Net Architecture for Medical Image Segmentation},
}
@article{Liu2021,
   abstract = {To detect, track and characterize solar filaments more accurately, novel filament segmentation methods based on improved U-Nets are proposed. The full-disk Hα images from the Huairou Solar Observing Station of the National Astronomical Observatory and the Big Bear Solar Observatory were used for training and verifying the effectiveness of different improved networks’ filament segmentation performance. Comparative experiments with different solar dataset sizes and input image quality were performed. The impact of each improvement method on the segmentation effect was analyzed and compared based on experimental results. In order to further explore the influence of network depth on filament-segmentation accuracy, the segmentation results produced by Conditional Generative Adversarial Networks (CGAN) were obtained and compared with improved U-nets. Experiments verified that U-Net with an Atrous Spatial Pyramid Pooling Module performs better for high-quality input solar images regardless of dataset sizes. CGAN performs better for low-quality input solar images with large dataset size. The algorithm may provide guidance for filament segmentation and more accurate segmentation results with less noise were acquired.},
   author = {Dan Liu and Wei Song and Ganghua Lin and Haimin Wang},
   doi = {10.1007/s11207-021-01920-3},
   issn = {1573093X},
   issue = {12},
   journal = {Solar Physics},
   title = {Solar Filament Segmentation Based on Improved U-Nets},
   volume = {296},
   year = {2021},
}
@inproceedings{Zhu2015,
   abstract = {The image processing is becoming a key issue in astronomical dataanalysis. This paper introduces the algorithms and methods applied in astronomicalimage processing from different aspects. The paper first shows methodsof lossless compression of astronomical images, like the pyramidal mediantransform, segment-based DPCM and 5/3 integer wavelet transform. Secondly itshows the algorithms of astronomical image segmentation, edge detection andde-noising. Finally, many different methods of image recovery and restoration areintroduced briefly. We summarize a number of recent achievements on astronomicalimage processing in this survey, and list the recent published papers.},
   author = {Hai Jing Zhu and Bo Chong Han and Bo Qiu},
   doi = {10.1007/978-3-319-21969-1_37},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   title = {Survey of astronomical image processing methods},
   volume = {9219},
   year = {2015},
}
@article{Niblack1986,
   abstract = {A textbook designed to guide students through the theory and practice of digital image processing. Examples are mainly satellite imagery (particularly Landsat TM data of Copenhagen), but medical and astronomical images are also included. Each chapter is balanced between the theory of the approach and its applications. Topics include: image display, filtering, Fourier transform, segmentation, geometric operations, and classifications. A mathematical background and some basic software written in PL/I, Pascal and Fortran are also included.-R.Harris image},
   author = {W. Niblack},
   doi = {10.1007/978-3-030-88439-0_14},
   journal = {An introduction to digital image processing.},
   title = {An introduction to digital image processing.},
   year = {1986},
}
@book_section{Tozza2019,
   abstract = {Astronomical images are of crucial importance for astronomers since they contain a lot of information about celestial bodies that can not be directly accessible. Most of the information available for the analysis of these objects starts with sky explorations via telescopes and satellites. Unfortunately, the quality of astronomical images is usually very low with respect to other real images and this is due to technical and physical features related to their acquisition process. This increases the percentage of noise and makes more difficult to use directly standard segmentation methods on the original image. In this work we will describe how to process astronomical images in two steps: in the first step we improve the image quality by a rescaling of light intensity whereas in the second step we apply level-set methods to identify the objects. Several experiments will show the effectiveness of this procedure and the results obtained via various discretization techniques for level-set equations.},
   author = {Silvia Tozza and Maurizio Falcone},
   doi = {10.1007/978-3-030-32882-5_7},
   issn = {22815198},
   journal = {Springer INdAM Series},
   title = {On the Segmentation of Astronomical Images via Level-Set Methods},
   volume = {36},
   year = {2019},
}
@article{Xavier2012,
   author = {Gintu Xavier},
   doi = {10.9790/0661-0652129},
   issn = {22788727},
   issue = {5},
   journal = {IOSR Journal of Computer Engineering},
   title = {An Efficient Algorithm for the Segmentation of Astronomical Images},
   volume = {6},
   year = {2012},
}
@article{Sultana2020,
   abstract = {From the autonomous car driving to medical diagnosis, the requirement of the task of image segmentation is everywhere. Segmentation of an image is one of the indispensable tasks in computer vision. This task is comparatively complicated than other vision tasks as it needs low-level spatial information. Basically, image segmentation can be of two types: semantic segmentation and instance segmentation. The combined version of these two basic tasks is known as panoptic segmentation. In the recent era, the success of deep convolutional neural networks (CNN) has influenced the field of segmentation greatly and gave us various successful models to date. In this survey, we are going to take a glance at the evolution of both semantic and instance segmentation work based on CNN. We have also specified comparative architectural details of some state-of-the-art models and discuss their training details to present a lucid understanding of hyper-parameter tuning of those models. We have also drawn a comparison among the performance of those models on different datasets. Lastly, we have given a glimpse of some state-of-the-art panoptic segmentation models.},
   author = {Farhana Sultana and Abu Sufian and Paramartha Dutta},
   doi = {10.1016/j.knosys.2020.106062},
   issn = {09507051},
   journal = {Knowledge-Based Systems},
   title = {Evolution of Image Segmentation using Deep Convolutional Neural Network: A Survey},
   volume = {201-202},
   year = {2020},
}
@article{Hafiz2020,
   abstract = {Object detection or localization is an incremental step in progression from coarse to fine digital image inference. It not only provides the classes of the image objects, but also provides the location of the image objects which have been classified. The location is given in the form of bounding boxes or centroids. Semantic segmentation gives fine inference by predicting labels for every pixel in the input image. Each pixel is labelled according to the object class within which it is enclosed. Furthering this evolution, instance segmentation gives different labels for separate instances of objects belonging to the same class. Hence, instance segmentation may be defined as the technique of simultaneously solving the problem of object detection as well as that of semantic segmentation. In this survey paper on instance segmentation, its background, issues, techniques, evolution, popular datasets, related work up to the state of the art and future scope have been discussed. The paper provides valuable information for those who want to do research in the field of instance segmentation.},
   author = {Abdul Mueed Hafiz and Ghulam Mohiuddin Bhat},
   doi = {10.1007/s13735-020-00195-x},
   issn = {2192662X},
   issue = {3},
   journal = {International Journal of Multimedia Information Retrieval},
   title = {A survey on instance segmentation: state of the art},
   volume = {9},
   year = {2020},
}
@inproceedings{Chen2018,
   abstract = {In this work, we tackle the problem of instance segmentation, the task of simultaneously solving object detection and semantic segmentation. Towards this goal, we present a model, called MaskLab, which produces three outputs: box detection, semantic segmentation, and direction prediction. Building on top of the Faster-RCNN object detector, the predicted boxes provide accurate localization of object instances. Within each region of interest, MaskLab performs foreground/background segmentation by combining semantic and direction prediction. Semantic segmentation assists the model in distinguishing between objects of different semantic classes including background, while the direction prediction, estimating each pixel's direction towards its corresponding center, allows separating instances of the same semantic class. Moreover, we explore the effect of incorporating recent successful methods from both segmentation and detection (e.g., atrous convolution and hypercolumn). Our proposed model is evaluated on the COCO instance segmentation benchmark and shows comparable performance with other state-of-art models.},
   author = {Liang Chieh Chen and Alexander Hermans and George Papandreou and Florian Schroff and Peng Wang and Hartwig Adam},
   doi = {10.1109/CVPR.2018.00422},
   issn = {10636919},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   title = {MaskLab: Instance Segmentation by Refining Object Detection with Semantic and Direction Features},
   year = {2018},
}
@article{Pal1993,
   abstract = {Many image segmentation techniques are available in the literature. Some of these techniques use only the gray level histogram, some use spatial details while others use fuzzy set theoretic approaches. Most of these techniques are not suitable for noisy environments. Some works have been done using the Markov Random Field (MRF) model which is robust to noise, but is computationally involved. Neural network architectures which help to get the output in real time because of their parallel processing ability, have also been used for segmentation and they work fine even when the noise level is very high. The literature on color image segmentation is not that rich as it is for gray tone images. This paper critically reviews and summarizes some of these techniques. Attempts have been made to cover both fuzzy and non-fuzzy techniques including color image segmentation and neural network based approaches. Adequate attention is paid to segmentation of range images and magnetic resonance images. It also addresses the issue of quantitative evaluation of segmentation results. © 1993.},
   author = {Nikhil R. Pal and Sankar K. Pal},
   doi = {10.1016/0031-3203(93)90135-J},
   issn = {0031-3203},
   issue = {9},
   journal = {Pattern Recognition},
   keywords = {Clustering,Edge detection,Fuzzy sets,Image segmentation,Markov Random Field,Relaxation,Thresholding},
   month = {9},
   pages = {1277-1294},
   publisher = {Pergamon},
   title = {A review on image segmentation techniques},
   volume = {26},
   year = {1993},
}
@article{Mohan2021,
   abstract = {<p>Understanding the scene in which an autonomous robot operates is critical for its competent functioning. Such scene comprehension necessitates recognizing instances of traffic participants along with general scene semantics which can be effectively addressed by the panoptic segmentation task. In this paper, we introduce the Efficient Panoptic Segmentation (EfficientPS) architecture that consists of a shared backbone which efficiently encodes and fuses semantically rich multi-scale features. We incorporate a new semantic head that aggregates fine and contextual features coherently and a new variant of Mask R-CNN as the instance head. We also propose a novel panoptic fusion module that congruously integrates the output logits from both the heads of our EfficientPS architecture to yield the final panoptic segmentation output. Additionally, we introduce the KITTI panoptic segmentation dataset that contains panoptic annotations for the popularly challenging KITTI benchmark. Extensive evaluations on Cityscapes, KITTI, Mapillary Vistas and Indian Driving Dataset demonstrate that our proposed architecture consistently sets the new state-of-the-art on all these four benchmarks while being the most efficient and fast panoptic segmentation architecture to date.</p>},
   author = {Rohit Mohan and Abhinav Valada},
   doi = {10.1007/s11263-021-01445-z},
   issn = {0920-5691},
   issue = {5},
   journal = {International Journal of Computer Vision},
   month = {5},
   pages = {1551-1579},
   title = {EfficientPS: Efficient Panoptic Segmentation},
   volume = {129},
   year = {2021},
}
@report{,
   abstract = {We propose and study a task we name panoptic segmen-tation (PS). Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete , an important step toward real-world vision systems. While early work in computer vision addressed related im-age/scene parsing tasks, these are not currently popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. The aim of our work is to revive the interest of the community in a more unified view of image segmentation.},
   author = {Alexander Kirillov and Kaiming He and Ross Girshick and Carsten Rother and Piotr Dollár},
   title = {Panoptic Segmentation},
}
@book_section{Peng2010,
   author = {Bo Peng and Lei Zhang and Jian Yang},
   doi = {10.1007/978-3-642-12304-7_64},
   pages = {677-686},
   title = {Iterated Graph Cuts for Image Segmentation},
   year = {2010},
}
@article{R2011,
   author = {Muthukrishnan R},
   doi = {10.5121/ijcsit},
   journal = {International journal of computer science and information technology},
   month = {8},
   pages = {259-267},
   title = {Edge Detection Techniques For Image Segmentation},
   volume = {3},
   year = {2011},
}
@report{,
   abstract = {Clustering algorithms have successfully been applied as a digital image segmentation technique in various fields and applications. However, those clustering algorithms are only applicable for specific images such as medical images, microscopic images etc. In this paper, we present a new clustering algorithm called Adaptive Fuzzy-K-means (AFKM) clustering for image segmentation which could be applied on general images and/or specific images (i.e., medical and microscopic images), captured using different consumer electronic products namely, for example, the common digital cameras and CCD cameras. The algorithm employs the concepts of fuzziness and belongingness to provide a better and more adaptive clustering process as compared to several conventional clustering algorithms. Both qualitative and quantitative analyses favour the proposed AFKM algorithm in terms of providing a better segmentation performance for various types of images and various number of segmented regions. Based on the results obtained, the proposed algorithm gives better visual quality as compared to several other clustering methods. 1 .},
   author = {Siti Noraini Sulaiman and Nor Ashidi Mat Isa},
   doi = {10.1109/TCE.2010.5681154},
   keywords = {Index Terms-Adaptive Fuzzy-K-means Clustering (AFKM),clustering,digital image processing,image segmentation},
   title = {S. N. Sulaiman and N. A. M. Isa: Adaptive Fuzzy-K-means Clustering Algorithm for Image Segmentation Adaptive Fuzzy-K-means Clustering Algorithm for Image Segmentation},
   year = {2010},
}
@article{Tao2007,
   abstract = {In this correspondence, we develop a novel approach that provides effective and robust segmentation of color images. By incorporating the advantages of the mean shift (MS) segmentation and the normalized cut (Ncut) partitioning methods, the proposed method requires low computational complexity and is therefore very feasible for real-time image segmentation processing. It preprocesses an image by using the MS algorithm to form segmented regions that preserve the desirable discontinuity characteristics of the image. The segmented regions are then represented by using the graph structures, and the Ncut method is applied to perform globally optimized clustering. Because the number of the segmented regions is much smaller than that of the image pixels, the proposed method allows a low-dimensional image clustering with significant reduction of the complexity compared to conventional graph-partitioning methods that are directly applied to the image pixels. In addition, the image clustering using the segmented regions, instead of the image pixels, also reduces the sensitivity to noise and results in enhanced image segmentation performance. Furthermore, to avoid some inappropriate partitioning when considering every region as only one graph node, we develop an improved segmentation strategy using multiple child nodes for each region. The superiority of the proposed method is examined and demonstrated through a large number of experiments using color natural scene images. Index Terms-Color image segmentation, graph partitioning, mean shift (MS), normalized cut (Ncut).},
   author = {Wenbing Tao and Hai Jin and Senior Member and Yimin Zhang},
   doi = {10.1109/TSMCB.2007.902249},
   isbn = {2007.902249},
   issue = {5},
   journal = {CYBERNETICS},
   title = {Color Image Segmentation Based on Mean Shift and Normalized Cuts},
   volume = {37},
   url = {http://ieeexplore.ieee.org.},
   year = {2007},
}
@report{,
   abstract = {This paper presents a new unsupervised method image segmentation, and there are various methods, which based on the Expectation-Maximization (EM) algorithm that can be roughly categorized as three typical ways: (1) color we apply for color image segmentation. The method firstly space clustering based segmentation, (2) edge or contour Convert Image from RGB Color Space to HSV Color Space; detection based segmentation and (3) region or area Secondly we make use of a model of mixture K Gaussians, the extraction based segmentation [4, 5, 6, and 7]. Expectation Maximization (EM) fornula is used to estimate The usually RGB(Red, green, and Blue) color space the parameters of the Gaussian Mixture Model (GMM), which allows for a very straightforward representation of colors the desired number of partitions and fits the image histogram but, unfortunately, it has a Riemannian nature; this means using a mixture of Gaussian distributions and provides a that it is not a uniform space and perceived differences classified image; Thirdly, those pixels that have similar features will be regarded a group; Finally, for each group we among colrsc be assessed n by an of segment pixels again according to their positions and we can CompLicatd Mer s. er colorspae,sc as rcIe-LAB get segmentation regions of the image. Experiment shows this CI-U an Musl ofe imrvd pcptl method has better segmentation performance. The results of uniformity [8]. In general they represent with equal our methods are separately segmented and their combination emphasis the three color variants that characterize color: allows the color image to be eventually partitioned. hue, lightness, and saturation. This separation is attractive because color image processing performed independently Index Terms-Color segmentation, Expectation-Maximization on the color channels does not introduce false colors [9]. (EM) algorithm, Gaussian Mixture Model (GMM) and histogram Furthermore, it is easier to compensate for many artifacts and color distortions. For example, lighting and shading I. INTRODUCTION artifacts will typically be isolated to the lightness channel. In a great number of fields of computer vision image In general, these color spaces are often inconvenient due to segmentation plays an essential role as a preliminary step the basic non-linearity in forward and reverse towards further and higher levels of image processing. transformations with RGB space. For color extraction we Segmentation is an operation of paramount importance in a utilize the more tractable HSV (Hue, Saturation, and Value) number of image processing applications. Color image color space because it has the above mentioned segmentation has been receiving an increasing attention characteristics and the transformation from RGB space is only in recent years, as color images convey more non-linear but easily invertible. information than gray-level images and allow one to obtain In this paper, we first Convert Image from RGB Color more meaningful and robust segmentation. At the same Space to HSV Color Space, then use a Gaussian mixture time, color is known to be a strong cue in attracting an model (GMM) to approximate the color histogram of each observer's attention. Actually, the motivation for using subband of color image. We use the color for segmentation comes from the fact that it provides Expectation-Maximization (EM) algorithm [11] to estimate region information and that when specified appropriately, it the parameters of GMM model; EM algorithm returns an can be relatively insensitive to variation in illumination index corresponding to a cluster; label Every Pixel in the conditions and appearances of objects [1]. Image Using the Results from index, the resulting During the last three decades hundreds of algorithms pixel-cluster memberships provide a segmentation of the for different aspects of feature detection and image color image. This method is computationally inexpensive, segmentation have been developed. A great deal of and also works effectively for color image segmentation. techniques has been proposed for color images II. OVERVIEW HSV COLOR SPACE AND EM ALGORITHM segmentation [2]. A comprehensive review of segmentation methods isgiven in [3]. In other words, color image AVRIWSCLRPC segmentation can be viewed as an extension of gray level A color space is a model for representing color in 1-4244-1220-X/07/$25.OO ©C2007 IEEE 316 Authorized licensed use limited to: UNIVERSIDAD DE GRANADA. Downloaded on August 10,2022 at 15:31:43 UTC from IEEE Xplore. Restrictions apply.},
   author = {Zhi-Kai Huang and De-Hui Liu},
   isbn = {142441220X},
   title = {Segmentation of Color Image Using EM algorithm in HSV Color Space},
}
@inproceedings{,
   author = {H.P. Ng and S.H. Ong and K.W.C. Foong and P.S. Goh and W.L. Nowinski},
   doi = {10.1109/SSIAI.2006.1633722},
   isbn = {1-4244-0069-4},
   journal = {2006 IEEE Southwest Symposium on Image Analysis and Interpretation},
   pages = {61-65},
   publisher = {IEEE},
   title = {Medical Image Segmentation Using K-Means Clustering and Improved Watershed Algorithm},
}
@article{,
   author = {Bo Peng and Lei Zhang and D. Zhang},
   doi = {10.1109/TIP.2011.2157512},
   issn = {1057-7149},
   issue = {12},
   journal = {IEEE Transactions on Image Processing},
   month = {12},
   pages = {3592-3605},
   title = {Automatic Image Segmentation by Dynamic Region Merging},
   volume = {20},
   year = {2011},
}
@article{Davis1975,
   author = {Larry S. Davis and Azriel Rosenfeld and Joan S. Weszka},
   doi = {10.1109/TSMC.1975.5408419},
   issn = {0018-9472},
   issue = {3},
   journal = {IEEE Transactions on Systems, Man, and Cybernetics},
   pages = {383-388},
   title = {Region Extraction by Averaging and Thresholding},
   volume = {SMC-5},
   year = {1975},
}
@article{Girshick2014,
   abstract = {在规范的 PASCAL VOC 数据集上测量的物体检测性能 在过去几年中已经趋于稳定。表现最佳的方法是复杂的集成 系统，通常将多个低级图像特征与高级上下文相结合。在本 文中，我们提出了一种简单且可扩展的检测算法，相对于之 前对 VOC 2012 的最佳结果，平均精度（mAP）提高了 30％以上-实现了 53.3％的 mAP。我们的方法结合了两个 关键知识： （1）可以将大容量卷积神经网络（CNN）应用于自下 而上的区域生成，以便对象进行定位和分割;（2）当标记的 训练数据稀缺时，监督辅助任务的预训练，随后进行特定领 域的微调，产生显著的性能提升。由于我们将区域生成与 CNN 结合起来，我们将方法称为 R-CNN：具有 CNN 功能 的区域。我们还将 R-CNN 与最近提出的基于类似 CNN 架 构的滑 动窗 口检 测器 OverFeat 进行了 比较 。我 们发现 RCNN 在 200 级 ILSVRC2013 检测数据集上大大优于 OverFeat。整个系统的源代码可在以下位置获得： 1. 介绍 特征很重要。 各种视觉识别任务的最后十年进展基于 SIFT [29]和 HOG [7]的使用。 但是，如果我们看一下规范 视觉识别任务 PASCAL VOC 物体检测[15]的表现，人们普 遍认为 2010-2012 年进展缓慢，因为这一阶段主要建立整 体系统和设计成功方法的微小变种。 SIFT 和 HOG 是块状方向直方图，我们可以粗略地与 V1 中的复杂细胞相关联，这是灵长类动物视觉路径中的第一个 皮层区域。 但我们也知道识别发生在下游的几个阶段，这 表明可能存在多等级的， warped region aeroplane? no.. |person? yes. CNN tvmonitor? no. 1. Input 2. Extract region 3. Compute 4. Classify image proposals (~2k) CNN features regions 图 1：对象检测系统概述。 我们的系统（1）采用输入图像，（2） 提取大约 2000 个自下而上区域生成，（3）使用大型卷积神经网络 （CNN）计算每个生成的特征，然后（4）使用特定的线性分类器 SVM 对每个区域进行分类。 R-CNN 在 PASCAL VOC 2010 上实 现了 53.7％的平均精确度（mAP）。相比之下，[39]使用相同的区 域生成，但是具有空间金字塔和视觉词袋路径，只得到了 35.1％的 mAP。 流行的可变形的组件模型的得分为 33.4％。 在 200 级 ILSVRC2013 检测数据集中，RCNN 的 mAP 为 31.4 ％，比 OverFeat [34]有很大改进，其中之前的最佳结果为 24.3％。 多阶段的计算特征的过程，这些过程对于视觉识别更具信息 性。 福岛的"neocognitron"[19]，一种用于模式识别的生 物学启发的分层和移位不变模型，是这种过程的早期尝试。 然而，新认知缺乏监督训练算法。以 Rumelhart 等人[33] 为基础，LeCun 等人[26]表明，通过反向传播的随机梯度下 降对于训练卷积神经网络（CNN）是有效的，CNN 是一类 扩展 neocognitron 的模型。 CNN 在 20 世纪 90 年代得到了大量使用（例如， [27] ），但随着支持向量机的兴起，它们已经过时了。 2012 年，Krizhevsky 等人[25]通过在 ImageNet 大规模视 觉识别挑战（ILSVRC）上显示出更高的图像分类准确度， 重新燃起了对 CNN 的兴趣[9,10]。他们的成功是由于荷兰 国际列车集团的基于 120 万个带标记图像的超大 CNN，对 LeCun 的 CNN 部分曲折在一起（例如，MAX（X,0）,修正 非线性以及 Dropout 正则化等）。 ImageNet 结果的重要意义引起了非常强烈的讨论 1},
   author = {Ross Girshick and Jeff Donahue and Trevor Darrell and Jitendra Malik},
   journal = {Proceedings of the ieee conference on computer vision and pattern recognition},
   title = {R-CNN: Regions with CNN features},
   url = {http://gwylab.com/pdf/rcnn_chs.pdf},
   year = {2014},
}
@article{,
   author = {Jorge Núñez and Jorge Llacer},
   doi = {10.1016/S0083-6656(96)00040-2},
   issn = {00836656},
   issue = {4},
   journal = {Vistas in Astronomy},
   month = {1},
   pages = {547-553},
   title = {Image reconstruction with variable resolution using Gaussian Invariant functions in a segmentation process},
   volume = {40},
   year = {1996},
}
@article{Young1998,
   author = {C. K. Young and M. J. Currie},
   doi = {10.1051/aas:1998107},
   issn = {0365-0138},
   issue = {3},
   journal = {Astronomy and Astrophysics Supplement Series},
   month = {2},
   pages = {367-395},
   title = {The "Virgo photometry catalogue”; a catalogue of 1180 galaxies in the direction of the Virgo Cluster's core},
   volume = {127},
   year = {1998},
}
@article{,
   author = {M. Sanchez Cuberes and J. A. Bonet and M. Vazquez and A. D. Wittmann},
   doi = {10.1086/309155},
   issn = {0004-637X},
   issue = {2},
   journal = {The Astrophysical Journal},
   month = {8},
   pages = {940-959},
   title = {Center‐to‐Limb Variation of Solar Granulation from Partial Eclipse Observations},
   volume = {538},
   year = {2000},
}
@article{Alokasi2022,
   abstract = {<p>Semantic segmentation using machine learning and computer vision techniques is one of the most popular topics in autonomous driving-related research. With the revolution of deep learning, the need for more efficient and accurate segmentation systems has increased. This paper presents a detailed review of deep learning-based frameworks used for semantic segmentation of road scenes, highlighting their architectures and tasks. It also discusses well-known standard datasets that evaluate semantic segmentation systems in addition to new datasets in the field. To overcome a lack of enough data required for the training process, data augmentation techniques and their experimental results are reviewed. Moreover, domain adaptation methods that have been deployed to transfer knowledge between different domains in order to reduce the domain gap are presented. Finally, this paper provides quantitative analysis and performance evaluation and discusses the results of different frameworks on the reviewed datasets and highlights future research directions in the field of semantic segmentation using deep learning.</p>},
   author = {Haneen Alokasi and Muhammad Bilal Ahmad},
   doi = {10.3390/electronics11121884},
   issn = {2079-9292},
   issue = {12},
   journal = {Electronics},
   month = {6},
   pages = {1884},
   title = {Deep Learning-Based Frameworks for Semantic Segmentation of Road Scenes},
   volume = {11},
   year = {2022},
}
@article{Takos2020,
   abstract = {Semantic image segmentation is one of fastest growing areas in computer
vision with a variety of applications. In many areas, such as robotics and
autonomous vehicles, semantic image segmentation is crucial, since it provides
the necessary context for actions to be taken based on a scene understanding at
the pixel level. Moreover, the success of medical diagnosis and treatment
relies on the extremely accurate understanding of the data under consideration
and semantic image segmentation is one of the important tools in many cases.
Recent developments in deep learning have provided a host of tools to tackle
this problem efficiently and with increased accuracy. This work provides a
comprehensive analysis of state-of-the-art deep learning architectures in image
segmentation and, more importantly, an extensive list of techniques to achieve
fast inference and computational efficiency. The origins of these techniques as
well as their strengths and trade-offs are discussed with an in-depth analysis
of their impact in the area. The best-performing architectures are summarized
with a list of methods used to achieve these state-of-the-art results.},
   author = {Georgios Takos},
   doi = {10.48550/arxiv.2009.12942},
   month = {9},
   title = {A Survey on Deep Learning Methods for Semantic Image Segmentation in Real-Time},
   url = {https://arxiv.org/abs/2009.12942},
   year = {2020},
}
@article{,
   abstract = {Image semantic segmentation is more and more being of interest for computer
vision and machine learning researchers. Many applications on the rise need
accurate and efficient segmentation mechanisms: autonomous driving, indoor
navigation, and even virtual or augmented reality systems to name a few. This
demand coincides with the rise of deep learning approaches in almost every
field or application target related to computer vision, including semantic
segmentation or scene understanding. This paper provides a review on deep
learning methods for semantic segmentation applied to various application
areas. Firstly, we describe the terminology of this field as well as mandatory
background concepts. Next, the main datasets and challenges are exposed to help
researchers decide which are the ones that best suit their needs and their
targets. Then, existing methods are reviewed, highlighting their contributions
and their significance in the field. Finally, quantitative results are given
for the described methods and the datasets in which they were evaluated,
following up with a discussion of the results. At last, we point out a set of
promising future works and draw our own conclusions about the state of the art
of semantic segmentation using deep learning techniques.},
   author = {Alberto Garcia-Garcia and Sergio Orts-Escolano and Sergiu Oprea and Victor Villena-Martinez and Jose Garcia-Rodriguez},
   doi = {10.48550/arxiv.1704.06857},
   month = {4},
   title = {A Review on Deep Learning Techniques Applied to Semantic Segmentation},
   url = {https://arxiv.org/abs/1704.06857},
   year = {2017},
}
@report{,
   abstract = {Much of the recent progress made in image classification research can be credited to training procedure refinements, such as changes in data augmentations and optimization methods. In the literature, however, most refinements are either briefly mentioned as implementation details or only visible in source code. In this paper, we will examine a collection of such refinements and empirically evaluate their impact on the final model accuracy through ablation study. We will show that, by combining these refinements together, we are able to improve various CNN models significantly. For example, we raise ResNet-50's top-1 validation accuracy from 75.3% to 79.29% on ImageNet. We will also demonstrate that improvement on image classification accuracy leads to better transfer learning performance in other application domains such as object detection and semantic segmentation.},
   author = {Tong He and Zhi Zhang and Hang Zhang and Zhongyue Zhang and Junyuan Xie and Mu Li},
   title = {Bag of Tricks for Image Classification with Convolutional Neural Networks},
   url = {https://github.com/dmlc/gluon-cv},
}
@report{,
   abstract = {Deep semi-supervised learning is a fast-growing field with a range of practical applications. This paper provides a comprehensive survey on both fundamentals and recent advances in deep semi-supervised learning methods from perspectives of model design and unsupervised loss functions. We first present a taxonomy for deep semi-supervised learning that categorizes existing methods, including deep generative methods, consistency regularization methods, graph-based methods, pseudo-labeling methods, and hybrid methods. Then we provide a comprehensive review of 52 representative methods and offer a detailed comparison of these methods in terms of the type of losses, contributions, and architecture differences. In addition to the progress in the past few years, we further discuss some shortcomings of existing methods and provide some tentative heuristic solutions for solving these open problems.},
   author = {Xiangli Yang and Zixing Song and Irwin King and Zenglin Xu and Senior Member},
   keywords = {Index Terms-Deep semi-supervised learning,image classification,machine learning,survey !},
   title = {A Survey on Deep Semi-supervised Learning},
}
@article{,
   abstract = {Semi-supervised learning is the branch of machine learning concerned with using labelled as well as unlabelled data to perform certain learning tasks. Conceptually situated between supervised and unsupervised learning, it permits harnessing the large amounts of unlabelled data available in many use cases in combination with typically smaller sets of labelled data. In recent years, research in this area has followed the general trends observed in machine learning, with much attention directed at neural network-based models and generative learning. The literature on the topic has also expanded in volume and scope, now encompassing a broad spectrum of theory, algorithms and applications. However, no recent surveys exist to collect and organize this knowledge, impeding the ability of researchers and engineers alike to utilize it. Filling this void, we present an up-to-date overview of semi-supervised learning methods, covering earlier work as well as more recent advances. We focus primarily on semi-supervised classification, where the large majority of semi-supervised learning research takes place. Our survey aims to provide researchers and practitioners new to the field as well as more advanced readers with a solid understanding of the main approaches and algorithms developed over the past two decades, with an emphasis on the most prominent and currently relevant work. Furthermore, we propose a new taxonomy of semi-supervised classification algorithms, which sheds light on the different conceptual and methodological approaches for incorporating unlabelled data into the training process. Lastly, we show how the fundamental assumptions underlying most semi-supervised learning algorithms are closely connected to each other, and how they relate to the well-known semi-supervised clustering assumption.},
   author = {Jesper E van Engelen and Holger H Hoos and Tom B Fawcett Jesper E van Engelen and Holger H Hoos hh},
   doi = {10.1007/s10994-019-05855-6},
   journal = {Machine Learning},
   keywords = {Classification,Machine learning,Semi-supervised learning},
   pages = {373-440},
   title = {A survey on semi-supervised learning},
   volume = {109},
   url = {https://doi.org/10.1007/s10994-019-05855-6},
   year = {2020},
}
@report{,
   abstract = {Deep neural networks (DNNs) have witnessed great successes in semantic segmentation, which requires a large number of labeled data for training. We present a novel learning framework called Uncertainty guided Cross-head Co-training (UCC) for semi-supervised semantic segmen-tation. Our framework introduces weak and strong augmentations within a shared encoder to achieve co-training, which naturally combines the benefits of consistency and self-training. Every segmentation head interacts with its peers and, the weak augmentation result is used for supervising the strong. The consistency training samples' diversity can be boosted by Dynamic Cross-Set Copy-Paste (DCSCP), which also alleviates the distribution mismatch and class imbalance problems. Moreover, our proposed Uncertainty Guided Re-weight Module (UGRM) enhances the self-training pseudo labels by suppressing the effect of the low-quality pseudo labels from its peer via model-ing uncertainty. Extensive experiments on Cityscapes and PASCAL VOC 2012 demonstrate the effectiveness of our UCC. Our approach significantly outperforms other state-of-the-art semi-supervised semantic segmentation methods. It achieves 77.17%, 76.49% mIoU on Cityscapes and PAS-CAL VOC 2012 datasets respectively under 1/16 protocols , which are +10.1%, +7.91% better than the supervised baseline.},
   author = {Jiashuo Fan and Bin Gao and Huan Jin and Lihui Jiang},
   title = {UCC: Uncertainty guided Cross-head Co-training for Semi-Supervised Semantic Segmentation},
}
@report{,
   abstract = {As a powerful way of realizing semi-supervised segmentation, the cross supervision method learns cross consistency based on independent ensemble models using abundant unlabeled images. However, the wrong pseudo labeling information generated by cross supervision would confuse the training process and negatively affect the effectiveness of the segmentation model. Besides, the training process of ensemble models in such methods also multiplies the cost of computation resources and decreases the training efficiency. To solve these problems, we propose a novel cross supervision method, namely uncertainty-guided self cross supervision (USCS). In addition to ensemble models, we first design a multi-input multi-output (MIMO) segmentation model which can generate multiple outputs with shared model and consequently impose consistency over the outputs, saving the cost on parameters and calculations. On the other hand, we employ uncertainty as guided information to encourage the model to focus on the high confident regions of pseudo labels and mitigate the effects of wrong pseudo labeling in self cross supervision, improving the performance of the segmentation model. Extensive experiments show that our method achieves state-of-the-art performance while saving 40.5% and 49.1% cost on parameters and calculations. The code is available on GitHub. 1},
   author = {Yunyang Zhang and Zhiqiang Gong and Xiaohu Zheng and Xiaoyu Zhao and Wen Yao},
   keywords = {Consistency Regularization,Multi-Input Multi-Output,Semi-Supervised Semantic Segmentation,Uncertainty},
   title = {SEMI-SUPERVISED SEMANTIC SEGMENTATION WITH UNCERTAINTY-GUIDED SELF CROSS SUPERVISION A PREPRINT},
}
@web_page{,
   author = {Anon 2022b},
   title = {COCO detection evaluation},
   url = {https://cocodataset.org/#detection-eval},
}
@article{Goldberg2009,
   abstract = {Semi-supervised learning is a learning paradigm concerned with the study of how computers and natural systems such as humans learn in the presence of both labeled and unlabeled data. Traditionally, learning has been studied either in the unsupervised paradigm (e.g., clustering, outlier detection) where all the data are unlabeled, or in the supervised paradigm (e.g., classification, regression) where all the data are labeled. The goal of semi-supervised learning is to understand how combining labeled and unlabeled data may change the learning behavior, and design algorithms that take advantage of such a combination. Semi-supervised learning is of great interest in machine learning and data mining because it can use readily available unlabeled data to improve supervised learning tasks when the labeled data are scarce or expensive. Semi-supervised learning also shows potential as a quantitative tool to understand human category learning, where most of the input is self-evidently unlabeled. In this introductory book, we present some popular semi-supervised learning models, including self-training, mixture models, co-training and multiview learning, graph-based methods, and semi-supervised support vector machines. For each model, we discuss its basic mathematical formulation. The success of semi-supervised learning depends critically on some underlying assumptions. We emphasize the assumptions made by each model and give counterexamples when appropriate to demonstrate the limitations of the different models. In addition, we discuss semi-supervised learning for cognitive psychology. Finally, we give a computational learning theoretic perspective on semi-supervised learning, and we conclude the book with a brief discussion of open questions in the field. Copyright; 2009 by Morgan & Claypool.},
   author = {Xiaojin Goldberg},
   doi = {10.2200/S00196ED1V01Y200906AIM006},
   issn = {19394608},
   journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
   title = {Introduction to semi-supervised learning},
   volume = {6},
   year = {2009},
}
@inproceedings{Blum1998,
   abstract = {The problem of using a large unlabeled sample is considered to boost the performance of a learning algorithm when only a small set of labeled examples is available. In particular, a problem setting is considered to classify web pages, in which the description of each example can be partitioned into two distinct views. A PAC-style analysis for this setting, and, more broadly, a PAC-style framework for the general problem of learning from both labeled and unlabeled data are presented. Also, empirical results on real web-page is giving, indicating that this use of unlabeled examples can lead to significant improvement of hypotheses in practice.},
   author = {Avrim Blum and Tom Mitchell},
   doi = {10.1145/279943.279962},
   journal = {Proceedings of the Annual ACM Conference on Computational Learning Theory},
   title = {Combining labeled and unlabeled data with co-training},
   year = {1998},
}
@web_page{,
   author = {Anon 2022a},
   title = {COCO dataset format},
   url = {https://cocodataset.org/#format-data},
}
@article{Liao2001,
   abstract = {Otsu reference proposed a criterion for maximizing the between-class variance of pixel intensity to perform picture thresholding. However, Otsu's method for image segmentation is very time-consuming because of the inefficient formulation of the between-class variance. In this paper, a faster version of Otsu's method is proposed for improving the efficiency of computation for the optimal thresholds of an image. First, a criterion for maximizing a modified between-class variance that is equivalent to the criterion of maximizing the usual between-class variance is proposed for image segmentation. Next, in accordance with the new criterion, a recursive algorithm is designed to efficiently find the optimal threshold. This procedure yields the same set of thresholds as the original method. In addition, the modified between-class variance can be pre-computed and stored in a look-up table. Our analysis of the new criterion clearly shows that it takes less computation to compute both the cumulative probability (zeroth order moment) and the mean (first order moment) of a class, and that determining the modified between-class variance by accessing a look-up table is quicker than that by performing mathematical arithmetic operations. For example, the experimental results of a five-level threshold selection show that our proposed method can reduce down the processing time from more than one hour by the conventional Otsu's method to less than 107 seconds.},
   author = {P. S. Liao and T. S. Chen and P. C. Chung},
   issn = {10162364},
   issue = {5},
   journal = {Journal of Information Science and Engineering},
   title = {A fast algorithm for multilevel thresholding},
   volume = {17},
   year = {2001},
}
@article{Khan2019,
   abstract = {The scale of ongoing and future electromagnetic surveys pose formidable challenges to classify astronomical objects. Pioneering efforts on this front include citizen science campaigns adopted by the Sloan Digital Sky Survey (SDSS). SDSS datasets have been recently used to train neural network models to classify galaxies in the Dark Energy Survey (DES) that overlap the footprint of both surveys. Herein, we demonstrate that knowledge from deep learning algorithms, pre-trained with real-object images, can be transferred to classify galaxies that overlap both SDSS and DES surveys, achieving state-of-the-art accuracy ≳99.6%. We demonstrate that this process can be completed within just eight minutes using distributed training. While this represents a significant step towards the classification of DES galaxies that overlap previous surveys, we need to initiate the characterization of unlabelled DES galaxies in new regions of parameter space. To accelerate this program, we use our neural network classifier to label over ten thousand unlabelled DES galaxies, which do not overlap previous surveys. Furthermore, we use our neural network model as a feature extractor for unsupervised clustering and find that unlabelled DES images can be grouped together in two distinct galaxy classes based on their morphology, which provides a heuristic check that the learning is successfully transferred to the classification of unlabelled DES images. We conclude by showing that these newly labelled datasets can be combined with unsupervised recursive training to create large-scale DES galaxy catalogs in preparation for the Large Synoptic Survey Telescope era.},
   author = {Asad Khan and E. A. Huerta and Sibo Wang and Robert Gruendl and Elise Jennings and Huihuo Zheng},
   doi = {10.1016/J.PHYSLETB.2019.06.009},
   issn = {0370-2693},
   journal = {Physics Letters B},
   keywords = {Convolutional neural networks,Dark Energy Survey,Deep learning,Large Synoptic Survey Telescope,Sloan Digital Sky Survey,Unsupervised learning},
   month = {8},
   pages = {248-258},
   publisher = {North-Holland},
   title = {Deep learning at scale for the construction of galaxy catalogs in the Dark Energy Survey},
   volume = {795},
   year = {2019},
}
@article{Simmons2017,
   author = {B. D. Simmons and Chris Lintott and Kyle W. Willett and Karen L. Masters and Jeyhan S. Kartaltepe and Boris Häußler and Sugata Kaviraj and Coleman Krawczyk and S. J. Kruk and Daniel H. McIntosh and R. J. Smethurst and Robert C. Nichol and Claudia Scarlata and Kevin Schawinski and Christopher J. Conselice and Omar Almaini and Henry C. Ferguson and Lucy Fortson and William Hartley and Dale Kocevski and Anton M. Koekemoer and Alice Mortlock and Jeffrey A. Newman and Steven P. Bamford and N. A. Grogin and Ray A. Lucas and Nimish P. Hathi and Elizabeth McGrath and Michael Peth and Janine Pforr and Zachary Rizer and Stijn Wuyts and Guillermo Barro and Eric F. Bell and Marco Castellano and Tomas Dahlen and Avishai Dekel and Jamie Ownsworth and Sandra M. Faber and Steven L. Finkelstein and Adriano Fontana and Audrey Galametz and Ruth Grützbauch and David Koo and Jennifer Lotz and Bahram Mobasher and Mark Mozena and Mara Salvato and Tommy Wiklind},
   doi = {10.1093/mnras/stw2587},
   issn = {0035-8711},
   issue = {4},
   journal = {Monthly Notices of the Royal Astronomical Society},
   month = {2},
   pages = {4420-4447},
   title = {Galaxy Zoo: quantitative visual morphological classifications for 48 000 galaxies from CANDELS},
   volume = {464},
   year = {2017},
}
@article{Willett2017,
   author = {Kyle W. Willett and Melanie A. Galloway and Steven P. Bamford and Chris J. Lintott and Karen L. Masters and Claudia Scarlata and B. D. Simmons and Melanie Beck and Carolin N. Cardamone and Edmond Cheung and Edward M. Edmondson and Lucy F. Fortson and Roger L. Griffith and Boris Häußler and Anna Han and Ross Hart and Thomas Melvin and Michael Parrish and Kevin Schawinski and R. J. Smethurst and Arfon M. Smith},
   doi = {10.1093/mnras/stw2568},
   issn = {0035-8711},
   issue = {4},
   journal = {Monthly Notices of the Royal Astronomical Society},
   month = {2},
   pages = {4176-4203},
   title = {Galaxy Zoo: morphological classifications for 120 000 galaxies in HST legacy imaging},
   volume = {464},
   year = {2017},
}
@article{Willett2013,
   author = {Kyle W. Willett and Chris J. Lintott and Steven P. Bamford and Karen L. Masters and Brooke D. Simmons and Kevin R. V. Casteels and Edward M. Edmondson and Lucy F. Fortson and Sugata Kaviraj and William C. Keel and Thomas Melvin and Robert C. Nichol and M. Jordan Raddick and Kevin Schawinski and Robert J. Simpson and Ramin A. Skibba and Arfon M. Smith and Daniel Thomas},
   doi = {10.1093/mnras/stt1458},
   issn = {1365-2966},
   issue = {4},
   journal = {Monthly Notices of the Royal Astronomical Society},
   month = {11},
   pages = {2835-2860},
   title = {Galaxy Zoo 2: detailed morphological classifications for 304 122 galaxies from the Sloan Digital Sky Survey},
   volume = {435},
   year = {2013},
}
@article{Lintott2011,
   author = {Chris Lintott and Kevin Schawinski and Steven Bamford and Anže Slosar and Kate Land and Daniel Thomas and Edd Edmondson and Karen Masters and Robert C. Nichol and M. Jordan Raddick and Alex Szalay and Dan Andreescu and Phil Murray and Jan Vandenberg},
   doi = {10.1111/j.1365-2966.2010.17432.x},
   issn = {00358711},
   issue = {1},
   journal = {Monthly Notices of the Royal Astronomical Society},
   month = {1},
   pages = {166-178},
   title = {Galaxy Zoo 1: data release of morphological classifications for nearly 900 000 galaxies★},
   volume = {410},
   year = {2011},
}
@article{Lupton2004,
   abstract = {We present a new, and we believe arguably correct, algorithm for producing red-green-blue (RGB) composites from three-band astronomical images. Our method ensures that an object with a specified astronomical color (e.g., g-r and r-i) has a unique color in the RGB image, as opposed to the burnt-out white stars to which we are accustomed. A natural consequence of this is that we can use the same colors to code color-magnitude diagrams, providing a natural "index" to our images. We also introduce the use of an arcsinh stretch that allows us to show faint objects while simultaneously preserving the structure of brighter objects in the field, such as the spiral arms of large galaxies. We believe that in addition to their aesthetic value, our images convey far more information than do the traditional ones, and we provide examples from Sloan Digital Sky Survey (SDSS) imaging, the Hubble Deep Field (HDF), and Chandra to support our claims.},
   author = {R. Lupton and M.R. Blanton and G. Fekete and D.W. Hogg and W. O'Mullane and A. Szalay and N. Wherry},
   doi = {10.1086/382245},
   issue = {816},
   journal = {Publications of the Astronomical Society of the Pacific},
   pages = {133-137},
   title = {Preparing Red-Green-Blue Images from CCD Data},
   volume = {116},
   year = {2004},
}
@article{Pence2010,
   author = {W. D. Pence and L. Chiappetti and C. G. Page and R. A. Shaw and E. Stobie},
   doi = {10.1051/0004-6361/201015362},
   issn = {0004-6361},
   journal = {Astronomy & Astrophysics},
   month = {12},
   pages = {A42},
   title = {Definition of the Flexible Image Transport System (FITS), version 3.0},
   volume = {524},
   year = {2010},
}
@inproceedings{Souly2017,
   abstract = {Semantic segmentation has been a long standing challenging task in computer vision. It aims at assigning a label to each image pixel and needs a significant number of pixel-level annotated data, which is often unavailable. To address this lack of annotations, in this paper, we leverage, on one hand, a massive amount of available unlabeled or weakly labeled data, and on the other hand, non-real images created through Generative Adversarial Networks. In particular, we propose a semi-supervised framework - based on Generative Adversarial Networks (GANs) - which consists of a generator network to provide extra training examples to a multi-class classifier, acting as discriminator in the GAN framework, that assigns sample a label y from the K possible classes or marks it as a fake sample (extra class). The underlying idea is that adding large fake visual data forces real samples to be close in the feature space, which, in turn, improves multiclass pixel classification. To ensure a higher quality of generated images by GANs with consequently improved pixel classification, we extend the above framework by adding weakly annotated data, i.e., we provide class level information to the generator. We test our approaches on several challenging benchmarking visual datasets, i.e. PASCAL, SiftFLow, Stanford and CamVid, achieving competitive performance compared to state-of-the-art semantic segmentation methods.},
   author = {N. Souly and C. Spampinato and M. Shah},
   doi = {10.1109/ICCV.2017.606},
   isbn = {9781538610329},
   journal = {Proceedings of the IEEE International Conference on Computer Vision},
   pages = {5689-5697},
   title = {Semi Supervised Semantic Segmentation Using Generative Adversarial Network},
   volume = {2017-Octob},
   year = {2017},
}
@article{Zhang2020,
   abstract = {Image semantic segmentation is one of the most important tasks in the field of computer vision, and it has made great progress in many applications. Many fully supervised deep learning models are designed to implement complex semantic segmentation tasks and the experimental results are remarkable. However, the acquisition of pixel-level labels in fully supervised learning is time consuming and laborious, semi-supervised and weakly supervised learning is gradually replacing fully supervised learning, thus achieving good results at a lower cost. Based on the commonly used models such as convolutional neural networks, fully convolutional networks, generative adversarial networks, this paper focuses on the core methods and reviews the semi- and weakly supervised semantic segmentation models in recent years. In the following chapters, existing evaluations and data sets are summarized in details and the experimental results are analyzed according to the data set. The last part of the paper is an objective summary. In addition, it points out the possible direction of research and inspiring suggestions for future work.},
   author = {M. Zhang and Y. Zhou and J. Zhao and Y. Man and B. Liu and R. Yao},
   doi = {10.1007/s10462-019-09792-7},
   issue = {6},
   journal = {Artificial Intelligence Review},
   pages = {4259-4288},
   title = {A survey of semi- and weakly supervised semantic segmentation of images},
   volume = {53},
   year = {2020},
}
@inproceedings{Yi2021,
   abstract = {To improve the accuracy of image detail structure segmentation is a challenging research topic. In this paper, a progressively refined image semantics segmentation method based on recurrent neural network is presented. By introducing a recurrent processing layer into the neural network, a recurrent network structure is constructed, which makes the image segmentation from coarse to fine. Using the coarse-grained segmentation result of the previous segmentation module, it provides an effective context clue for the next fine-grained segmentation, and achieves progressive, multi-grained semantic image segmentation from coarse to fine. Comparative experiments on several datasets show that: image semantic segmentation based on recurrent neural network can effectively utilize the image context information and improve the accuracy of fine-grained image segmentation.},
   author = {L. Yi},
   doi = {10.1109/ICSP51882.2021.9408920},
   isbn = {9780738143705},
   journal = {2021 IEEE 6th International Conference on Intelligent Computing and Signal Processing, ICSP 2021},
   pages = {765-768},
   title = {A progressive image semantic segmentation method using recurrent neural network},
   year = {2021},
}
@article{Chen2020,
   author = {C Chen and S Tang and J Li},
   doi = {10.11834/jig.190458},
   issue = {6},
   journal = {Journal of Image and Graphics},
   note = {cited By 1},
   pages = {1190-1200},
   title = {Weakly supervised semantic segmentation based on dynamic mask generation [动态生成掩膜弱监督语义分割]},
   volume = {25},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091014927&doi=10.11834%2fjig.190458&partnerID=40&md5=ac131d611893e5de82159b99dfb87e5d},
   year = {2020},
}
@article{Peng2020,
   author = {J Peng and G Estrada and M Pedersoli and C Desrosiers},
   doi = {10.1016/j.patcog.2020.107269},
   journal = {Pattern Recognition},
   note = {cited By 43},
   title = {Deep co-training for semi-supervised image segmentation},
   volume = {107},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086570857&doi=10.1016%2fj.patcog.2020.107269&partnerID=40&md5=55483a4325cc535e8bf2237aaa9a5f8d},
   year = {2020},
}
@inproceedings{Costillo2006,
   abstract = {The Sierra Nevada Observatory (Granada, Spain) has a number of telescopes. Our study will focus on two Nasmyth telescopes with apertures of 1.5m and 0.9m and an equatorial mount. The system currently installed to control these telescopes is a 1995 centralized VME module. However, given the problems which have arisen due to the number of wires and other complications, we have decided to change this control module. We will control each telescope with a distributed control philosophy, using a serial linear communication bus between independent nodes, although all system capabilities are accessible from a central unit anywhere and at any time via internet. We have divided the tasks and have one node for alpha control, another for delta control, one for the dome, one for the focus and the central unit to interface with a pc. The nodes for alpha, delta and the dome will be used by means of FPGA's in order to efficiently sample the encoders and the control algorithms, and to generate the output for the motors and the servo. The focus will have a microcontroller, and the system is easy to expand in the event of the inclusion of more nodes. After having studied several fieldbus systems, we have opted for the CAN bus, because of its reliability and broadcasting possibilities. In this way, all the important information will be on the bus, and every node will be able to access the information at any time. This document explains the new design made in the IAA for the new consoles of control whose basic characteristics are, the distributed control, the hardware simplify, the cable remove, the safety and maintenance improve and facilitating the observation improving the interface with the user, and finally to prepare the system for the remote observation.},
   author = {L.P. Costillo and J.L. Ramos and J.M. Ibáñez and B. Aparicio and M. Herránz and A.J. García},
   doi = {10.1117/12.671308},
   isbn = {0819463396},
   issn = {0277786X},
   journal = {Proceedings of SPIE - The International Society for Optical Engineering},
   keywords = {CAN Bus,Distributed control,Embedded,Hardware design,IAA,OSN,Servo-control,Software design,Telescope Control System},
   title = {New control system for the 1.5m and 0.9m telescopes at Sierra Nevada Observatory},
   volume = {6274},
   year = {2006},
}
@inproceedings{Baumeister2008,
   abstract = {PANIC is a wide-field NIR camera, which is currently under development for the CalarAlto observatory (CAHA) in Spain. It uses a mosaic of four Hawaii-2RG detectors and covers the spectral range from 0.8-2.5 μm (z to K-band). The field-of-view is 30x30 arcmin. This instrument can be used at the 2.2m telescope (0.45arcsec/pixel, 0.5x0.5 degree FOV) and at the 3.5m telescope (0.23arcsec/pixel, 0.25x0.25 degree FOV).The operating temperature is about 77K, achieved by liquid Nitrogen cooling. The cryogenic optics has three flatfolding mirrors with diameters up to 282 mm and nine lenses with diameters between 130 mmand 255 mm. A compact filter unit can carry up to 19 filters distributed over four filter wheels. Narrow band (1%) filters can be used.The instrument has a diameter of 1.1 m and its about 1 m long. The weight limit of 400 kg at the 2.2m telescope requires a light-weight cryostat design. The aluminium vacuum vessel and radiation shield have wall thicknesses of only 6 mm and 3 mm respectively.},
   author = {H. Baumeister and M. Alter and M.C.C. Vázquez and F. Fernandez and J. Fried and J. Helmling and A. Huber and J.-M.I. Mengual and J.F.R. Gómez and W. Laun and R. Lenzen and U. Mall and V. Naranjo and J.-R. Ramos and R.-R. Rohloff and A.G. Segura and C. Storz and M. Ubierna and K. Wagner},
   doi = {10.1117/12.788796},
   isbn = {9780819472243},
   issn = {0277786X},
   journal = {Proceedings of SPIE - The International Society for Optical Engineering},
   title = {PANIC: the new panoramic NIR camera for Calar Alto},
   volume = {7014},
   year = {2008},
}
@inproceedings{Costillo2006,
   abstract = {The Observatorio de Sierra Nevada (OSN) is located at an altitude of 2800m at the Loma de Dilar in the Sierra Nevada mountain range, in the province of Granada, Spain. It is operated and maintained by the Institute de Astrofisica de Andalucia (IAA-CSIC) and contains two Nasmyth telescopes with apertures of 1.5 and 0.9m and an Altazimuth telescope with an aperture of 0.6 m. Given that the quality of the images and, indeed, the performance of the instruments are influenced by weather conditions, it would appear that the existence of a weather station capable of producing accurate descriptions is an essential component of any observatory. This is particularly true, however, in our case where, given the altitude, weather conditions at certain times of the year are especially harsh1. For this reason, our observatory has required the installation of a robust weather station with easily replaceable sensors which can provide accurate and reliable measures of wind, temperature and humidity2. At the same time, in order to avoid a complex topology of unmanageably long wires due to the distribution of a large number of sensors around the buildings, domes telescope mirrors and instruments, it has been necessary to implement a distributed system with several independent nodes connected to a CAN bus. This system is now in operation and running automatically at the OSN and provides all the data from sensors to the observatory control systems and to internet users. This paper gives a detailed description of the SNOWS project, including the development of the weather station, the software and hardware architecture, and the use of distributed nodes with a linear serial bus. The paper also provides some results regarding the wind, temperature and humidity sensors employed at the OSN.},
   author = {L.P. Costillo and J.M. Ibáñez and B. Aparicio and A.J. García},
   doi = {10.1117/12.671256},
   isbn = {0819463396},
   issn = {0277786X},
   journal = {Proceedings of SPIE - The International Society for Optical Engineering},
   keywords = {CAN bus,Data acquisition,IAA,Instrumentation,LabView,OSN,Observatory control system,Software design,Weather},
   title = {SNOWS: Sierra Nevada observatory weather system},
   volume = {6274},
   year = {2006},
}
@inproceedings{Mengual2010,
   abstract = {PANIC is the Panoramic Near Infrared Camera for the 2.2m and 3.5m telescopes at Calar Alto observatory. The aim of the project is to build a wide-field general purpose NIR camera. In this paper we describe the software system of the instrument, which comprises four main packages: GEIRS for the instrument control and the data acquisition; the Observation Tool (OT), the software used for detailed definition and pre-planning the observations, developed in Java; the Quick Look tool (PQL) for easy inspection of the data in real-time and a scientific pipeline (PAPI), both based on the Python programming language. © 2010 SPIE.},
   author = {J.M.I. Mengual and M. Fernández and J.F.R. Gómez and A.J.G. Segura and C. Storz},
   doi = {10.1117/12.856029},
   isbn = {9780819482303},
   issn = {0277786X},
   issue = {PART 1},
   journal = {Proceedings of SPIE - The International Society for Optical Engineering},
   keywords = {control software,data reduction,near-infrared,pipeline},
   title = {The PANIC software system},
   volume = {7740},
   year = {2010},
}
@inproceedings{Naranjo2010,
   abstract = {PANIC, the PAnoramic Near-Infrared Camera for Calar Alto, is one of the next generation instruments for this observatory. In order to cover a field of view of approximately 30 arcmin, PANIC uses a mosaic of four 2k x 2k HAWAII-2RG arrays from Teledyne. This document presents the preliminary results of the basic characterization of the mosaic. The performance of the system as a whole, as well as the in-house readout electronics and software capabilities will also be briefly discussed. © 2010 SPIE.},
   author = {V. Naranjo and U. Mall and J.R. Ramos and C. Storz and K. Wagner and M. Alter and H. Baumeister and P. Bizenberger and M.C. Cárdenas and M. Fernández and J.W. Fried and A.J. García Segura and J. Helmling and A. Huber and J.M. Ibáñez Mengual and W. Laun and R. Lenzen and J.F. Rodríguez Gómez and R.-R. Rohloff},
   doi = {10.1117/12.855996},
   isbn = {9780819482327},
   issn = {0277786X},
   journal = {Proceedings of SPIE - The International Society for Optical Engineering},
   keywords = {HAWAII-2RG,characterization,mosaic,readout electronics,software},
   title = {Characterization and performance of the 4k x 4k Hawaii-2RG Mosaic for PANIC},
   volume = {7742},
   year = {2010},
}
@inproceedings{,
   abstract = {PANIC, the Panoramic Near Infrared Camera, is an instrument for the Calar Alto Observatory currently being integrated in laboratory and whose first light is foreseen for end 2012 or early 2013. We present here how the PANIC Quick-Look tool (PQL) and pipeline (PAPI) are being implemented, using existing rapid programming Python technologies and packages, together with well-known astronomical software suites (Astromatic, IRAF) and parallel processing techniques. We will briefly describe the structure of the PQL tool, whose main characteristics are the use of the SQLite database and PyQt, a Python binding of the GUI toolkit Qt. © 2012 SPIE.},
   author = {J.-M. Ibáñez and A.J. García Segura and C. Storz and J.W. Fried and M. Fernández and J.F. Rodríguez Gómez and V. Terrón and M.C. Cárdenas},
   doi = {10.1117/12.924165},
   isbn = {9780819491527},
   issn = {0277786X},
   journal = {Proceedings of SPIE - The International Society for Optical Engineering},
   keywords = {Data reduction,Image processing,Pipeline,Python,Quick-look},
   title = {Advanced PANIC quick-look tool using Python},
   volume = {8451},
   year = {2012},
}
@inproceedings{Dorner2014,
   abstract = {PANIC is the new PAnoramic Near-Infrared camera for Calar Alto, a joint project by the MPIA in Heidelberg, Germany, and the IAA in Granada, Spain. It can be operated at the 2.2m or 3.5m CAHA telescopes to observe a field of view of 30'x30' or 15'x15' respectively, with a sampling of 4096x4096 pixels. It is designed for the spectral bands from Z to K, and can be equipped with additional narrow-band filters. The instrument is close to completion and will be delivered to the observatory in Spain in fall 2014. It is currently in the last stage of assembly, where the optical elements are being aligned, which will be followed by final laboratory tests of the instrument. This paper contains an update of the recent progress and shows results from the optical alignment and detector performance tests.},
   author = {B. Dorner and A. Huber and M.C. Cárdenas Vázquez and I. Ferro Rodriguez and P. Bizenberger and V. Naranjo and J. Panduro and U. Mall and M. Alter and R. Mathar and C. Storz and R.-R. Rohloff and P. Fopp and W. Laun and J.M. Ibaáñez and A.J. Garciáa Segura and V. Terroán and J.W. Fried and M. Fernández and J.F. Rodríguez Gómez and K. Meisenheimer},
   doi = {10.1117/12.2054922},
   isbn = {9780819496157},
   issn = {1996756X},
   journal = {Proceedings of SPIE - The International Society for Optical Engineering},
   keywords = {Alignment,CAHA,HAWAII-2RG,Infrared,Optical,PANIC,Performance},
   title = {PANIC in the lab: Status before commissioning},
   volume = {9147},
   year = {2014},
}
@article{,
   abstract = {Solar-mass stars form via disk-mediated accretion. Recent findings indicate that this process is probably episodic in the form of accretion bursts, possibly caused by disk fragmentation. Although it cannot be ruled out that high-mass young stellar objects arise from the coalescence of their low-mass brethren, the latest results suggest that they more likely form via disks. It follows that disk-mediated accretion bursts should occur. Here we report on the discovery of the first disk-mediated accretion burst from a roughly twenty-solar-mass high-mass young stellar object. Our near-infrared images show the brightening of the central source and its outflow cavities. Near-infrared spectroscopy reveals emission lines typical for accretion bursts in low-mass protostars, but orders of magnitude more luminous. Moreover, the released energy and the inferred mass-accretion rate are also orders of magnitude larger. Our results identify disk-accretion as the common mechanism of star formation across the entire stellar mass spectrum.},
   author = {A. Caratti O Garatti and B. Stecklum and R. Garcia Lopez and J. Eislöffel and T.P. Ray and A. Sanna and R. Cesaroni and C.M. Walmsley and R.D. Oudmaijer and W.J. De Wit and L. Moscadelli and J. Greiner and A. Krabbe and C. Fischer and R. Klein and J.M. Ibañez},
   doi = {10.1038/nphys3942},
   issn = {17452481},
   issue = {3},
   journal = {Nature Physics},
   title = {Disk-mediated accretion burst in a high-mass young stellar object},
   volume = {13},
   year = {2017},
}
@generic{,
   abstract = {Solar-mass stars form via circumstellar disk accretion (disk-mediated accretion). Recent findings indicate that this process is likely episodic in the form of accretion bursts (1), possibly caused by disk fragmentation (2; 3; 4). Although it cannot be ruled out that high-mass young stellar objects (HMYSOs; M >8 M☉, Lbol >5×103 L☉) arise from the coalescence of their low-mass brethren (5), latest results suggest that they more likely form via disks (6; 7; 8; 9). Accordingly, disk-mediated accretion bursts should occur (10; 11). Here we report on the discovery of the first disk-mediated accretion burst from a ∼20 M☉ HMYSO (12). Our near-infrared images show the brightening of the central source and its outflow cavities. Near-infrared spectroscopy reveals emission lines typical of accretion bursts in low-mass protostars, but orders of magnitude more luminous. Moreover, the energy released and the inferred mass-accretion rate are also orders of magnitude larger. Our results identify disk accretion as the common mechanism of star formation across the entire stellar mass spectrum.},
   author = {A. Caratti o Garatti and B. Stecklum and R. Garcia Lopez and J. Eislöffel and T.P. Ray and A. Sanna and R. Cesaroni and C.M. Walmsley and R.D. Oudmaijer and W.J. de Wit and L. Moscadelli and J. Greiner and A. Krabbe and C. Fischer and R. Klein and J.M. Ibañez},
   issn = {23318422},
   journal = {arXiv},
   title = {Disk-mediated accretion burst in a high-mass young stellar object},
   year = {2017},
}
@generic{Stecklum2017,
   abstract = {There is growing evidence for disk-mediated accretion being the dominant mode of star formation across nearly the whole stellar mass spectrum. The stochastic nature of this process has been realized which implies an inherent source variability. It can be traced more easily for low-mass YSOs (LMYSOs) since high-mass YSOs (HMYSOs) are still embedded even when reaching the ZAMS. While variable reection nebulae around LMYSOs were among the earliest signs of star formation, little is known on the variability of scattered light from embedded clusters, the birthplaces of HMYSOs. Since the few most massive stars dominate this emission, their variability is literally reected in scattered light. Moreover, because of their high luminosity, for a given ambient dust density and source distance, the associated nebulosities are much larger than those of LMYSOs. In this case, the light travel time becomes substantial. So the apparent brightness distribution constitutes a light echo, shaped by both the HMYSO variability history and the spatial distribution of the scattering medium. We report on early results of a NIR variability study of HMYSOs associated with Class II methanol masers which aims at revealing a possible correlation between maser ux density and infrared brightness. Additionally, relevant findings for the eruptive HMYSO S255IR-NIRS3 are presented.},
   author = {B. Stecklum and S. Heese and S. Wolf and A.C. O Garatti and J.M. Ibañez and H. Linz},
   issn = {23318422},
   journal = {arXiv},
   title = {Tracing accretion variability of high-mass YSOs via light echoes},
   year = {2017},
}
@article{,
   abstract = {PANIC7 is the new PAnoramic Near-Infrared Camera for Calar Alto and is a project jointly developed by the MPIA in Heidelberg, Germany, and the IAA in Granada, Spain, for the German-Spanish Astronomical Center at Calar Alto Observatory (CAHA; Almería, Spain). This new instrument works with the 2.2 m and 3.5 m CAHA telescopes covering a field of view of 30×30 arcmin and 15×15 arcmin, respectively, with a sampling of 4096×4096 pixels. It is designed for the spectral bands from Z to KS, and can also be equipped with narrowband filters. The instrument was delivered to the observatory in 2014 October and was commissioned at both telescopes between 2014 November and 2015 June. Science verification at the 2.2 m telescope was carried out during the second semester of 2015 and the instrument is now at full operation. We describe the design, assembly, integration, and verification process, the final laboratory tests and the PANIC instrument performance. We also present first-light data obtained during the commissioning and preliminary results of the scientific verification. The final optical model and the theoretical performance of the camera were updated according to the as-built data. The laboratory tests were made with a star simulator. Finally, the commissioning phase was done at both telescopes to validate the camera real performance on sky. The final laboratory test confirmed the expected camera performances, complying with the scientific requirements. The commissioning phase on sky has been accomplished.},
   author = {M.-C.C. Vázquez and B. Dorner and A. Huber and E. Sánchez-Blanco and M. Alter and J.F. Rodríguez Gómez and P. Bizenberger and V. Naranjo and J.-M.I. Mengual and J. Panduro and A.J. García Segura and U. Mall and M. Fernández and W. Laun and I.M. Ferro Rodríguez and J. Helmling and V. Terrón and K. Meisenheimer and J.W. Fried and R.J. Mathar and H. Baumeister and R.-R. Rohloff and C. Storz and L. Verdes-Montenegro and H. Bouy and M. Ubierna and P. Fopp and B. Funke},
   doi = {10.1088/1538-3873/aa9884},
   issn = {00046280},
   issue = {984},
   journal = {Publications of the Astronomical Society of the Pacific},
   keywords = {Instrumentation: detectors,Instrumentation: miscellaneous,Techniques: image processing},
   title = {PANIC: A general-purpose panoramic near-infrared camera for the calar alto observatory},
   volume = {130},
   year = {2018},
}
@inproceedings{Lombardo2020,
   abstract = {The Calar Alto Schmidt-Lemaitre Explorer (CASTLE) is an innovative 35 cm robotic telescope aimed at demonstrating the impact and performance of curved detectors for astronomical observations. This telescope will use a spherically curved science-grade sensor matching its curved focal surface and it will be installed at the Calar Alto Observatory in Spain. In this paper we will show the design and we will present the status of the opto-mechanical design and construction. We will also show the preliminary results of the straylight analysis and the general plan towards commissioning and first light in 2021/2022.},
   author = {S. Lombardo and E.R. Muslimov and K. Joaquina and G.R. Lemaitre and M. Pons and E. Pérez and J.-M. Ibáñez Mengual and J. Sánchez and F. Prada and E. Hugot},
   doi = {10.1117/12.2561369},
   isbn = {9781510636897},
   issn = {1996756X},
   journal = {Proceedings of SPIE - The International Society for Optical Engineering},
   keywords = {Telescope,gravitational-wave counterpart,imaging,innovative detectors,robotic,wide-field},
   title = {CASTLE: A curved-sensor-based wide-field telescope at Calar Alto},
   volume = {11451},
   year = {2020},
}
@article{Schmidhuber2015,
   author = {J Schmidhuber},
   doi = {10.1016/j.neunet.2014.09.003},
   journal = {Neural Networks},
   note = {cited By 9438},
   pages = {85-117},
   title = {Deep Learning in neural networks: An overview},
   volume = {61},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910651844&doi=10.1016%2fj.neunet.2014.09.003&partnerID=40&md5=1e380e40a7a616540c705ef8a63ce456},
   year = {2015},
}
@article{Lecun2015,
   author = {Y Lecun and Y Bengio and G Hinton},
   doi = {10.1038/nature14539},
   issue = {7553},
   journal = {Nature},
   note = {cited By 36738},
   pages = {436-444},
   title = {Deep learning},
   volume = {521},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930630277&doi=10.1038%2fnature14539&partnerID=40&md5=e324cb9ec992f892ebc74f3e06078083},
   year = {2015},
}
@inproceedings{Long2015,
   author = {J Long and E Shelhamer and T Darrell},
   doi = {10.1109/CVPR.2015.7298965},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   note = {cited By 15194},
   pages = {431-440},
   title = {Fully convolutional networks for semantic segmentation},
   volume = {07-12-June-2015},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959205572&doi=10.1109%2fCVPR.2015.7298965&partnerID=40&md5=2c171b6055fca5fbb1871fa72ee0ee41},
   year = {2015},
}
@article{Ball2017,
   author = {J E Ball and D T Anderson and C S Chan},
   doi = {10.1117/1.JRS.11.042609},
   issue = {4},
   journal = {Journal of Applied Remote Sensing},
   note = {cited By 349},
   title = {Comprehensive survey of deep learning in remote sensing: Theories, tools, and challenges for the community},
   volume = {11},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032865390&doi=10.1117%2f1.JRS.11.042609&partnerID=40&md5=51c93993ddf0601eab19bf381ddf9762},
   year = {2017},
}
@article{Badrinarayanan2017,
   author = {V Badrinarayanan and A Kendall and R Cipolla},
   doi = {10.1109/TPAMI.2016.2644615},
   issue = {12},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   note = {cited By 7128},
   pages = {2481-2495},
   title = {SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation},
   volume = {39},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033697420&doi=10.1109%2fTPAMI.2016.2644615&partnerID=40&md5=1209b1085754e5f73e02de902063028a},
   year = {2017},
}
@inproceedings{Wang2018,
   author = {P Wang and P Chen and Y Yuan and D Liu and Z Huang and X Hou and G Cottrell},
   doi = {10.1109/WACV.2018.00163},
   journal = {Proceedings - 2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018},
   note = {cited By 802},
   pages = {1451-1460},
   title = {Understanding Convolution for Semantic Segmentation},
   volume = {2018-January},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051043298&doi=10.1109%2fWACV.2018.00163&partnerID=40&md5=0a426e9f32b2b103734372bc766b71ea},
   year = {2018},
}
@article{,
   author = {H D Sánchez and M Huertas-Company and M Bernardi and D Tuccillo and J L Fischer},
   doi = {10.1093/MNRAS/STY338},
   issue = {3},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 145},
   pages = {3661-3676},
   title = {Improving galaxy morphologies for SDSS with deep learning},
   volume = {476},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047266730&doi=10.1093%2fMNRAS%2fSTY338&partnerID=40&md5=02e370205e9597031d756b22670ec436},
   year = {2018},
}
@inproceedings{Sevak2018,
   author = {J S Sevak and A D Kapadia and J B Chavda and A Shah and M Rahevar},
   doi = {10.1109/ISS1.2017.8389420},
   journal = {Proceedings of the International Conference on Intelligent Sustainable Systems, ICISS 2017},
   note = {cited By 20},
   pages = {306-313},
   title = {Survey on semantic image segmentation techniques},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050032971&doi=10.1109%2fISS1.2017.8389420&partnerID=40&md5=df363c8ff7ab42802545b6a9c553cd7b},
   year = {2018},
}
@article{,
   author = {R E González and R P Muñoz and C A Hernández},
   doi = {10.1016/j.ascom.2018.09.004},
   journal = {Astronomy and Computing},
   note = {cited By 31},
   pages = {103-109},
   title = {Galaxy detection and identification using deep learning and data augmentation},
   volume = {25},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053837360&doi=10.1016%2fj.ascom.2018.09.004&partnerID=40&md5=f0b306c40df435dfd141fbecb6c400dd},
   year = {2018},
}
@article{Ma2019,
   author = {Z Ma and J Zhu and Y Zhu and H Xu},
   doi = {10.1007/978-981-32-9563-6_20},
   journal = {Communications in Computer and Information Science},
   note = {cited By 4},
   pages = {191-200},
   title = {Classification of radio galaxy images with semi-supervised learning},
   volume = {1071},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069983222&doi=10.1007%2f978-981-32-9563-6_20&partnerID=40&md5=3efd83b714178121b141c095db138b4b},
   year = {2019},
}
@article{Alom2019,
   author = {M Z Alom and T M Taha and C Yakopcic and S Westberg and P Sidike and M S Nasrin and M Hasan and B C Van Essen and A A S Awwal and V K Asari},
   doi = {10.3390/electronics8030292},
   issue = {3},
   journal = {Electronics (Switzerland)},
   note = {cited By 531},
   title = {A state-of-the-art survey on deep learning theory and architectures},
   volume = {8},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064323901&doi=10.3390%2felectronics8030292&partnerID=40&md5=86662b134866afe0a888b8ac356b7c8f},
   year = {2019},
}
@inproceedings{Lecun2019,
   author = {Y Lecun},
   doi = {10.1109/ISSCC.2019.8662396},
   journal = {Digest of Technical Papers - IEEE International Solid-State Circuits Conference},
   note = {cited By 71},
   pages = {12-19},
   title = {1.1 Deep Learning Hardware: Past, Present, and Future},
   volume = {2019-February},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063468498&doi=10.1109%2fISSCC.2019.8662396&partnerID=40&md5=9620a5913092442ba2a14db3db084861},
   year = {2019},
}
@article{,
   author = {A Vafaei Sadr and E E Vos and B A Bassett and Z Hosenie and N Oozeer and M Lochner},
   doi = {10.1093/mnras/stz131},
   issue = {2},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 17},
   pages = {2793-2806},
   title = {DeepSource: Point source detection using deep learning},
   volume = {484},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063373805&doi=10.1093%2fmnras%2fstz131&partnerID=40&md5=a88115f4326f03d9d14cb94d9ff273a8},
   year = {2019},
}
@article{Ghosh2019,
   author = {S Ghosh and N Das and I Das and U Maulik},
   doi = {10.1145/3329784},
   issue = {4},
   journal = {ACM Computing Surveys},
   note = {cited By 113},
   title = {Understanding deep learning techniques for image segmentation},
   volume = {52},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072022224&doi=10.1145%2f3329784&partnerID=40&md5=4df8566bf9674839cf9cd1ab989c4682},
   year = {2019},
}
@article{Wang2019,
   author = {X.-G. Wang and J.-S. Wang and P Tang and W.-Y. Liu},
   doi = {10.1007/s11390-019-1975-z},
   issue = {6},
   journal = {Journal of Computer Science and Technology},
   note = {cited By 3},
   pages = {1269-1278},
   title = {Weakly- and Semi-Supervised Fast Region-Based CNN for Object Detection},
   volume = {34},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076515970&doi=10.1007%2fs11390-019-1975-z&partnerID=40&md5=169587289f3f270017c2cea53e578b7d},
   year = {2019},
}
@article{Boucaud2020,
   author = {A Boucaud and M Huertas-Company and C Heneka and E E O Ishida and N Sedaghat and R S de Souza and B Moews and H Dole and M Castellano and E Merlin and V Roscani and A Tramacere and M Killedar and A M M Trindade},
   doi = {10.1093/mnras/stz3056},
   issue = {2},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 19},
   pages = {2481-2495},
   title = {Photometry of high-redshift blended galaxies using deep learning},
   volume = {491},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079691586&doi=10.1093%2fmnras%2fstz3056&partnerID=40&md5=be271d05f3cb7f72c6e2c846652652c7},
   year = {2020},
}
@article{Burke2019,
   author = {C J Burke and P D Aleo and Y.-C. Chen and X Liu and J R Peterson and G H Sembroski and J.Y.-Y. Lin},
   doi = {10.1093/mnras/stz2845},
   issue = {3},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 16},
   pages = {3952-3965},
   title = {Deblending and classifying astronomical sources with Mask R-CNN deep learning},
   volume = {490},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079601659&doi=10.1093%2fmnras%2fstz2845&partnerID=40&md5=dfaf4f426451bcfa87240fed43a99af6},
   year = {2019},
}
@inproceedings{Martinazzo2020,
   author = {A Martinazzo and M Espadoto and N S T Hirata},
   journal = {VISIGRAPP 2020 - Proceedings of the 15th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
   note = {cited By 3},
   pages = {87-95},
   title = {Deep learning for astronomical object classification: A case study},
   volume = {5},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083492576&partnerID=40&md5=4d31ae5637e3dee61035edae84a6ac46},
   year = {2020},
}
@article{Logan2020,
   author = {C H A Logan and S Fotopoulou},
   doi = {10.1051/0004-6361/201936648},
   journal = {Astronomy and Astrophysics},
   note = {cited By 9},
   title = {Unsupervised star, galaxy, QSO classification: Application of HDBSCAN},
   volume = {633},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089149548&doi=10.1051%2f0004-6361%2f201936648&partnerID=40&md5=9ab6570500f7ab8fed6b97ee62507337},
   year = {2020},
}
@article{Ahmed2020,
   author = {I Ahmed and M Ahmad and F A Khan and M Asif},
   doi = {10.1109/ACCESS.2020.3011406},
   journal = {IEEE Access},
   note = {cited By 27},
   pages = {136361-136373},
   title = {Comparison of deep-learning-based segmentation models: Using top view person images},
   volume = {8},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090385233&doi=10.1109%2fACCESS.2020.3011406&partnerID=40&md5=35ac1ef63dea0664f42c66aec113bb84},
   year = {2020},
}
@article{Paillassa2020,
   author = {M Paillassa and E Bertin and H Bouy},
   doi = {10.1051/0004-6361/201936345},
   journal = {Astronomy and Astrophysics},
   note = {cited By 3},
   title = {MAXI MASK and MAXI TRACK: Two new tools for identifying contaminants in astronomical images using convolutional neural networks},
   volume = {634},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088314366&doi=10.1051%2f0004-6361%2f201936345&partnerID=40&md5=ebf8407c275240967f4b6ad61d14a313},
   year = {2020},
}
@article{Cheng2020,
   author = {T.-Y. Cheng and C J Conselice and A Aragon-Salamanca and N Li and A F L Bluck and W G Hartley and J Annis and D Brooks and P Doel and J García-Bellido and D J James and K Kuehn and N Kuropatkin and M Smith and F Sobreira and G Tarle},
   doi = {10.1093/mnras/staa501},
   issue = {3},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 35},
   pages = {4209-4228},
   title = {Optimizing automatic morphological classification of galaxies with machine learning and deep learning using Dark Energy Survey imaging},
   volume = {493},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083917544&doi=10.1093%2fmnras%2fstaa501&partnerID=40&md5=cbd6e19795e184402894a075bfffb3ee},
   year = {2020},
}
@article{Jia2020,
   author = {P Jia and Q Liu and Y Sun},
   doi = {10.3847/1538-3881/ab800a},
   issue = {5},
   journal = {Astronomical Journal},
   note = {cited By 12},
   title = {Detection and Classification of Astronomical Targets with Deep Neural Networks in Wide-field Small Aperture Telescopes},
   volume = {159},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086580530&doi=10.3847%2f1538-3881%2fab800a&partnerID=40&md5=5bf826e3f895540d83fda365b123feb0},
   year = {2020},
}
@article{Hausen2020,
   author = {R Hausen and B E Robertson and B E Robertson},
   doi = {10.3847/1538-4365/ab8868},
   issue = {1},
   journal = {Astrophysical Journal, Supplement Series},
   note = {cited By 24},
   title = {Morpheus: A Deep Learning Framework for the Pixel-level Analysis of Astronomical Image Data},
   volume = {248},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087209259&doi=10.3847%2f1538-4365%2fab8868&partnerID=40&md5=7503d45326ec1128ad5e87f4d272242d},
   year = {2020},
}
@article{Chen2020,
   author = {C Chen and S Tang and J Li},
   doi = {10.11834/jig.190458},
   issue = {6},
   journal = {Journal of Image and Graphics},
   note = {cited By 1},
   pages = {1190-1200},
   title = {Weakly supervised semantic segmentation based on dynamic mask generation [动态生成掩膜弱监督语义分割]},
   volume = {25},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091014927&doi=10.11834%2fjig.190458&partnerID=40&md5=ac131d611893e5de82159b99dfb87e5d},
   year = {2020},
}
@article{Farias2020,
   author = {H Farias and D Ortiz and G Damke and M Jaque Arancibia and M Solar},
   doi = {10.1016/j.ascom.2020.100420},
   journal = {Astronomy and Computing},
   note = {cited By 3},
   title = {Mask galaxy: Morphological segmentation of galaxies},
   volume = {33},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090410967&doi=10.1016%2fj.ascom.2020.100420&partnerID=40&md5=a2b55d08d709f4027ec1486b21cea72e},
   year = {2020},
}
@article{Mostert2021,
   author = {R I J Mostert and K J Duncan and H J A Röttgering and K L Polsterer and P N Best and M Brienza and M Brüggen and M J Hardcastle and N Jurlin and B Mingo and R Morganti and T Shimwell and D Smith and W L Williams},
   doi = {10.1051/0004-6361/202038500},
   journal = {Astronomy and Astrophysics},
   note = {cited By 7},
   title = {Unveiling the rarest morphologies of the LOFAR Two-metre Sky Survey radio source population with self-organised maps},
   volume = {645},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099882184&doi=10.1051%2f0004-6361%2f202038500&partnerID=40&md5=9ba913b00e0b6c4aa8c7a393fd13429e},
   year = {2021},
}
@article{Kots2021,
   author = {M Kots and M Pozigun and A Konstantinov and V Chukanov},
   doi = {10.1007/978-981-33-6632-9_21},
   journal = {Smart Innovation, Systems and Technologies},
   note = {cited By 0},
   pages = {245-253},
   title = {Semi-supervised Learning for Medical Image Segmentation},
   volume = {220},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105857278&doi=10.1007%2f978-981-33-6632-9_21&partnerID=40&md5=4868e6aecd19ae478c128e1cf6760fd3},
   year = {2021},
}
@inproceedings{Alonso2021,
   author = {I Alonso and A Sabater and D Ferstl and L Montesano and A C Murillo},
   doi = {10.1109/ICCV48922.2021.00811},
   journal = {Proceedings of the IEEE International Conference on Computer Vision},
   note = {cited By 1},
   pages = {8199-8208},
   title = {Semi-Supervised Semantic Segmentation with Pixel-Level Contrastive Learning from a Class-wise Memory Bank},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126871474&doi=10.1109%2fICCV48922.2021.00811&partnerID=40&md5=73a08ff88d1997f85bed55a487d5947d},
   year = {2021},
}
@inproceedings{Fielding2021,
   author = {E Fielding and C N Nyirenda and M Vaccari},
   doi = {10.1109/ICECET52533.2021.9698414},
   journal = {International Conference on Electrical, Computer, and Energy Technologies, ICECET 2021},
   note = {cited By 0},
   title = {A Comparison of Deep Learning Architectures for Optical Galaxy Morphology Classification},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127045365&doi=10.1109%2fICECET52533.2021.9698414&partnerID=40&md5=9d3f363e61e3758017de06dc28c6333e},
   year = {2021},
}
@inproceedings{Chen2021,
   author = {H Chen and X Ma and T Xia and F Jia},
   doi = {10.1145/3456529.3456549},
   journal = {ACM International Conference Proceeding Series},
   note = {cited By 0},
   pages = {112-118},
   title = {Semi-supervised Semantic Segmentation of Cataract Surgical Images based on DeepLab v3+},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112405832&doi=10.1145%2f3456529.3456549&partnerID=40&md5=79310f8674ec23e387ae93e23daa8d97},
   year = {2021},
}
@article{Spindler2021,
   author = {A Spindler and J E Geach and M J Smith},
   doi = {10.1093/mnras/staa3670},
   issue = {1},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 12},
   pages = {985-1007},
   title = {AstroVaDEr: Astronomical variational deep embedder for unsupervised morphological classification of galaxies and synthetic image generation},
   volume = {502},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105024377&doi=10.1093%2fmnras%2fstaa3670&partnerID=40&md5=12eacf206d33a9f33e64ad162ff88b84},
   year = {2021},
}
@article{Becker2021,
   author = {B Becker and M Vaccari and M Prescott and T Grobler},
   doi = {10.1093/mnras/stab325},
   issue = {2},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 6},
   pages = {1828-1846},
   title = {CNN architecture comparison for radio galaxy classification},
   volume = {503},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107867055&doi=10.1093%2fmnras%2fstab325&partnerID=40&md5=557999d3b67999e52ad41c8443c83511},
   year = {2021},
}
@article{Bengio2021,
   author = {Y Bengio and Y Lecun and G Hinton},
   doi = {10.1145/3448250},
   issue = {7},
   journal = {Communications of the ACM},
   note = {cited By 56},
   pages = {58-65},
   title = {Deep learning for AI},
   volume = {64},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108519656&doi=10.1145%2f3448250&partnerID=40&md5=94c84fbb59edc3d0fd2fce789df60333},
   year = {2021},
}
@article{Cheng2021,
   author = {T.-Y. Cheng and C J Conselice and A Aragón-Salamanca and M Aguena and S Allam and F Andrade-Oliveira and J Annis and A F L Bluck and D Brooks and D L Burke and M C Kind and J Carretero and A Choi and M Costanzi and L N da Costa and M E S Pereira and J de Vicente and H T Diehl and A Drlica-Wagner and K Eckert and S Everett and A E Evrard and I Ferrero and P Fosalba and J Frieman and J García-Bellido and D W Gerdes and T Giannantonio and D Gruen and R A Gruendl and J Gschwend and G Gutierrez and S R Hinton and D L Hollowood and K Honscheid and D J James and E Krause and K Kuehn and N Kuropatkin and O Lahav and M A G Maia and M March and F Menanteau and R Miquel and R Morgan and F Paz-Chinchón and A Pieres and A A P Malagón and A Roodman and E Sanchez and V Scarpine and S Serrano and I Sevilla-Noarbe and M Smith and M Soares-Santos and E Suchyta and M E C Swanson and G Tarle and D Thomas},
   doi = {10.1093/mnras/stab2142},
   issue = {3},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 5},
   pages = {4425-4444},
   title = {Galaxy morphological classification catalogue of the Dark Energy Survey Year 3 data with convolutional neural networks},
   volume = {507},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114423282&doi=10.1093%2fmnras%2fstab2142&partnerID=40&md5=5dd9261eec2a05a90b1a33e7bc994971},
   year = {2021},
}
@article{Alzubaidi2021,
   author = {L Alzubaidi and J Zhang and A J Humaidi and A Al-Dujaili and Y Duan and O Al-Shamma and J Santamaría and M A Fadhel and M Al-Amidie and L Farhan},
   doi = {10.1186/s40537-021-00444-8},
   issue = {1},
   journal = {Journal of Big Data},
   note = {cited By 286},
   title = {Review of deep learning: concepts, CNN architectures, challenges, applications, future directions},
   volume = {8},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103843272&doi=10.1186%2fs40537-021-00444-8&partnerID=40&md5=496b9e0e5849c64409f425ffd1076275},
   year = {2021},
}
@article{Zhou2022,
   author = {Y Zhou and R Jiao and D Wang and J Mu and J Li},
   doi = {10.1109/ACCESS.2022.3172664},
   journal = {IEEE Access},
   note = {cited By 0},
   pages = {48855-48864},
   title = {Catastrophic Forgetting Problem in Semi-Supervised Semantic Segmentation},
   volume = {10},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129631065&doi=10.1109%2fACCESS.2022.3172664&partnerID=40&md5=4a3ad56584f304d8de2b4dd9f683c779},
   year = {2022},
}
@article{Vavilova2022,
   author = {I B Vavilova and V Khramtsov and D V Dobrycheva and M.Yu. Vasylenko and A A Elyiv and O V Melnyk},
   doi = {10.15407/knit2022.01.003},
   issue = {1},
   journal = {Space Science and Technology},
   note = {cited By 0},
   pages = {3-22},
   title = {MACHINE LEARNING TECHNIQUE FOR MORPHOLOGICAL CLASSIFICATION OF GALAXIES FROM SDSS. II. THE IMAGE-BASED MORPHOLOGICAL CATALOGS OF GALAXIES AT 0.02 &lt; Z &lt; 0.1 [МАШИННЕ НАВЧАННЯ ДЛЯ МОРФОЛОГІЧНОЇ КЛАСИФІКАЦІЇ ГАЛАКТИК ІЗ ОГЛЯДУ SDSS. II. МОРФОЛОГІЧНІ КАТАЛОГИ ЗОБРАЖЕНЬ ГАЛАКТИК НА 0.02 &lt; Z &lt; 0.1]},
   volume = {28},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130364484&doi=10.15407%2fknit2022.01.003&partnerID=40&md5=f87c9b57a27f2a9296a2ca579b34d7c0},
   year = {2022},
}
@article{Sen2022,
   author = {S Sen and S Agarwal and P Chakraborty and K P Singh},
   doi = {10.1007/s10686-021-09827-4},
   issue = {1},
   journal = {Experimental Astronomy},
   note = {cited By 1},
   title = {Astronomical big data processing using machine learning: A comprehensive review},
   volume = {53},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123175630&doi=10.1007%2fs10686-021-09827-4&partnerID=40&md5=2e73d97ec0fb5b7b60b61633553e9001},
   year = {2022},
}
@article{Minaee2022,
   author = {S Minaee and Y Boykov and F Porikli and A Plaza and N Kehtarnavaz and D Terzopoulos},
   doi = {10.1109/TPAMI.2021.3059968},
   issue = {7},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   note = {cited By 160},
   pages = {3523-3542},
   title = {Image Segmentation Using Deep Learning: A Survey},
   volume = {44},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100948197&doi=10.1109%2fTPAMI.2021.3059968&partnerID=40&md5=115a97279a77fb75cf00d20ca1578417},
   year = {2022},
}
@article{Lai2022,
   author = {W Lai and F Hu and X Kong and P Yan and K Bian and X Dai},
   doi = {10.1016/j.powtec.2022.117655},
   journal = {Powder Technology},
   note = {cited By 0},
   title = {The study of coal gangue segmentation for location and shape predicts based on multispectral and improved Mask R-CNN},
   volume = {407},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132730250&doi=10.1016%2fj.powtec.2022.117655&partnerID=40&md5=b7e0712e597ba45454fc61fbfc4d428d},
   year = {2022},
}
@article{Bertin1994,
   author = {E Bertin},
   doi = {10.1007/BF00990023},
   issue = {1-2},
   journal = {Astrophysics and Space Science},
   note = {cited By 7},
   pages = {49-51},
   title = {Classification of astronomical images with a neural network},
   volume = {217},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-21844501965&doi=10.1007%2fBF00990023&partnerID=40&md5=5625a11c5da7761bb5d169bd3792d00b},
   year = {1994},
}
@article{,
   author = {J de la Calleja and O Fuentes},
   doi = {10.1007/978-3-540-30134-9_55},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   note = {cited By 12},
   pages = {411-418},
   title = {Automated classification of galaxy images},
   volume = {3215},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975474600&doi=10.1007%2f978-3-540-30134-9_55&partnerID=40&md5=837e0b3b1049c8e04526dfc24f8574e0},
   year = {2004},
}
@article{Ball2010,
   author = {N M Ball and R J Brunner},
   doi = {10.1142/S0218271810017160},
   issue = {7},
   journal = {International Journal of Modern Physics D},
   note = {cited By 187},
   pages = {1049-1106},
   title = {Data mining and machine learning in astronomy},
   volume = {19},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955253091&doi=10.1142%2fS0218271810017160&partnerID=40&md5=1351b59da5671cda0e36fb8715f4a32e},
   year = {2010},
}
@inproceedings{Friedlander2012,
   author = {A Friedlander and M Frean and M Johnston-Hollitt and C Hollitt},
   doi = {10.1145/2425836.2425918},
   journal = {ACM International Conference Proceeding Series},
   note = {cited By 2},
   pages = {429-434},
   title = {Latent Dirichlet allocation for image segmentation and source finding in radio astronomy images},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873401885&doi=10.1145%2f2425836.2425918&partnerID=40&md5=59f94d1850ff0ea75e12074b152190d3},
   year = {2012},
}
@article{Gieseke2013,
   author = {F Gieseke},
   doi = {10.1007/s13218-013-0248-1},
   issue = {3},
   journal = {KI - Kunstliche Intelligenz},
   note = {cited By 1},
   pages = {281-285},
   title = {From Supervised to Unsupervised Support Vector Machines and Applications in Astronomy},
   volume = {27},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087950387&doi=10.1007%2fs13218-013-0248-1&partnerID=40&md5=2898101d0e9e75f87026a3eed6434925},
   year = {2013},
}
@article{Hobson2015,
   author = {M Hobson and P Graff and F Feroz and A Lasenby},
   doi = {10.1017/S1743921314013672},
   journal = {Proceedings of the International Astronomical Union},
   note = {cited By 6},
   pages = {279-287},
   title = {Machine-learning in astronomy},
   volume = {10},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958983270&doi=10.1017%2fS1743921314013672&partnerID=40&md5=3e0eba5325608cee0c12fa1bff3d2529},
   year = {2015},
}
@article{Dieleman2015,
   author = {S Dieleman and K W Willett and J Dambre},
   doi = {10.1093/mnras/stv632},
   issue = {2},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 407},
   pages = {1441-1459},
   title = {Rotation-invariant convolutional neural networks for galaxy morphology prediction},
   volume = {450},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936124348&doi=10.1093%2fmnras%2fstv632&partnerID=40&md5=f3bdef9ae5d9f46c8005ec499d889d44},
   year = {2015},
}
@inproceedings{Long2015,
   author = {J Long and E Shelhamer and T Darrell},
   doi = {10.1109/CVPR.2015.7298965},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   note = {cited By 15194},
   pages = {431-440},
   title = {Fully convolutional networks for semantic segmentation},
   volume = {07-12-June-2015},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959205572&doi=10.1109%2fCVPR.2015.7298965&partnerID=40&md5=2c171b6055fca5fbb1871fa72ee0ee41},
   year = {2015},
}
@article{Kartaltepe2015,
   author = {J S Kartaltepe and M Mozena and D Kocevski and D H McIntosh and J Lotz and E F Bell and S Faber and H Ferguson and D Koo and R Bassett and M Bernyk and K Blancato and F Bournaud and P Cassata and M Castellano and E Cheung and C J Conselice and D Croton and T Dahlen and D F De Mello and L Degroot and J Donley and J Guedes and N Grogin and N Hathi and M Hilton and B Hollon and A Koekemoer and N Liu and R A Lucas and M Martig and E McGrath and C McPartland and B Mobasher and A Morlock and E O'Leary and M Peth and J Pforr and A Pillepich and D Rosario and E Soto and A Straughn and O Telford and B Sunnquist and J Trump and B Weiner and S Wuyts},
   doi = {10.1088/0067-0049/221/1/11},
   issue = {1},
   journal = {Astrophysical Journal, Supplement Series},
   note = {cited By 77},
   title = {Candels visual classifications: Scheme, data release, and first results},
   volume = {221},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948742037&doi=10.1088%2f0067-0049%2f221%2f1%2f11&partnerID=40&md5=812160447b0000a78bad146b2a12863f},
   year = {2015},
}
@article{Kremer2017,
   author = {J Kremer and K Stensbo-Smidt and F Gieseke and K S Pedersen and C Igel},
   doi = {10.1109/MIS.2017.40},
   issue = {2},
   journal = {IEEE Intelligent Systems},
   note = {cited By 47},
   pages = {16-22},
   title = {Big Universe, Big Data: Machine Learning and Image Analysis for Astronomy},
   volume = {32},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017157521&doi=10.1109%2fMIS.2017.40&partnerID=40&md5=037c13d48cfeb3ca4f55e653a5f74add},
   year = {2017},
}
@article{Zhao2017,
   author = {B Zhao and J Feng and X Wu and S Yan},
   doi = {10.1007/s11633-017-1053-3},
   issue = {2},
   journal = {International Journal of Automation and Computing},
   note = {cited By 184},
   pages = {119-135},
   title = {A survey on deep learning-based fine-grained object classification and semantic segmentation},
   volume = {14},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009812471&doi=10.1007%2fs11633-017-1053-3&partnerID=40&md5=3326d044cea5a6c750fac2a7b896eb87},
   year = {2017},
}
@article{Liu2017,
   author = {W Liu and Z Wang and X Liu and N Zeng and Y Liu and F E Alsaadi},
   doi = {10.1016/j.neucom.2016.12.038},
   journal = {Neurocomputing},
   note = {cited By 1628},
   pages = {11-26},
   title = {A survey of deep neural network architectures and their applications},
   volume = {234},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010651075&doi=10.1016%2fj.neucom.2016.12.038&partnerID=40&md5=2604d57c32f7d54249dbe43ab24c912d},
   year = {2017},
}
@inproceedings{He2017,
   author = {K He and G Gkioxari and P Dollar and R Girshick},
   doi = {10.1109/ICCV.2017.322},
   journal = {Proceedings of the IEEE International Conference on Computer Vision},
   note = {cited By 9790},
   pages = {2980-2988},
   title = {Mask R-CNN},
   volume = {2017-October},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040313738&doi=10.1109%2fICCV.2017.322&partnerID=40&md5=ec54a6e3213e19d0e80a918920697dc5},
   year = {2017},
}
@article{Soo2018,
   author = {J Y H Soo and B Moraes and B Joachimi and W Hartley and O Lahav and A Charbonnier and M Makler and M E S Pereira and J Comparat and T Erben and A Leauthaud and H Shan and L Van Waerbeke},
   doi = {10.1093/mnras/stx3201},
   issue = {3},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 28},
   pages = {3613-3632},
   title = {Morpho-z: Improving photometric redshifts with galaxy morphology},
   volume = {475},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042427499&doi=10.1093%2fmnras%2fstx3201&partnerID=40&md5=fa98f2ca58a280961050d2d78068b061},
   year = {2018},
}
@article{Chen2018,
   author = {L.-C. Chen and G Papandreou and I Kokkinos and K Murphy and A L Yuille},
   doi = {10.1109/TPAMI.2017.2699184},
   issue = {4},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   note = {cited By 7110},
   pages = {834-848},
   title = {DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs},
   volume = {40},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042712042&doi=10.1109%2fTPAMI.2017.2699184&partnerID=40&md5=3749c2afbfb0702e32e54f7ddbe7956a},
   year = {2018},
}
@article{Dimauro2018,
   author = {P Dimauro and M Huertas-Company and E Daddi and P G Pérez-González and M Bernardi and G Barro and F Buitrago and F Caro and A Cattaneo and H Dominguez-Sánchez and S M Faber and B Häußler and D D Kocevski and A M Koekemoer and D C Koo and C T Lee and S Mei and B Margalef-Bentabol and J Primack and A Rodriguez-Puebla and M Salvato and F Shankar and D Tuccillo},
   doi = {10.1093/mnras/sty1379},
   issue = {4},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 32},
   pages = {5410-5426},
   title = {A catalog of polychromatic bulge-disc decompositions of ~17.600 galaxies in CANDELS},
   volume = {478},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050825242&doi=10.1093%2fmnras%2fsty1379&partnerID=40&md5=7d90a9921fc1301b96a6459cc8104bf7},
   year = {2018},
}
@article{,
   author = {H D Sánchez and M Huertas-Company and M Bernardi and D Tuccillo and J L Fischer},
   doi = {10.1093/MNRAS/STY338},
   issue = {3},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 145},
   pages = {3661-3676},
   title = {Improving galaxy morphologies for SDSS with deep learning},
   volume = {476},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047266730&doi=10.1093%2fMNRAS%2fSTY338&partnerID=40&md5=02e370205e9597031d756b22670ec436},
   year = {2018},
}
@article{,
   author = {A Garcia-Garcia and S Orts-Escolano and S Oprea and V Villena-Martinez and P Martinez-Gonzalez and J Garcia-Rodriguez},
   doi = {10.1016/j.asoc.2018.05.018},
   journal = {Applied Soft Computing Journal},
   note = {cited By 356},
   pages = {41-65},
   title = {A survey on deep learning techniques for image and video semantic segmentation},
   volume = {70},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048708333&doi=10.1016%2fj.asoc.2018.05.018&partnerID=40&md5=150488012f60dd7f769d88461bb3ad3c},
   year = {2018},
}
@article{,
   author = {R E González and R P Muñoz and C A Hernández},
   doi = {10.1016/j.ascom.2018.09.004},
   journal = {Astronomy and Computing},
   note = {cited By 31},
   pages = {103-109},
   title = {Galaxy detection and identification using deep learning and data augmentation},
   volume = {25},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053837360&doi=10.1016%2fj.ascom.2018.09.004&partnerID=40&md5=f0b306c40df435dfd141fbecb6c400dd},
   year = {2018},
}
@article{Wu2019,
   author = {C Wu and O I Wong and L Rudnick and S S Shabala and M J Alger and J K Banfield and C S Ong and S V White and A F Garon and R P Norris and H Andernach and J Tate and V Lukic and H Tang and K Schawinski and F I Diakogiannis},
   doi = {10.1093/mnras/sty2646},
   issue = {1},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 35},
   pages = {1211-1230},
   title = {Radio Galaxy Zoo: CLARAN - A deep learning classifier for radio morphologies},
   volume = {482},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057199911&doi=10.1093%2fmnras%2fsty2646&partnerID=40&md5=94606f642a9c56c01c3faf334354b7b4},
   year = {2019},
}
@inproceedings{Vilalta2018,
   author = {R Vilalta},
   doi = {10.1088/1742-6596/1085/5/052014},
   issue = {5},
   journal = {Journal of Physics: Conference Series},
   note = {cited By 2},
   title = {Transfer Learning in Astronomy: A New Machine-Learning Paradigm},
   volume = {1085},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055659474&doi=10.1088%2f1742-6596%2f1085%2f5%2f052014&partnerID=40&md5=7c3dd6acb5556bd30f08213209435629},
   year = {2018},
}
@article{Li2019,
   author = {W Li and H Xu and Z Ma and R Zhu and D Hu and Z Zhu and J Gu and C Shan and J Zhu and X.-P. Wu},
   doi = {10.1093/mnras/stz582},
   issue = {2},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 11},
   pages = {2628-2637},
   title = {Separating the EoR signal with a convolutional denoising autoencoder: A deep-learning-based method},
   volume = {485},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067059186&doi=10.1093%2fmnras%2fstz582&partnerID=40&md5=ba59dcca6c56f9be71936fe675267fe0},
   year = {2019},
}
@article{,
   author = {H Doḿinguez Sanchez and M Huertas-Company and M Bernardi and S Kaviraj and J L Fischer and T M C Abbott and F B Abdalla and J Annis and S Avila and D Brooks and E Buckley-Geer and A Carnero Rosell and M Carrasco Kind and J Carretero and C E Cunha and C B D'Andrea and L N Da Costa and C Davis and J De Vicente and P Doel and A E Evrard and P Fosalba and J Frieman and J Garćia-Bellido and E Gaztanaga and D W Gerdes and D Gruen and R A Gruendl and J Gschwend and G Gutierrez and W G Hartley and D L Hollowood and K Honscheid and B Hoyle and D J James and K Kuehn and N Kuropatkin and O Lahav and M A G Maia and M March and P Melchior and F Menanteau and R Miquel and B Nord and A A Plazas and E Sanchez and V Scarpine and R Schindler and M Schubnell and M Smith and R C Smith and M Soares-Santos and F Sobreira and E Suchyta and M E C Swanson and G Tarle and D Thomas and A R Walker and J Zuntz},
   doi = {10.1093/mnras/sty3497},
   issue = {1},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 16},
   pages = {93-100},
   title = {Transfer learning for galaxy morphology from one survey to another},
   volume = {484},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062264285&doi=10.1093%2fmnras%2fsty3497&partnerID=40&md5=39ed6dc3c0794e6a3f27df3f5e1c478e},
   year = {2019},
}
@article{,
   author = {M Jiménez and I Triguero and R John},
   doi = {10.1016/j.ins.2018.12.011},
   journal = {Information Sciences},
   note = {cited By 15},
   pages = {301-320},
   title = {Handling uncertainty in citizen science data: Towards an improved amateur-based large-scale classification},
   volume = {479},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058419696&doi=10.1016%2fj.ins.2018.12.011&partnerID=40&md5=59a90159880fc481a523a5c7c6ba6229},
   year = {2019},
}
@article{Wang2019,
   author = {W Wang and Y Yang and X Wang and W Wang and J Li},
   doi = {10.1117/1.OE.58.4.040901},
   issue = {4},
   journal = {Optical Engineering},
   note = {cited By 86},
   title = {Development of convolutional neural network and its application in image classification: A survey},
   volume = {58},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065484828&doi=10.1117%2f1.OE.58.4.040901&partnerID=40&md5=24b960f03e7a0a683749f07e7976ec2c},
   year = {2019},
}
@article{Dobbels2019,
   author = {W Dobbels and S Krier and S Pirson and S Viaene and G De Geyter and S Salim and M Baes},
   doi = {10.1051/0004-6361/201834575},
   journal = {Astronomy and Astrophysics},
   note = {cited By 4},
   title = {Morphology-assisted galaxy mass-to-light predictions using deep learning},
   volume = {624},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065023132&doi=10.1051%2f0004-6361%2f201834575&partnerID=40&md5=9b206ec3d0e610d0af77e3c774268e1a},
   year = {2019},
}
@article{Lateef2019,
   author = {F Lateef and Y Ruichek},
   doi = {10.1016/j.neucom.2019.02.003},
   journal = {Neurocomputing},
   note = {cited By 215},
   pages = {321-348},
   title = {Survey on semantic segmentation using deep learning techniques},
   volume = {338},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061756782&doi=10.1016%2fj.neucom.2019.02.003&partnerID=40&md5=c4b7aaac6ce1711705862b5a368eecb1},
   year = {2019},
}
@article{Wang2019,
   author = {X.-G. Wang and J.-S. Wang and P Tang and W.-Y. Liu},
   doi = {10.1007/s11390-019-1975-z},
   issue = {6},
   journal = {Journal of Computer Science and Technology},
   note = {cited By 3},
   pages = {1269-1278},
   title = {Weakly- and Semi-Supervised Fast Region-Based CNN for Object Detection},
   volume = {34},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076515970&doi=10.1007%2fs11390-019-1975-z&partnerID=40&md5=169587289f3f270017c2cea53e578b7d},
   year = {2019},
}
@article{Burke2019,
   author = {C J Burke and P D Aleo and Y.-C. Chen and X Liu and J R Peterson and G H Sembroski and J.Y.-Y. Lin},
   doi = {10.1093/mnras/stz2845},
   issue = {3},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 16},
   pages = {3952-3965},
   title = {Deblending and classifying astronomical sources with Mask R-CNN deep learning},
   volume = {490},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079601659&doi=10.1093%2fmnras%2fstz2845&partnerID=40&md5=dfaf4f426451bcfa87240fed43a99af6},
   year = {2019},
}
@article{Barchi2020,
   author = {P H Barchi and R R de Carvalho and R R Rosa and R A Sautter and M Soares-Santos and B A D Marques and E Clua and T S Gonçalves and C de Sá-Freitas and T C Moura},
   doi = {10.1016/j.ascom.2019.100334},
   journal = {Astronomy and Computing},
   note = {cited By 36},
   title = {Machine and Deep Learning applied to galaxy morphology - A comparative study},
   volume = {30},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074279988&doi=10.1016%2fj.ascom.2019.100334&partnerID=40&md5=edab2ec324f1b86645c65431ff9dc4f9},
   year = {2020},
}
@article{Bharati2020,
   author = {P Bharati and A Pramanik},
   doi = {10.1007/978-981-13-9042-5_56},
   journal = {Advances in Intelligent Systems and Computing},
   note = {cited By 33},
   pages = {657-668},
   title = {Deep Learning Techniques—R-CNN to Mask R-CNN: A Survey},
   volume = {999},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077122324&doi=10.1007%2f978-981-13-9042-5_56&partnerID=40&md5=5ab39c3f6259e6972efd624052c39735},
   year = {2020},
}
@article{Boucaud2020,
   author = {A Boucaud and M Huertas-Company and C Heneka and E E O Ishida and N Sedaghat and R S de Souza and B Moews and H Dole and M Castellano and E Merlin and V Roscani and A Tramacere and M Killedar and A M M Trindade},
   doi = {10.1093/mnras/stz3056},
   issue = {2},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 19},
   pages = {2481-2495},
   title = {Photometry of high-redshift blended galaxies using deep learning},
   volume = {491},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079691586&doi=10.1093%2fmnras%2fstz3056&partnerID=40&md5=be271d05f3cb7f72c6e2c846652652c7},
   year = {2020},
}
@article{Jimenez2020,
   author = {M Jimenez and M Torres Torres and R John and I Triguero},
   doi = {10.1109/ACCESS.2020.2978804},
   journal = {IEEE Access},
   note = {cited By 8},
   pages = {47232-47246},
   title = {Galaxy image classification based on citizen science data: A comparative study},
   volume = {8},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082172694&doi=10.1109%2fACCESS.2020.2978804&partnerID=40&md5=7578263554be0b0ae67362011e089f59},
   year = {2020},
}
@article{Logan2020,
   author = {C H A Logan and S Fotopoulou},
   doi = {10.1051/0004-6361/201936648},
   journal = {Astronomy and Astrophysics},
   note = {cited By 9},
   title = {Unsupervised star, galaxy, QSO classification: Application of HDBSCAN},
   volume = {633},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089149548&doi=10.1051%2f0004-6361%2f201936648&partnerID=40&md5=9ab6570500f7ab8fed6b97ee62507337},
   year = {2020},
}
@article{He2020,
   author = {K He and G Gkioxari and P Dollár and R Girshick},
   doi = {10.1109/TPAMI.2018.2844175},
   issue = {2},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   note = {cited By 920},
   pages = {386-397},
   title = {Mask R-CNN},
   volume = {42},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048205789&doi=10.1109%2fTPAMI.2018.2844175&partnerID=40&md5=017faa4e71724d54c516ceea41ce0340},
   year = {2020},
}
@article{,
   author = {L Cabayol-Garcia and M Eriksen and A Alarcón and A Amara and J Carretero and R Casas and F J Castander and E Fernández and J García-Bellido and E Gaztanaga and H Hoekstra and R Miquel and C Neissner and C Padilla and E Sánchez and S Serrano and I Sevilla-Noarbe and M Siudek and P Tallada and L Tortorelli},
   doi = {10.1093/mnras/stz3274},
   issue = {4},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 3},
   pages = {5392-5405},
   title = {The PAU survey: Background light estimation with deep learning techniques},
   volume = {491},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096993929&doi=10.1093%2fmnras%2fstz3274&partnerID=40&md5=f171ea06535b8f8b197d16448fa71a61},
   year = {2020},
}
@article{Fluke2020,
   author = {C J Fluke and C Jacobs},
   doi = {10.1002/widm.1349},
   issue = {2},
   journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
   note = {cited By 35},
   title = {Surveying the reach and maturity of machine learning and artificial intelligence in astronomy},
   volume = {10},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076878415&doi=10.1002%2fwidm.1349&partnerID=40&md5=f3c82b63fa809d22f518ff8089d496f1},
   year = {2020},
}
@article{Lukic2020,
   author = {V Lukic and F de Gasperin and M Brüggen},
   doi = {10.3390/GALAXIES8010003},
   issue = {1},
   journal = {Galaxies},
   note = {cited By 5},
   title = {ConvoSource: Radio-Astronomical source-finding with convolutional neural networks},
   volume = {8},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079819732&doi=10.3390%2fGALAXIES8010003&partnerID=40&md5=2124c8103a10b4d1c2278e1a3f7e4b20},
   year = {2020},
}
@article{Jia2020,
   author = {P Jia and Q Liu and Y Sun},
   doi = {10.3847/1538-3881/ab800a},
   issue = {5},
   journal = {Astronomical Journal},
   note = {cited By 12},
   title = {Detection and Classification of Astronomical Targets with Deep Neural Networks in Wide-field Small Aperture Telescopes},
   volume = {159},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086580530&doi=10.3847%2f1538-3881%2fab800a&partnerID=40&md5=5bf826e3f895540d83fda365b123feb0},
   year = {2020},
}
@article{Hausen2020,
   author = {R Hausen and B E Robertson and B E Robertson},
   doi = {10.3847/1538-4365/ab8868},
   issue = {1},
   journal = {Astrophysical Journal, Supplement Series},
   note = {cited By 24},
   title = {Morpheus: A Deep Learning Framework for the Pixel-level Analysis of Astronomical Image Data},
   volume = {248},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087209259&doi=10.3847%2f1538-4365%2fab8868&partnerID=40&md5=7503d45326ec1128ad5e87f4d272242d},
   year = {2020},
}
@article{Chen2020,
   author = {C Chen and S Tang and J Li},
   doi = {10.11834/jig.190458},
   issue = {6},
   journal = {Journal of Image and Graphics},
   note = {cited By 1},
   pages = {1190-1200},
   title = {Weakly supervised semantic segmentation based on dynamic mask generation [动态生成掩膜弱监督语义分割]},
   volume = {25},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091014927&doi=10.11834%2fjig.190458&partnerID=40&md5=ac131d611893e5de82159b99dfb87e5d},
   year = {2020},
}
@inproceedings{Juyal2020,
   author = {P Juyal and S Sharma},
   doi = {10.1109/ICCCNT49239.2020.9225509},
   journal = {2020 11th International Conference on Computing, Communication and Networking Technologies, ICCCNT 2020},
   note = {cited By 11},
   title = {Estimation of Tree Volume Using Mask R-CNN based Deep Learning},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096621260&doi=10.1109%2fICCCNT49239.2020.9225509&partnerID=40&md5=9a0411dc61cc33b0d6427305c7700cde},
   year = {2020},
}
@article{Liu2020,
   author = {Y.-X. Liu and B Zhang and B Wang},
   doi = {10.11972/j.issn.1001-9014.2020.04.012},
   issue = {4},
   journal = {Hongwai Yu Haomibo Xuebao/Journal of Infrared and Millimeter Waves},
   note = {cited By 3},
   pages = {473-482},
   title = {Semi-supervised semantic segmentation based on Generative Adversarial Networks for remote sensing images [基于生成式对抗网络的遥感图像半监督语义分割]},
   volume = {39},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094931006&doi=10.11972%2fj.issn.1001-9014.2020.04.012&partnerID=40&md5=d0ff74f33999b2bb806f39f9caab7f6c},
   year = {2020},
}
@inproceedings{Tang2020,
   author = {R Tang},
   doi = {10.1109/CDS49703.2020.00008},
   journal = {Proceedings - 2020 International Conference on Computing and Data Science, CDS 2020},
   note = {cited By 1},
   pages = {3-6},
   title = {Machine learning meets astronomy},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098842813&doi=10.1109%2fCDS49703.2020.00008&partnerID=40&md5=13f1798c69c02397af4177edf8c345a2},
   year = {2020},
}
@article{Galvin2020,
   author = {T J Galvin and M T Huynh and R P Norris and X R Wang and E Hopkins and K Polsterer and N O Ralph and A N O'brien and G H Heald},
   doi = {10.1093/mnras/staa1890},
   issue = {3},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 7},
   pages = {2730-2758},
   title = {Cataloguing the radio-sky with unsupervised machine learning: A new approach for the SKA era},
   volume = {497},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095536488&doi=10.1093%2fmnras%2fstaa1890&partnerID=40&md5=ae1060e792123eb3866bfd7c19bc6f1b},
   year = {2020},
}
@article{Farias2020,
   author = {H Farias and D Ortiz and G Damke and M Jaque Arancibia and M Solar},
   doi = {10.1016/j.ascom.2020.100420},
   journal = {Astronomy and Computing},
   note = {cited By 3},
   title = {Mask galaxy: Morphological segmentation of galaxies},
   volume = {33},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090410967&doi=10.1016%2fj.ascom.2020.100420&partnerID=40&md5=a2b55d08d709f4027ec1486b21cea72e},
   year = {2020},
}
@article{Sadr2020,
   author = {A V Sadr and B A Bassett and N Oozeer and Y Fantaye and C Finlay},
   doi = {10.1093/mnras/staa2724},
   issue = {1},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 2},
   pages = {379-390},
   title = {Deep learning improves identification of Radio Frequency Interference},
   volume = {499},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098577837&doi=10.1093%2fmnras%2fstaa2724&partnerID=40&md5=e155665e5b1df9d36c85107310d60f0d},
   year = {2020},
}
@inproceedings{Zi2020,
   author = {M X Zi},
   doi = {10.1109/ICCEIC51584.2020.00026},
   journal = {Proceedings - 2020 International Conference on Computer Engineering and Intelligent Control, ICCEIC 2020},
   note = {cited By 3},
   pages = {92-96},
   title = {A survey of FPGA based on graph convolutional neural network accelerator},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102631035&doi=10.1109%2fICCEIC51584.2020.00026&partnerID=40&md5=b4f28759cdaf3ba2bd9a98af52632846},
   year = {2020},
}
@article{Khan2020,
   author = {A Khan and A Sohail and U Zahoora and A S Qureshi},
   doi = {10.1007/s10462-020-09825-6},
   issue = {8},
   journal = {Artificial Intelligence Review},
   note = {cited By 651},
   pages = {5455-5516},
   title = {A survey of the recent architectures of deep convolutional neural networks},
   volume = {53},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084072573&doi=10.1007%2fs10462-020-09825-6&partnerID=40&md5=daddafb5a455a7d19c470bf7f122e10e},
   year = {2020},
}
@inproceedings{Arslan2020,
   author = {E A Arslan},
   doi = {10.1145/3448823.3448881},
   journal = {ACM International Conference Proceeding Series},
   note = {cited By 0},
   title = {Radio galaxy morphology classification with mask R-CNN},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102940394&doi=10.1145%2f3448823.3448881&partnerID=40&md5=be3d61ff009a8c7effd2dcee9903f80d},
   year = {2020},
}
@inproceedings{Pham2020,
   author = {V Pham and C Pham and T Dang},
   doi = {10.1109/BigData50022.2020.9378027},
   journal = {Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020},
   note = {cited By 12},
   pages = {5592-5601},
   title = {Road Damage Detection and Classification with Detectron2 and Faster R-CNN},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103854918&doi=10.1109%2fBigData50022.2020.9378027&partnerID=40&md5=46b396f7f611b0fb4c67135c14b5b214},
   year = {2020},
}
@article{Zhuang2021,
   author = {F Zhuang and Z Qi and K Duan and D Xi and Y Zhu and H Zhu and H Xiong and Q He},
   doi = {10.1109/JPROC.2020.3004555},
   issue = {1},
   journal = {Proceedings of the IEEE},
   note = {cited By 587},
   pages = {43-76},
   title = {A Comprehensive Survey on Transfer Learning},
   volume = {109},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088803089&doi=10.1109%2fJPROC.2020.3004555&partnerID=40&md5=0fa6f3e7a994b6a931ad46ac92607f78},
   year = {2021},
}
@article{Bonaldi2021,
   author = {A Bonaldi and T An and M Bruggen and S Burkutean and B Coelho and H Goodarzi and P Hartley and P K Sandhu and C Wu and L Yu and M H Zhoolideh Haghighi and S Anton and Z Bagheri and D Barbosa and J P Barraca and D Bartashevich and M Bergano and M Bonato and J Brand and F de Gasperin and A Giannetti and R Dodson and P Jain and S Jaiswal and B Lao and B Liu and E Liuzzo and Y Lu and V Lukic and D Maia and N Marchili and M Massardi and P Mohan and J B Morgado and M Panwar and P Prabhakar and V.A.R.M. Ribeiro and K L J Rygl and V Sabz Ali and E Saremi and E Schisano and S Sheikhnezami and A Vafaei Sadr and A Wong and O I Wong},
   doi = {10.1093/mnras/staa3023},
   issue = {3},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 7},
   pages = {3821-3837},
   title = {Square Kilometre Array Science Data Challenge 1: Analysis and results},
   volume = {500},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098587408&doi=10.1093%2fmnras%2fstaa3023&partnerID=40&md5=06d99875555fea9ee9a595ad3ff5e3cd},
   year = {2021},
}
@article{Hosenie2021,
   author = {Z Hosenie and S Bloemen and P Groot and R Lyon and B Scheers and B Stappers and F Stoppa and P Vreeswijk and S De Wet and M K Wolt and E Körding and V McBride and R Le Poole and K Paterson and D L A Pieterse and P Woudt},
   doi = {10.1007/s10686-021-09757-1},
   journal = {Experimental Astronomy},
   note = {cited By 0},
   title = {MeerCRAB: MeerLICHT classification of real and bogus transients using deep learning},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107339600&doi=10.1007%2fs10686-021-09757-1&partnerID=40&md5=3ad53702fd806cd272e60976f905dbeb},
   year = {2021},
}
@article{Noor2021,
   author = {S Noor and M Waqas and M I Saleem and H N Minhas},
   doi = {10.1109/ACCESS.2021.3101054},
   journal = {IEEE Access},
   note = {cited By 3},
   pages = {106550-106559},
   title = {Automatic Object Tracking and Segmentation Using Unsupervised SiamMask},
   volume = {9},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111594245&doi=10.1109%2fACCESS.2021.3101054&partnerID=40&md5=41b65a3bb58e24f9f4a6f0a60bdd9343},
   year = {2021},
}
@article{Schilliro2021,
   author = {F Schilliro and P Romano},
   doi = {10.1093/mnras/stab507},
   issue = {2},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 1},
   pages = {2676-2687},
   title = {Segmentation of spectroscopic images of the low solar atmosphere by the self-organizing map technique},
   volume = {503},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116859555&doi=10.1093%2fmnras%2fstab507&partnerID=40&md5=4b25deb9595deb270da0f724f58a91c6},
   year = {2021},
}
@article{Pino2021,
   author = {C Pino and R Sortino and E Sciacca and S Riggi and C Spampinato},
   doi = {10.1007/978-3-030-89691-1_38},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   note = {cited By 0},
   pages = {393-403},
   title = {Semantic Segmentation of Radio-Astronomical Images},
   volume = {13055 LNCS},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119891386&doi=10.1007%2f978-3-030-89691-1_38&partnerID=40&md5=df52c3b3680da84b44e884e474f9fcb3},
   year = {2021},
}
@inproceedings{Vy2021,
   author = {S Vy and S Sen and K Santosh},
   doi = {10.1109/CONECCT52877.2021.9622583},
   journal = {Proceedings of CONECCT 2021: 7th IEEE International Conference on Electronics, Computing and Communication Technologies},
   note = {cited By 0},
   title = {Analyzing and Processing of Astronomical Images using Deep Learning Techniques},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123359906&doi=10.1109%2fCONECCT52877.2021.9622583&partnerID=40&md5=a496cda046ad785c37ad8412f1cc3933},
   year = {2021},
}
@inproceedings{Karypidou2021,
   author = {S Karypidou and I Georgousis and G A Papakostas},
   doi = {10.1109/PIC53636.2021.9687023},
   journal = {Proceedings of the 2021 IEEE International Conference on Progress in Informatics and Computing, PIC 2021},
   note = {cited By 0},
   pages = {94-101},
   title = {Computer Vision for Astronomical Image Analysis},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125758418&doi=10.1109%2fPIC53636.2021.9687023&partnerID=40&md5=2598b58fa5fb4f327e732d17f10314e5},
   year = {2021},
}
@inproceedings{Xiao2021,
   author = {X Xiao and X Tian},
   doi = {10.1109/ICDSBA53075.2021.00017},
   journal = {Proceedings - 2021 5th International Conference on Data Science and Business Analytics, ICDSBA 2021},
   note = {cited By 0},
   pages = {41-44},
   title = {Research on Reference Target Detection of Deep Learning Framework Faster-RCNN},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127007185&doi=10.1109%2fICDSBA53075.2021.00017&partnerID=40&md5=e5359d3da11f09dafdc2579f5682b192},
   year = {2021},
}
@inproceedings{Fielding2021,
   author = {E Fielding and C N Nyirenda and M Vaccari},
   doi = {10.1109/ICECET52533.2021.9698414},
   journal = {International Conference on Electrical, Computer, and Energy Technologies, ICECET 2021},
   note = {cited By 0},
   title = {A Comparison of Deep Learning Architectures for Optical Galaxy Morphology Classification},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127045365&doi=10.1109%2fICECET52533.2021.9698414&partnerID=40&md5=9d3f363e61e3758017de06dc28c6333e},
   year = {2021},
}
@article{Sadeghi2021,
   author = {M Sadeghi and M Javaherian and H Miraghaei},
   doi = {10.3847/1538-3881/abd314},
   issue = {2},
   journal = {Astronomical Journal},
   note = {cited By 1},
   title = {Morphological-based classifications of radio galaxies using supervised machinelearning methods associated with image moments},
   volume = {161},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101654166&doi=10.3847%2f1538-3881%2fabd314&partnerID=40&md5=611317e157d11448166a6835dbf550f3},
   year = {2021},
}
@article{,
   author = {A Vafaei Sadr and F Farsian},
   doi = {10.1088/1475-7516/2021/03/012},
   issue = {3},
   journal = {Journal of Cosmology and Astroparticle Physics},
   note = {cited By 1},
   title = {Filling in Cosmic Microwave Background map missing regions via Generative Adversarial Networks},
   volume = {2021},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103221947&doi=10.1088%2f1475-7516%2f2021%2f03%2f012&partnerID=40&md5=3a4fcafdcd9e4b7dad3b04c90a996642},
   year = {2021},
}
@inproceedings{Dere2021,
   author = {S Dere and M Fatima and R Jagtap and U Inamdar and N B Shardoor},
   doi = {10.1109/ICACCS51430.2021.9441857},
   journal = {2021 7th International Conference on Advanced Computing and Communication Systems, ICACCS 2021},
   note = {cited By 1},
   pages = {702-706},
   title = {Anomaly Detection in Astronomical Objects of Galaxies Using Deep Learning},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108026755&doi=10.1109%2fICACCS51430.2021.9441857&partnerID=40&md5=d575dcc62f6cf268586bf2e224407a83},
   year = {2021},
}
@article{Mittal2021,
   author = {S Mittal and M Tatarchenko and T Brox},
   doi = {10.1109/TPAMI.2019.2960224},
   issue = {4},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   note = {cited By 34},
   pages = {1369-1379},
   title = {Semi-Supervised Semantic Segmentation with High- And Low-Level Consistency},
   volume = {43},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102306381&doi=10.1109%2fTPAMI.2019.2960224&partnerID=40&md5=9ac3cf9d593faabcfe10c9ec5851a939},
   year = {2021},
}
@article{Ono2021,
   author = {Y Ono and R Itoh and T Shibuya and M Ouchi and Y Harikane and S Yamanaka and A K Inoue and T Amagasa and D Miura and M Okura and K Shimasaku and Y Taniguchi and S Fujimoto and M Iye and A T Jaelani and I Iwata and N Kashikawa and S Kikuchihara and S Kikuta and M A R Kobayashi and H Kusakabe and C.-H. Lee and Y Liang and Y Matsuoka and R Momose and T Nagao and K Nakajima and K.-I. Tadaki},
   doi = {10.3847/1538-4357/abea15},
   issue = {2},
   journal = {Astrophysical Journal},
   note = {cited By 4},
   title = {Silverrush X: Machine learning-aided selection of 9318 LAEs at z=2.2, 3.3, 4.9, 5.7, 6.6, and 7.0 from the HSC SSP and CHORUS survey data},
   volume = {911},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105561292&doi=10.3847%2f1538-4357%2fabea15&partnerID=40&md5=54b09d1fc3aefb14eebacba045329b24},
   year = {2021},
}
@article{Becker2021,
   author = {B Becker and M Vaccari and M Prescott and T Grobler},
   doi = {10.1093/mnras/stab325},
   issue = {2},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 6},
   pages = {1828-1846},
   title = {CNN architecture comparison for radio galaxy classification},
   volume = {503},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107867055&doi=10.1093%2fmnras%2fstab325&partnerID=40&md5=557999d3b67999e52ad41c8443c83511},
   year = {2021},
}
@article{Meher2021,
   author = {S K Meher and G Panda},
   doi = {10.1140/epjs/s11734-021-00207-9},
   issue = {10},
   journal = {European Physical Journal: Special Topics},
   note = {cited By 1},
   pages = {2285-2317},
   title = {Deep learning in astronomy: a tutorial perspective},
   volume = {230},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110617354&doi=10.1140%2fepjs%2fs11734-021-00207-9&partnerID=40&md5=280f40ef9f0024a9b9f844a8498f7719},
   year = {2021},
}
@article{,
   author = {J Vega-Ferrero and H Domínguez Sánchez and M Bernardi and M Huertas-Company and R Morgan and B Margalef and M Aguena and S Allam and J Annis and S Avila and D Bacon and E Bertin and D Brooks and A Carnero Rosell and M Carrasco Kind and J Carretero and A Choi and C Conselice and M Costanzi and L N Da Costa and M E S Pereira and J De Vicente and S Desai and I Ferrero and P Fosalba and J Frieman and J García-Bellido and D Gruen and R A Gruendl and J Gschwend and G Gutierrez and W G Hartley and S R Hinton and D L Hollowood and K Honscheid and B Hoyle and M Jarvis and A G Kim and K Kuehn and N Kuropatkin and M Lima and M A G Maia and F Menanteau and R Miquel and R L C Ogando and A Palmese and F Paz-Chinchón and A A Plazas and A K Romer and E Sanchez and V Scarpine and M Schubnell and S Serrano and I Sevilla-Noarbe and M Smith and E Suchyta and M E C Swanson and G Tarle and F Tarsitano and C To and D L Tucker and T N Varga and R D Wilkinson},
   doi = {10.1093/mnras/stab594},
   issue = {2},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 11},
   pages = {1927-1943},
   title = {Pushing automated morphological classifications to their limits with the Dark Energy Survey},
   volume = {506},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106122349&doi=10.1093%2fmnras%2fstab594&partnerID=40&md5=7ff153327c0fad772f50337b73d0357b},
   year = {2021},
}
@article{Kim2021,
   author = {D.-W. Kim and D Yeo and C A L Bailer-Jones and G Lee},
   doi = {10.1051/0004-6361/202140369},
   journal = {Astronomy and Astrophysics},
   note = {cited By 1},
   title = {Deep transfer learning for the classification of variable sources},
   volume = {653},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114556819&doi=10.1051%2f0004-6361%2f202140369&partnerID=40&md5=d9b27a11e2c1e294893bb9a70dcdc1e0},
   year = {2021},
}
@article{Bom2021,
   author = {C R Bom and A Cortesi and G Lucatelli and L O Dias and P Schubert and G B Oliveira Schwarz and N M Cardoso and E V R Lima and C Mendes De Oliveira and L Sodre and A V Smith Castelli and F Ferrari and G Damke and R Overzier and A Kanaan and T Ribeiro and W Schoenell},
   doi = {10.1093/mnras/stab1981},
   issue = {2},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 1},
   pages = {1937-1955},
   title = {Deep Learning assessment of galaxy morphology in S-PLUS Data Release 1},
   volume = {507},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116599784&doi=10.1093%2fmnras%2fstab1981&partnerID=40&md5=9daaba4e599c07d1bc87ab4d55253b44},
   year = {2021},
}
@article{Cheng2021,
   author = {T.-Y. Cheng and C J Conselice and A Aragón-Salamanca and M Aguena and S Allam and F Andrade-Oliveira and J Annis and A F L Bluck and D Brooks and D L Burke and M C Kind and J Carretero and A Choi and M Costanzi and L N da Costa and M E S Pereira and J de Vicente and H T Diehl and A Drlica-Wagner and K Eckert and S Everett and A E Evrard and I Ferrero and P Fosalba and J Frieman and J García-Bellido and D W Gerdes and T Giannantonio and D Gruen and R A Gruendl and J Gschwend and G Gutierrez and S R Hinton and D L Hollowood and K Honscheid and D J James and E Krause and K Kuehn and N Kuropatkin and O Lahav and M A G Maia and M March and F Menanteau and R Miquel and R Morgan and F Paz-Chinchón and A Pieres and A A P Malagón and A Roodman and E Sanchez and V Scarpine and S Serrano and I Sevilla-Noarbe and M Smith and M Soares-Santos and E Suchyta and M E C Swanson and G Tarle and D Thomas},
   doi = {10.1093/mnras/stab2142},
   issue = {3},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 5},
   pages = {4425-4444},
   title = {Galaxy morphological classification catalogue of the Dark Energy Survey Year 3 data with convolutional neural networks},
   volume = {507},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114423282&doi=10.1093%2fmnras%2fstab2142&partnerID=40&md5=5dd9261eec2a05a90b1a33e7bc994971},
   year = {2021},
}
@article{Banerjee2021,
   author = {S Banerjee and B R Ghosh and A Gangapadhyay and H S Chatterjee},
   doi = {10.24996/ijs.2021.62.10.27},
   issue = {10},
   journal = {Iraqi Journal of Science},
   note = {cited By 0},
   pages = {3690-3696},
   title = {Galaxy morphological image classification using resnet},
   volume = {62},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118805412&doi=10.24996%2fijs.2021.62.10.27&partnerID=40&md5=1b7e41d6220b4ceb1f47b0a5a9425678},
   year = {2021},
}
@article{Panes2021,
   author = {B Panes and C Eckner and L Hendriks and S Caron and K DIjkstra and G Jóhannesson and R R De Austri and G Zaharijas},
   doi = {10.1051/0004-6361/202141193},
   journal = {Astronomy and Astrophysics},
   note = {cited By 5},
   title = {Identification of point sources in gamma rays using U-shaped convolutional neural networks and a data challenge},
   volume = {656},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121036912&doi=10.1051%2f0004-6361%2f202141193&partnerID=40&md5=b7e67a58b0ce8656adf7a6fca8165a0c},
   year = {2021},
}
@article{Lao2021,
   author = {B Lao and T An and A Wang and Z Xu and S Guo and W Lv and X Wu and Y Zhang},
   doi = {10.1016/j.scib.2021.07.015},
   issue = {21},
   journal = {Science Bulletin},
   note = {cited By 2},
   pages = {2145-2147},
   title = {Artificial intelligence for celestial object census: the latest technology meets the oldest science},
   volume = {66},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111519330&doi=10.1016%2fj.scib.2021.07.015&partnerID=40&md5=1c3f795b301e0f62ec371b080802cffc},
   year = {2021},
}
@article{Mackovjak2021,
   author = {Š Mackovjak and M Harman and V Maslej-Krešňáková and P Butka},
   doi = {10.1093/mnras/stab2536},
   issue = {3},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 0},
   pages = {3111-3124},
   title = {SCSS-Net: solar corona structures segmentation by deep learning},
   volume = {508},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119504100&doi=10.1093%2fmnras%2fstab2536&partnerID=40&md5=83545df2ed8d026e64259c71d6f6a322},
   year = {2021},
}
@article{Ruhe2022,
   author = {D Ruhe and M Kuiack and A Rowlinson and R Wijers and P Forré},
   doi = {10.1016/j.ascom.2021.100512},
   journal = {Astronomy and Computing},
   note = {cited By 1},
   title = {Detecting dispersed radio transients in real time using convolutional neural networks},
   volume = {38},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125718119&doi=10.1016%2fj.ascom.2021.100512&partnerID=40&md5=7900333970621a194770a43add08ee49},
   year = {2022},
}
@inproceedings{Hu2022,
   author = {Y Hu and Y Liu and Z Liu},
   doi = {10.1109/ICCRD54409.2022.9730377},
   journal = {2022 IEEE 14th International Conference on Computer Research and Development, ICCRD 2022},
   note = {cited By 1},
   pages = {100-107},
   title = {A Survey on Convolutional Neural Network Accelerators: GPU, FPGA and ASIC},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127747390&doi=10.1109%2fICCRD54409.2022.9730377&partnerID=40&md5=9e9d452cb17ede20ce0238a17537e773},
   year = {2022},
}
@article{Zhou2022,
   author = {Y Zhou and R Jiao and D Wang and J Mu and J Li},
   doi = {10.1109/ACCESS.2022.3172664},
   journal = {IEEE Access},
   note = {cited By 0},
   pages = {48855-48864},
   title = {Catastrophic Forgetting Problem in Semi-Supervised Semantic Segmentation},
   volume = {10},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129631065&doi=10.1109%2fACCESS.2022.3172664&partnerID=40&md5=4a3ad56584f304d8de2b4dd9f683c779},
   year = {2022},
}
@article{Dimililer2022,
   author = {K Dimililer and H Teimourian and F Al-Turjman},
   doi = {10.1080/0952813X.2022.2080277},
   journal = {Journal of Experimental and Theoretical Artificial Intelligence},
   note = {cited By 0},
   title = {Radio galaxies classification system using machine learning techniques in the IoT Era},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131693790&doi=10.1080%2f0952813X.2022.2080277&partnerID=40&md5=77afd2e9879b6e447c0252561d650565},
   year = {2022},
}
@article{Zhou2022,
   author = {C Zhou and Y Gu and G Fang and Z Lin},
   doi = {10.3847/1538-3881/ac4245},
   issue = {2},
   journal = {Astronomical Journal},
   note = {cited By 0},
   title = {Automatic Morphological Classification of Galaxies: Convolutional Autoencoder and Bagging-based Multiclustering Model},
   volume = {163},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125459037&doi=10.3847%2f1538-3881%2fac4245&partnerID=40&md5=ba769b8e50f66f28337527a8a442c151},
   year = {2022},
}
@article{Rezaei2022,
   author = {S Rezaei and J P Mckean and M Biehl and A Javadpour},
   doi = {10.1093/mnras/stab3519},
   issue = {4},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 0},
   pages = {5891-5907},
   title = {DECORAS: Detection and characterization of radio-astronomical sources using deep learning},
   volume = {510},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125126157&doi=10.1093%2fmnras%2fstab3519&partnerID=40&md5=2e12f120ec96fe3fbc3a34c4402131f6},
   year = {2022},
}
@article{Ostdiek2022,
   author = {B Ostdiek and A Diaz Rivero and C Dvorkin},
   doi = {10.3847/1538-4357/ac2d8d},
   issue = {1},
   journal = {Astrophysical Journal},
   note = {cited By 0},
   title = {Extracting the Subhalo Mass Function from Strong Lens Images with Image Segmentation},
   volume = {927},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126590769&doi=10.3847%2f1538-4357%2fac2d8d&partnerID=40&md5=88fdfc63dd563f1431377a8da2172f51},
   year = {2022},
}
@article{Tanoglidis2022,
   author = {D Tanoglidis and A Ćiprijanović and A Drlica-Wagner and B Nord and M.H.L.S. Wang and A J Amsellem and K Downey and S Jenkins and D Kafkes and Z Zhang},
   doi = {10.1016/j.ascom.2022.100580},
   journal = {Astronomy and Computing},
   note = {cited By 0},
   title = {DeepGhostBusters: Using Mask R-CNN to detect and mask ghosting and scattered-light artifacts from optical survey images},
   volume = {39},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129587689&doi=10.1016%2fj.ascom.2022.100580&partnerID=40&md5=dd2ccb6a529adb77ffdb1f96481b7b21},
   year = {2022},
}
@article{Gu2022,
   author = {W Gu and S Bai and L Kong},
   doi = {10.1016/j.imavis.2022.104401},
   journal = {Image and Vision Computing},
   note = {cited By 1},
   title = {A review on 2D instance segmentation based on deep neural networks},
   volume = {120},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124482120&doi=10.1016%2fj.imavis.2022.104401&partnerID=40&md5=6b4816709d348067d918132af9797898},
   year = {2022},
}
@article{Walmsley2022,
   author = {M Walmsley and A M M Scaife and C Lintott and M Lochner and V Etsebeth and T Géron and H Dickinson and L Fortson and S Kruk and K L Masters and K B Mantha and B D Simmons},
   doi = {10.1093/mnras/stac525},
   issue = {2},
   journal = {Monthly Notices of the Royal Astronomical Society},
   note = {cited By 0},
   pages = {1581-1599},
   title = {Practical galaxy morphology tools from deep supervised representation learning},
   volume = {513},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130475727&doi=10.1093%2fmnras%2fstac525&partnerID=40&md5=c4ad8385b85b9f37c33da105720a4a8f},
   year = {2022},
}
@article{Feng2022,
   author = {Z Feng and Q Zhou and Q Gu and X Tan and G Cheng and X Lu and J Shi and L Ma},
   doi = {10.1016/j.patcog.2022.108777},
   journal = {Pattern Recognition},
   note = {cited By 1},
   title = {DMT: Dynamic mutual training for semi-supervised learning},
   volume = {130},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130197160&doi=10.1016%2fj.patcog.2022.108777&partnerID=40&md5=71f31fac3d1abbfe05813b5027a2c802},
   year = {2022},
}
@generic{Farias2020,
   abstract = {The classification of galaxies based on their morphology is instrumental for the understanding of galaxy formation and evolution. This, in addition to the ever-growing digital astronomical datasets, has motivated the application of advanced computer vision techniques, such as Deep Learning. However, these models have not been implemented as single pipelines that replicate detection, segmentation and morphological classification of galaxies directly from images, as it would be made by experts. We present the first implementation of an automatic machine learning pipeline for detection, segmentation and morphological classification of galaxies based on the Mask R-CNN Deep Learning architecture. This state-of-the-art model of Instance Segmentation also performs image segmentation at the pixel level, which is a recurrent need in the astronomical community. We achieve Mean Average Precision (mAP) of 0.93 in the morphological classification of Spiral or Elliptical galaxies for a set of 239,639 objects from the Galaxy Zoo sample and JPEG images from the Sloan Digital Sky Survey. As a direct use of segmentation, we test the model for deriving centroids of extended sources, reaching a precision better than 1.0 arcsecond. We also test the network under additive Gaussian noise. We find that the Mask R-CNN network is able to perform with accuracy over 92% for a distribution scale of 76.5 counts. The repository with the model code is in the following url: https://github.com/hfarias/mask_galaxy},
   author = {H. Farias and D. Ortiz and G. Damke and M. Jaque Arancibia and M. Solar},
   doi = {10.1016/j.ascom.2020.100420},
   issn = {22131337},
   journal = {Astronomy and Computing},
   keywords = {Computer-vision,Deep Learning,Mask R-CNN,Morphological-segmentation},
   month = {10},
   publisher = {Elsevier B.V.},
   title = {Mask galaxy: Morphological segmentation of galaxies},
   volume = {33},
   year = {2020},
}
@article{,
   abstract = {We present a method for automatic detection and classification of galaxies which includes a novel data-augmentation procedure to make trained models more robust against the data taken from different instruments and contrast-stretching functions. This method is shown as part of AstroCV, a growing open source computer vision repository for processing and analyzing big astronomical datasets, including high performance Python and C++ algorithms used in the areas of image processing and computer vision. The underlying models were trained using convolutional neural networks and deep learning techniques, which provide better results than methods based on manual feature engineering and SVMs in most of the cases where training datasets are large. The detection and classification methods were trained end-to-end using public datasets such as the Sloan Digital Sky Survey (SDSS), the Galaxy Zoo, and private datasets such as the Next Generation Virgo (NGVS) and Fornax (NGFS) surveys. Training results are strongly bound to the conversion method from raw FITS data for each band into a 3-channel color image. Therefore, we propose data augmentation for the training using 5 conversion methods. This greatly improves the overall galaxy detection and classification for images produced from different instruments, bands and data reduction procedures. The detection and classification methods were trained using the deep learning framework DARKNET and the real-time object detection system YOLO. These methods are implemented in C language and CUDA platform, and makes intensive use of graphical processing units (GPU). Using a single high-end Nvidia GPU card, it can process a SDSS image in 50 ms and a DECam image in less than 3 s. We provide the open source code, documentation, pre-trained networks, python tutorials, and how to train your own datasets, which can be found in the AstroCV repository. https://github.com/astroCV/astroCV.},
   author = {R. E. González and R. P. Muñoz and C. A. Hernández},
   doi = {10.1016/j.ascom.2018.09.004},
   issn = {22131337},
   journal = {Astronomy and Computing},
   keywords = {Computing methodologies,Galaxies,General,Image processing,Machine learning,Techniques},
   month = {10},
   pages = {103-109},
   publisher = {Elsevier B.V.},
   title = {Galaxy detection and identification using deep learning and data augmentation},
   volume = {25},
   year = {2018},
}
@generic{Lao2021,
   author = {Baoqiang Lao and Tao An and Ailing Wang and Zhijun Xu and Shaoguang Guo and Weijia Lv and Xiaocong Wu and Yingkang Zhang},
   doi = {10.1016/j.scib.2021.07.015},
   issn = {20959281},
   issue = {21},
   journal = {Science Bulletin},
   month = {11},
   pages = {2145-2147},
   publisher = {Elsevier B.V.},
   title = {Artificial intelligence for celestial object census: the latest technology meets the oldest science},
   volume = {66},
   year = {2021},
}
@article{Jimenez2020,
   abstract = {Many research fields are now faced with huge volumes of data automatically generated by specialised equipment. Astronomy is a discipline that deals with large collections of images difficult to handle by experts alone. As a consequence, astronomers have been relying on the power of the crowds, as a form of citizen science, for the classification of galaxy images by amateur people. However, the new generation of telescopes that will produce images at a higher rate highlights the limitations of this approach, and the use of machine learning methods for automatic classification is considered essential. The goal of this paper is to shed light on the automated classification of galaxy images exploring two distinct machine learning strategies. First, following the classical approach consisting of feature extraction together with a classifier, we compare the state-of-the-art feature extractor for this problem, the WND-CHARM, with our proposal based on autoencoders for feature extraction on galaxy images. We then compare these results with an end-to-end classification using convolutional neural networks. To better leverage the available citizen science data, we also investigate a pre-training scheme that exploits both amateur- and expert-labelled data. Our experiments reveal that autoencoders greatly speed up feature extraction in comparison with WND-CHARM and both classification strategies, either using convolutional neural networks or feature extraction, reach comparable accuracy. The use of pre-training in convolutional neural networks, however, has allowed us to provide even better results.},
   author = {Manuel Jimenez and Mercedes Torres Torres and Robert John and Isaac Triguero},
   doi = {10.1109/ACCESS.2020.2978804},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Astroinformatics,autoencoders,citizen science,convolutional neural networks,deep learning,feature extraction,galaxy morphologies,image classification},
   pages = {47232-47246},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Galaxy image classification based on citizen science data: A comparative study},
   volume = {8},
   year = {2020},
}
@article{Walmsley2022,
   abstract = {We present Galaxy Zoo DECaLS: detailed visual morphological classifications for Dark Energy Camera Legacy Survey images of galaxies within the SDSS DR8 footprint. Deeper DECaLS images (r = 23.6 versus r = 22.2 from SDSS) reveal spiral arms, weak bars, and tidal features not previously visible in SDSS imaging. To best exploit the greater depth of DECaLS images, volunteers select from a new set of answers designed to improve our sensitivity to mergers and bars. Galaxy Zoo volunteers provide 7.5 million individual classifications over 314 000 galaxies. 140 000 galaxies receive at least 30 classifications, sufficient to accurately measure detailed morphology like bars, and the remainder receive approximately 5. All classifications are used to train an ensemble of Bayesian convolutional neural networks (a state-of-the-art deep learning method) to predict posteriors for the detailed morphology of all 314 000 galaxies. We use active learning to focus our volunteer effort on the galaxies which, if labelled, would be most informative for training our ensemble. When measured against confident volunteer classifications, the trained networks are approximately 99 per cent accurate on every question. Morphology is a fundamental feature of every galaxy; our human and machine classifications are an accurate and detailed resource for understanding how galaxies evolve.},
   author = {Mike Walmsley and Chris Lintott and Tobias Géron and Sandor Kruk and Coleman Krawczyk and Kyle W. Willett and Steven Bamford and Lee S. Kelvin and Lucy Fortson and Yarin Gal and William Keel and Karen L. Masters and Vihang Mehta and Brooke D. Simmons and Rebecca Smethurst and Lewis Smith and Elisabeth M. Baeten and Christine MacMillan},
   doi = {10.1093/mnras/stab2093},
   issn = {13652966},
   issue = {3},
   journal = {Monthly Notices of the Royal Astronomical Society},
   keywords = {galaxies: bar,galaxies: general,galaxies: interactions,methods: data analysis},
   month = {1},
   pages = {3966-3988},
   publisher = {Oxford University Press},
   title = {Galaxy Zoo DECaLS: Detailed visual morphology measurements from volunteers and deep learning for 314 000 galaxies},
   volume = {509},
   year = {2022},
}
